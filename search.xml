<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[逻辑回归]]></title>
    <url>%2F2019%2F01%2F22%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[概念Logistic Regression 在《机器学习》-周志华一书中又叫对数几率回归。逻辑回归和多重线性回归实际上有很多的相同之处，除了它们的因变量（函数）不同外，其他的基本差不多，所以逻辑回归和线性回归又统属于广义线性模型（generalizedlinear model）。 广义线性模型的形式其实都差不多，不同的就是因变量（函数）的不同。 如果是连续的，就是多重线性回归 如果是二项分布，就是Logistic回归 如果是Poisson分布，就是Poisson分布 如果是负二项分布，就是负二项回归 Logistic回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最常用的就是二分类的Logistic回归。 主要用途 寻找危险因素：寻找某一疾病的危险因素等； 预测：根据模型，预测在不同的自变量情况下，发生某病或某种情况的概率有多大； 判别：实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。 一般步骤 寻找h函数（即hypothesis）； 构造J函数（损失函数）； 想办法使得J函数最小并求得回归参数（θ） 构造预测函数（hypothesis）Logistic回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题（即输出只有两种，分别代表两个类别），所以利用了Logistic函数（或称为Sigmoid函数），函数形式为： $g(z)=\frac{1}{1+e^{-z}}$ 为了方便后面使用我们求出g(z)g(z)的导数： $g’(z)=\frac1{1+e^{-z} } \cdot (1- \frac1{1+e^{-z} })=g(z)(1-g(z))$ Sigmoid 函数在有个很漂亮的“S”形，如下图所示: 构建预测函数对于线性边界的情况，边界形式如下： \theta_0+\theta_1x_1+\cdot \cdot \cdot + \theta_nx_n=\sum_{i=1}^n \theta_ix_i = \theta^Tx构建的预测函数为： h_\theta(x)=g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}构建损失函数由于是二项分布，函数$h_ \theta(x)$的值就有特殊的含义，它所表示的是结果取1的概率，因此对于输入x的分类结果就判别为类别1和类别0的概率分别为： $P(y=1|x;\theta)=h_\theta(x) $ $P(y=0|x;\theta)=1-h_\theta(x)$ 所以： P(y|x;\theta) = {h_\theta(x) }^y{(1-h_\theta(x)) }^{1-y}构建似然函数L(\theta)=\prod_{i=1}^n P(y_i|x_i;\theta) =\prod_{i=1}^n {h_\theta(x_i) }^{y_i}{(1-h_\theta(x_i)) }^{1-y_i}对数似然函数l(\theta)=logL(\theta)=\sum_{i=1}^n(y_ilogh_\theta(x_i)+(1-y_i)log(1-h_\theta(x_i)))最大似然估计就是求使$l(\theta)$取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数。 梯度下降法求的最小值θ更新过程： $\theta_j :=\theta_j-a(\frac{\partial l(\theta)}{\partial \theta_j})$ 对θ求偏导: $\frac{\partial l(\theta)}{\partial \theta_j}=\frac{\partial g(\theta^Tx)}{\partial \theta_j}(\frac{y}{g(\theta^Tx)}-\frac{1-y}{g(\theta^Tx)})$ $=g(\theta^Tx)(1-g(\theta^Tx)) \frac{\partial\theta^Tx}{\partial \theta_j}(\frac{y}{g(\theta^Tx)}-\frac{1-y}{g(\theta^Tx)})$ $=(y(1-g(\theta^Tx))-(1-y)g(\theta^Tx))x_j$ $=(y-h_\theta(x))x_j$ θ更新过程就可以写为： \theta_j :=\theta_j-a \sum_{i=1}^n (y_i-h_{\theta} (x_i))x_i^j但是在在Andrew Ng的课程中将$J(\theta)$取为下式，即： J(\theta)=-\frac{1}{m}l(\theta)因为乘了一个负的系数-1/m，所以取$J(\theta)$最小值时的θ为要求的最佳参数。 \frac{\partial l(\theta)}{\partial \theta_j}=\frac 1 m \sum_{i=1}^n(h_\theta(x_i)-y_i)x_i^j相应的θ: \theta_j :=\theta_j-a \frac 1 m \sum_{i=1}^n (h_\theta(x_i)-y_i)x_i^j向量化（Vectorization ）Vectorization是使用矩阵计算来代替for循环，以简化计算过程，提高效率。 如上式，Σ(…)是一个求和的过程，显然需要一个for语句循环m次，所以根本没有完全的实现vectorization。 下面介绍向量化的过程： 约定训练数据的矩阵形式如下，x的每一行为一条训练样本，而每一列为不同的特称取值： g(A)的参数A为一列向量，所以实现g函数时要支持列向量作为参数，并返回列向量。由上式可知$h_\theta(x)-y$可以由$g(A)-y$一次求得 θ更新过程可以改为: \theta_j :=\theta_j-a \frac 1 m \sum_{i=1}^n (h_\theta(x_i)-y_i)x_i^j=\theta_j-a \frac 1 m \sum_{i=1}^n e_ix_i^j=\theta_j-a \frac1 m x^TE综上所述，Vectorization后θ更新的步骤如下： $A=x \cdot \theta$ $E=g(A)-y$ $\theta:=\theta-ax^TE$ 正则化Regularization同样逻辑回归也有欠拟合、适合拟合、过拟合问题 对于线性回归或逻辑回归的损失函数构成的模型，可能会有些权重很大，有些权重很小，导致过拟合（就是过分拟合了训练数据），使得模型的复杂度提高，泛化能力较差（对未知数据的预测能力）。 解决方法1）减少特征数量（减少特征会失去一些信息，即使特征选的很好） 可用人工选择要保留的特征； 模型选择算法； 2）正则化（特征较多时比较有效） 保留所有特征，但减少θ的大小 正则化方法正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项就越大。 正则项可以取不同的形式，在回归问题中取平方损失，就是参数的L2范数，也可以取L1范数。取平方损失时，模型的损失函数变为： J(\theta)=\frac1{2m}\sum_{i=1}^{n}(h_\theta(x_i)-y_i)^2+\lambda\sum_{j=1}^n \theta_j^2lambda是正则项系数： 如果它的值很大，说明对模型的复杂度惩罚大，对拟合数据的损失惩罚小，这样它就不会过分拟合数据，在训练数据上的偏差较大，在未知数据上的方差较小，但是可能出现欠拟合的现象； 如果它的值很小，说明比较注重对训练数据的拟合，在训练数据上的偏差会小，但是可能会导致过拟合。 正则化后的梯度下降算法θ的更新变为： \theta_j :=\theta_j-a \frac 1 m \sum_{i=1}^n (h_\theta(x_i)-y_i)x_i^j - \frac \lambda m \theta_j其他优化算法 Conjugate gradient method(共轭梯度法) Quasi-Newton method(拟牛顿法) BFGS method(局部优化法) L-BFGS(Limited-memory BFGS)（有限内存局部优化法） 后二者由拟牛顿法引申出来，与梯度下降算法相比，这些算法的优点是： 第一，不需要手动的选择步长； 第二，通常比梯度下降算法快； 但是缺点是更复杂。 多类分类问题多类分类问题中,我们的训练集中有多个类(&gt;2),我们无法仅仅用一个二元变量(0或1)来做判断依据。例如我们要预测天气情况分四种类型:晴天、多云、下雨或下雪。 一种解决这类问题的途径是采用一对多(One-vs-All)方法（可以将其看做成二类分类问题：保留其中的一类，剩下的作为另一类 ）。在一对多方法中,我们将多类分类问题转化成二元分类问题。为了能实现这样的转变,我们将多个类中的一个类标记为正向类(y=1),然后将其他所有类都标记为负向类,这个模型记作： $h_\theta^{(1)}(x)$ 接着,类似地第我们选择另一个类标记为正向类(y=2),再将其它类都标记为负向类,将这个模型记作: $h_\theta^{(2)}(x)$ 依此类推。最后我们得到一系列的模型简记为: $h_\theta^{(i)}(x)=p(y=i|x;\theta)$ 最后,在我们需要做预测时,我们将所有的分类机都运行一遍,然后对每一个输入变量,都选择最高可能性的输出变量。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习简介]]></title>
    <url>%2F2019%2F01%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[机器学习任务的一般步骤１．确定特征 – 可能是最重要的步骤! （收集训练数据）２．确定模型 – 目标函数/决策边界形状３．模型训练：根据训练数据估计模型参数 – 优化计算４．模型评估：在校验集上评估模型预测性能５．模型应用/预测 模型 非线性模型 目标函数 损失函数—回归 损失函数—分类 正则项 常用正则函数 常见线性模型的损失和正则项组合 L2损失 L1损失 Huber Logistic损失 合叶损失 ε-insentive损失 L2正则 岭回归 L2正则 Logistic回归 SVM SVR L1正则 LASSO L1正则 Logistic回归 L2+L1正则 Elastic net]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性回归]]></title>
    <url>%2F2019%2F01%2F22%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[线性回归（Linear Regression）数理统计中回归分析，用来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，其表达形式为$y=wx+e$，e为误差服从均值为0的正态分布，其中只有一个自变量的情况称为简单回归，多个自变量的情况叫多元回归。 注意，统计学中的回归并非如线性回归非与严格直线函数完全能拟合，所以我们统计中称之为回归用以与其直线函数区别。 下面看个例子： 数据：工资和年龄（2个特征）目标：预测银行会贷款给我多少钱（标签）考虑：工资和年龄都会影响最终银行贷款的结果那么它们各自有多大的影响呢？（参数） 工资 年龄 额度 4000 25 20000 8000 30 70000 5000 28 35000 7500 33 50000 这个表格表示的是可贷款的金额 与 工资 和 年龄之间的关系，其中 工资 和 年龄 为 特征，可贷款金额为目标函数值。 那么根据线性函数可得到以下公式： h_\theta(x)=\theta_{1}x_{1}+\theta_{2}x_{2}上面的这个式子是当一个模型只有两个特征(x1,x2)的时候的线性回归式子。 正常情况下，现金贷中可贷款的额度和用户的很多特征相关联，并不只是简单的这两个特征。所以我们需要把这个式子进行通用化。 假如有n个特征的话，那么式子就会变成下面的样子： h_\theta(x)=\theta_{1}x_{1}+\theta_{2}x_{2} + \cdot \cdot \cdot \cdot \cdot+\theta_{n}x_{n} = \sum_{i=1}^{n}\theta_{i}x_{i}利用矩阵的知识对线性公式进行整合因为机器学习中基本上都是用矩阵的方式来表示参数的，也就是说我们需要把这个多项求和的式子用矩阵的方式表达出来，这样才方便后续的计算。 \theta_{i \times 1} = [\theta_1,\theta_2,\cdot\cdot\cdot\theta_i,]X_{i\times1}=[x_1,x_2,\cdot \cdot \cdot x_i]\theta^TX=\begin{bmatrix} \theta_1 \\ \theta_2 \\ \cdot \\ \cdot \\ \cdot \\ \theta_i \end{bmatrix} \cdot [x_1,x_2,\cdot \cdot \cdot x_i] = \sum_{i=1}^{n}\theta_{i}x_{i}我们把权重参数和特征参数，都看成是1行n列的矩阵(或者是行向量)。那么就可以根据矩阵乘法的相关知识，把上述多项求和的式子，转换成矩阵的乘法的表达式。 由此我们就把多项求和化简称了 。 h_\theta(x)=\theta^TX误差真实值和预测值之间肯定是存在误差的，我们用\varepsilon来表示该误差 所以回归函数变为： h_\theta(x)=\theta^Tx+\varepsilon我们根据实际情况，假设认为这个误差项是满足以下几个条件的: 误差\varepsilon_{(i)}是独立的。 具有相同的分布。 服从均值为0方差为θ_2的高斯分布。 然后我们回到刚开始的现金贷产品的贷款额度问题上面: 独立：张三和李四一起使用这款产品，可贷款额互不影响 同分布：张三和李四是使用的是同一款产品 高斯分布：绝大多数的情况下，在一个的空间内浮动不大 似然函数由前面两步，我们已经把线性回归模型，推导成下面的这个式子: $y_{(i)}=\theta^Tx_i+\varepsilon_i$ (1) 因为误差项是符合高斯分布的，所以误差项的概率值： $P(\varepsilon_i)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(\varepsilon_i)^2}{2\sigma^2})}$ (2) 将 (1) 式代入 (2) 式： $P(y_i|x_i,\theta)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$ 由于是误差值，所以是越小越好，所以我们接下来就是讨论什么样的特征值和特征组合能够让误差值最小，似然函数的作用就是要根据样本求什么样的参数和特征的组成能够接近真实值，越接近真实值则误差就越小。 引入似然函数(似然函数就是求能让真实值和预测值相等的那个参数 )： L(\theta) = \prod_{i=1}^{N} P(y_i|x_i,\theta)=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$\prod$表示各元素相乘 上面的式子是多个参数的乘积的形式，很难进行计算，所以我们又采用了对数的一个小技巧，把多个数相乘，转化成多个数相加的形式。 因为对数的性质: $logA\cdot B = logA+logB$ 根据上面的这种换算关系，我们就把似然函数的式子换算成下面的这个。 (因为似然函数是越大越好，似然函数的值和对数似然函数的值是成正比的，对值求对数，并不会影响到最后求极限的值。所以才敢进行对数处理。) $l(\theta) = logL(\theta) = log\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$ 对上式进行整理： $l(\theta) = logL(\theta) = \sum_{i=1}^{N}log\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$ $= \sum_{i=1}^{N}(log\frac{1}{\sqrt{2\pi}\sigma}+loge^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})})$ $= Nlog\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_i-\theta^Tx_i)^2$ 因为： $Nlog\frac{1}{\sqrt{2\pi}\sigma}$和$-\frac{1}{2\sigma^2}$是一个定值 似然函数是要越大越好 所以： $l(\theta) = \sum_{i=1}^{N}(y_i-\theta^Tx_i)^2$ $\sum_{i=1}^{N}(y_i-\theta^Tx_i)^2$越小越好——最小二乘法（损失函数） 最小二乘法上述代价函数中使用的均方误差，其实对应了我们常用的欧几里得的距离（欧式距离，Euclidean Distance）, 基于均方误差最小化进行模型求解的方法称为“最小二乘法”（least square method），即通过最小化误差的平方和寻找数据的最佳函数匹配； 当函数子变量为一维时，最小二乘法就蜕变成寻找一条直线； 然后我们把得到的损失函数推广到n维，转换成矩阵形式（参考前面利用矩阵的知识对线性公式进行整合）： $J(\theta)=\sum_{i=1}^{N}(y_i-\theta^Tx_i)^2$(损失函数) 其对应的均方误差表示为如下矩阵： $J(\theta) = {(y-X\theta)^T(y-X\theta)}$ 其中Ｘ: X=\begin{bmatrix} 1 && x_1^T \\ 1 && x_2^T \\ \cdot \\ \cdot \\ \cdot \\ 1 && x_N^T \end{bmatrix} =\begin{bmatrix} 1 && x_{11} && x_{12} && \cdot \cdot \cdot x_{1n} \\ 1 && x_{21} && x_{22} && \cdot \cdot \cdot x_{2n} \\ \cdot \\ \cdot \\ \cdot \\ 1&& x_{m1} && x_{m2} && \cdot \cdot \cdot x_{mn} \end{bmatrix}对$θ$求导: $J(\theta) = {(y-X\theta)^T(y-X\theta)}=y^Ty-y^Tx\theta-\theta^Tx^Ty+\theta^Tx^Tx\theta$ $\frac{\partial J(\theta)}{\partial(\theta)} = \frac{\partial y^Ty}{\partial(\theta)} - \frac{\partial y^Tx\theta}{\partial(\theta)} - \frac{\partial \theta^Tx^Ty}{\partial(\theta)} + \frac{\partial \theta^Tx^Tx\theta}{\partial(\theta)}$ $\frac{\partial J(\theta)}{\partial(\theta)} = 0-x^Ty-x^Ty+2x^Tx\theta$ $\frac{\partial J(\theta)}{\partial(\theta)} =2x^T(x\theta-y)$ 根据导数的性质，该值在导数为0时为最小 所以： 根据微积分定理，令上式等于零，可以得到 θ 最优的闭式解。当$2(x^Ty-x^Tx\theta)=0$时取得最小 最终： $\theta = (x^Tx)^{-1}x^Ty$ X和Y都是已知的，那么得到了最终的参数值。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM]]></title>
    <url>%2F2019%2F01%2F22%2FSVM%2F</url>
    <content type="text"><![CDATA[背景 最大间隔分类器 距离的计算在样本空间中，划分超平面可通过如下线性方程描述： w^Tx+b=0样本空间中任意点x到超平面的距离可写为: r=\frac{\lvert w^Tx+b\rvert}{\lVert w\rVert}数据标签定义 优化的目标 目标函数 拉格朗日乘子法 SVM求解 SVM求解实例 支持向量：真正发挥作用的数据点，ɑ值不为0的点 带松弛因子的SVM：C-SVM soft-margin 核方法 低维不可分问题 常用核函数 支持向量回归（SVR） Scikit learn 中的SVM实现 Scikit learn 中的SVC实现 核函数kernel RBF核的参数 RBF核—核参数正好 RBF核—欠拟合 RBF核—过拟合 总结SVM的优点： – 在高维空间中行之有效 – 当维数大于样本数时仍然可用（但性能不好） – 在决策函数中只使用训练点的一个子集（支持向量），大大节省了内存开 销 – 用途广泛：决策函数中可使用不同的核函数• 劣势： ​ – SVM不直接提供概率估计​ – 可通过交叉验证计算，代价比较高• Scikit-learn中的支持向量机同时支持密集样本向量（numpy.ndarray和可通过numpy.asarray转化的数据类型）和稀疏样本向量（任何scipy.sparse对象）。但如果想用SVM对稀疏数据进行预测，则必须先在这些数据上拟合。为了优化性能，应该使用C阶（C-Ordered）numpy.ndarray（密集的）或scipy.sparse.csr_matrix（稀疏的），并指定dtype=float64]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVD分解]]></title>
    <url>%2F2019%2F01%2F22%2FSVD%E5%88%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[奇异值分解(SVD)原理奇异值分解(Singular Value Decomposition，以下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。 特征值和特征向量 注意到要进行特征分解，矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？ 答案是可以，此时我们的SVD登场了。 SVD的定义 SVD计算举例 SVD性质 由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。下面我们就对SVD用于PCA降维做一个介绍。 SVD用于PCA在主成分分析（PCA）中，我们讲到要用PCA降维，需要找到样本协方差矩阵 $XX^T$ 的最大的d个特征向量，然后用这最大的d个特征向量张成的矩阵来做低维投影降维。可以看出，在这个过程中需要先求出协方差矩阵 $XX^T$ ，当样本数多样本特征数也多的时候，这个计算量是很大的。 注意到我们的SVD也可以得到协方差矩阵 $XX^T$ 最大的d个特征向量张成的矩阵，但是SVD有个好处，有一些SVD的实现算法可以不先求出协方差矩阵 $XX^T$ ，也能求出我们的右奇异矩阵V。也就是说，我们的PCA算法可以不用做特征分解，而是做SVD来完成。这个方法在样本量很大的时候很有效。实际上，scikit-learn的PCA算法的背后真正的实现就是用的SVD，而不是我们我们认为的暴力特征分解。 另一方面，注意到PCA仅仅使用了我们SVD的右奇异矩阵，没有使用左奇异矩阵，那么左奇异矩阵有什么用呢？ 假设我们的样本是m×n的矩阵X，如果我们通过SVD找到了矩阵 $XX^T$ 最大的d个特征向量张成的m×d维矩阵U，则我们如果进行如下处理： X'_{d\times n}=U_{d\times m}^TX_{m\times n}可以得到一个d×n的矩阵X‘,这个矩阵和我们原来的m×n维样本矩阵X相比，行数从m减到了d，可见对行数进行了压缩。也就是说，左奇异矩阵可以用于行数的压缩。相对的，右奇异矩阵可以用于列数即特征维度的压缩，也就是我们的PCA降维。 Numpy求解SVDimport numpy as npdata = np.array( [[1, 1, 1, 0, 0], [2, 2, 2, 0, 0], [3, 3, 3, 0, 0], [5, 5, 3, 2, 2], [0, 0, 0, 3, 3], [0, 0, 0, 6, 6]])u, sigma, vt = np.linalg.svd(data) #SVD分解print(u.shape, sigma.shape, vt.shape) SVD小结SVD作为一个很基本的算法，在很多机器学习算法中都有它的身影，特别是在现在的大数据时代，由于SVD可以实现并行化，因此更是大展身手。SVD的原理不难，只要有基本的线性代数知识就可以理解，实现也很简单因此值得仔细的研究。当然，SVD的缺点是分解出的矩阵解释性往往不强，有点黑盒子的味道，不过这不影响它的使用。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[点击预估]]></title>
    <url>%2F2019%2F01%2F21%2F%E7%82%B9%E5%87%BB%E9%A2%84%E4%BC%B0%2F</url>
    <content type="text"><![CDATA[目标广告显示：只显示相关的广告 背景： ​ – 用户喜欢相关的广告​ – 只有用户点击时，广告平台才能收到广告费 所以预测一个广告是否会被点击很关键 方式： – 预估点击率 (predict Click Through Rate, pCTR) 推荐系统 vs. 点击率预估 CTR的挑战• 用最少的资源在大量的数据上训练大量的模型 上亿的特征数目（模型系数的数目） 每天有上亿的流量（提供上亿次预测服务） 上亿的训练样本数目 • 其他 正负样本数目不均衡 面向稀有事件的 Logistic Regression 模型校准 特征稀疏（OneHot编码） https://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/12/15/model-calibration-for-logistic-regression-in-rare-events-data CTR预估• 预测模型：预估广告是否被点击，是一个两类分类问题 • 特征：用户、广告、场景、媒体 • 性能评估：准确率/召回率、AUC 基础特征展示广告：在某场景下，通过某媒体向用户展示广告– 场景：当时场景，如何时何地，使用何种设备，使用什么浏览器等– 广告 - 广告主特征、广告自身的特征如campaign、创意、类型，是否重定向等– 媒体 - 媒体（网页、app等）的特征、广告位的特征等– 用户 - 用户画像、用户浏览历史、检索关键词等 特征工程• 特征离散化• 特征交叉 FM/FFM： 类别型 特征 OneHot编码后再组合 GBDT：用树的叶子节点索引作为样本特征 特征组合并特征离散化• 特征选择：嵌入到模型 预测模型• Logistic回归（LR）： – 特征工程很重要 – LR-FTRL： Google在2010年提出了一些理论基础，2013年给出了Paper，并且带有FTRL的实现伪代码。在此之后，FTRL才大规模应用在工业界。• FM(Factorization Machines) – Steffen Rendle于2010年提出Factorization Machines（FM），并发布开源工具libFM。凭借这单个模型，他在KDD Cup 2012上，取得Track1的第2名和Track2的第3名。本质上可看作是：高效特征交叉 + LR。• 深度学习DNN – 特征工程工作量少，但10^11 级别的原始广告特征、以及特征的稀疏性对DNN还是巨大挑战 CTR模型评估• 原则上分类性能评价指标均可用于CTR模型评估 – Logloss、PR、…• 最常用的评估指标：ROC曲线下的面积（Area Under Curve， AUC ） – 随机分类模型AUC为0.5 – 越接近1模型的效果越好当随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。 AUC• 当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变 ,AUC不变• CTR场景下，测试数据中的正负样本的分布可能随着时间变化而变化 CTR预估-Logistic回归（LR） Logistic回归 + L1正则由于CTR预估中特征维数非常高，我们希望得到稀疏解，利用L1正则，目标函数为 Google: Large scale LR model• Ad Click Prediction A view from the trenches– Google FTRL模型在点击率预估任务上的应用• FTRL是L1正则的LR，但对模型训练进行了优化：– 在线学习（Online Learning）– 模型稀疏• 很重要，因为CTR问题中通常特征维数很高（上亿）• 很稀疏（离散特征＋OneHot 编码） FTRL (Follow The Regulized Leader)• 在批处理模式下，L1正则化通常产生稀疏模型• 但Online中，每次权重向量的更新并不是沿着全局梯度进行下降，而是沿着某个样本的产生的梯度方向进行下降，整个寻优过程变得像是一个随机查找的过程，这样Online最优化求解即使采用L1正则化的方式，也很难产生稀疏解。• 在线稀疏模：FTRL – FTRL 综合了RDA和FOBOS，在L1范数或者其他非光滑的正则项下，能高效地得到稀疏解 FTRL• FTRL和SGD是等价的。 • FTRL工程实现技巧（如节省内存等）请见原始论文。 • Tensorflow支持FRTL：FtrlOptimizer 性能： L1-FOBOS、L1-RDA和L1-FTRL在一个小规模数据集（10^6 ）上的性能 参考文献[1] Convex function. http://en.wikipedia.org/wiki/Convex_function[2] Lagrange multiplier. http://en.wikipedia.org/wiki/Lagrange_multiplier[3] Karush–Kuhn–Tucker conditions. http://en.wikipedia.org/wiki/Karush-Kuhn-Tucker_conditions[4] 冯扬. 并行逻辑回归 . http://blog.sina.com.cn/s/blog_6cb8e53d0101oetv.html[5] Gradient. http://sv.wikipedia.org/wiki/Gradient[6] Subgradient. http://sv.wikipedia.org/wiki/Subgradient[7] Andrew Ng. CS229 Lecture notes. http://cs229.stanford.edu/notes/cs229-notes1.pdf[8] Stochastic Gradient Descent. http://en.wikipedia.org/wiki/Stochastic_gradient_descent[9] T. Hastie, R. Tibshirani &amp; J. Friedman. The Elements of Statistical Learning, Second Edition: Data Mining,Inference, and Prediction. Springer Series in Statistics. Springer, 2009 [10] John Langford, Lihong Li &amp; Tong Zhang. Sparse Online Learning via Truncated Gradient. Journal of Machine Learning Research, 2009[11] John Duchi &amp; Yoram Singer. Efficient Online and Batch Learning using Forward Backward Splitting. Journal ofMachine Learning Research, 2009[12] Lin Xiao. Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization. Journal of Machine Learning Research, 2010[13] Convex Set. http://en.wikipedia.org/wiki/Convex_set[14] H. Brendan McMahan &amp; M Streter. Adaptive Bound Optimization for Online Convex Optimization. In COLT,2010[15] H. Brendan McMahan. Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization. In AISTATS, 2011 [16] H. Brendan McMahan, Gary Holt, D. Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom Boulos, Jeremy Kubica, Ad Click Prediction: a View from the Trenches. In ACM SIGKDD, 2013[17] Martin A. Zinkevich, Markus Weimer, Alex Smola &amp; Lihong Li. Parallelized Stochastic Gradient Descent. In NIPS 2010 Facebook : GBDT+LR• Practical Lessons from Predicting Clicks on Ads at Facebook [1]• 用GBDT编码特征，然后再用LR做分类– GBDT可替代FM做特征编码– LR可用FTRL代替 [1] Xinran He et al. Practical Lessons from Predicting Clicks on Ads at Facebook, 2014. https://cloud.tencent.com/developer/article/1005052 动机• 用LR做CTR预估时，需做大量的特征工程 （非线性特征）– 连续特征离散化（+ One-Hot编码）– 特征进行二阶或者三阶的特征组合• 问题：– 连续变量切分点如何选取？– 离散化为多少份合理？– 选择哪些特征交叉？– 多少阶交叉，二阶，三阶或更多？• GBDT：一举解决了上面的问题– 确定切分点和切分数目不在是凭主观经验，而是根据信息增益/Gini指标– 每棵决策树从根节点到叶节点的路径，会经过不同的特征，此路径就是特征组合，而且包含了二阶，三阶甚至更多（所以GBDT提取特征时层数不用太深） 如上图所示： GBDT训练得到：第一棵树有3个叶子结点第二棵树有2个叶子节点 GBDT编码：对于一个输入样本点x，如果它在第一棵树最后落在其中的第3个叶子结点，在第二棵树里最后落在第1个叶子结点，则通过GBDT获得的新特征向量为[0, 0, 1, 1,0]，向量中的前三位对应第一棵树的3个叶子结点，后两位对应第二棵树的２个叶子结点 实现• xgboost：predict函数– predict(data, output_margin=False, ntree_limit=0, pred_leaf=False, pred_contribs=False, approx_contribs=False)• lightGBM: predict函数– predict(data, output_margin=False, ntree_limit=0, pred_leaf=False, pred_contribs=False, approx_contribs=False) xgb_feature = xgb.predict(dtv, pred_leaf = True) GBDT+FM• Kaggle 2014年竞赛：Criteo Display Advertising Challenge– https://www.kaggle.com/c/criteo-display-ad-challenge• Rank1解决方案：3 idiot’s FM （FFM的发明者） • Kaggle 2015年竞赛： Click-Through Rate Prediction – https://www.kaggle.com/c/avazu-ctr-prediction• Rank2解决方案： 为什么不直接用GBDT？ • 因为GBDT在线预测比较困难，而且训练时间复杂度高于LR。 • 所以实际中，可以离线训练GBDT，然后将该模型作为在线ETL的一部分。 Factorization Machines（FM）• Steffen Rendle于2010年提出Factorization Machines[1]，并发布开源工具libFM （http://www.libfm.org/ ）。– 凭借这单个模型，他在KDD Cup 2012上，取得Track1的第2名和Track2的第3名。• FM旨在解决稀疏数据的特征组合问题– Recall：LR为线性模型，需要输入足够强的特征（特征组合） [1] Steffen Rendle, Factorization Machines, Proceedings of the 10th IEEE International Conference on Data Mining (ICDM 2010), Sydney, Australia 数据稀疏• CTR预估中很多类别型特征：例国家、节日– 国家和节日为类别型特征 ，需要One Hot 编码 原始特征： OneHot编码： 特征组合某些特征经过关联之后，与label之间的相关性就会提高例：将国家于假日组合： “ USA”与“Thanksgiving” “China”与“Chinese New Year” 这样的关联特征，对用户的点击有着正向的影响来自“China”的用户很可能会在“Chinese New Year”有大量的浏览、购买行为，在“Thanksgiving”却不会有特别的消费行为。更多示例：• “化妆品”类商品与“女”性• “球类运动配件”的商品与“男”性 二阶多项式模型 FM 模型训练 Field-aware Factorization Machines（ＦＦＭ） FFM的二次交叉项 FFM实现• 作者提供C++版本实现： libffm– https://github.com/guestwalk/libffm 实验结果 DNN for CTR• Google: Deep wide DNN – Wide &amp; Deep Learning for Recommender Systems• FNN:– Deep Learning over Multi - field Categorical Data – A Case Study on User ResponsePrediction• PNN:– Product - based Neural Networks for User Response Prediction• NFM– Neural Factorization Machines for Sparse Predictive Analytics• Alibaba display ads: Deep interest network– Deep Interest Network for Click - Through Rate Prediction– Use attention - like network structure to model local activation of user interest to candidatead. 参考文献1、http://blog.csdn.net/lilyth_lilyth/article/details/480321192、http://www.cnblogs.com/Matrix_Yao/p/4773221.html3、http://www.herbrich.me/papers/adclicksfacebook.pdf4、https://www.kaggle.com/c/criteo-display-ad-challenge5、https://www.kaggle.com/c/avazu-ctr-prediction6、https://en.wikipedia.org/wiki/Demand-side_platform7、http://www.algo.uni-konstanz.de/members/rendle/pdf/Rendle2010FM.pdf8、http://www.cs.cmu.edu/~wcohen/10-605/2015-guest-lecture/FM.pdf9、http://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf10、https://github.com/guestwalk/libffm11、https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad12、http://openmp.org/wp/openmp-specifications/13、http://blog.csdn.net/gengshenghong/article/details/700870414、https://kaggle2.blob.core.windows.net/competitions/kddcup2012/2748/media/Opera.pdf]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>点击预估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LSTM]]></title>
    <url>%2F2019%2F01%2F21%2FLSTM%2F</url>
    <content type="text"><![CDATA[ＬＳＴＭ模型LSTM（Long Short-Term Memory）是长短期记忆网络，是一种时间递归神经网络，适合于处理和预测时间序列中间隔和延迟相对较长的重要事件。就是所谓的该记得会一直传递，不该记得就被“忘记”。 “记忆细胞”变得稍微复杂了一点 细胞状态细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传会很容易保持不变。 LSTM控制“细胞状态”的方式： 通过“门”让信息选择性通过，来去除或者增加信息到细胞状态。 包含一个SIGMOD神经元层和一个pointwise乘法操作。 SIGMOD层输出0到1之间的概率值，描述每个部分有多少量可以通过。0代表“不许任何量通过”，1就表示“允许任意量通过”。 遗忘门遗忘门（forget gate）顾名思义，是控制是否遗忘的，在LSTM中即以一定的概率控制是否遗忘上一层的隐藏细胞状态。遗忘门子结构如下图所示： 图中输入的有上一序列的隐藏状态h_{t−1}和本序列数据x_t，通过一个激活函数，一般情况下是SIGMOD，得到遗忘门的输出$f_t$。由于SIGMOD的输出$f_t$在[0,1]之间，因此这里的输出$f_t$代表了遗忘上一层隐藏细胞的概率。 数学表达式： $f(t)=\sigma(Wf h{t-1}+U_f x_t+b_f)$ 其中：$W_f、U_f、b_f$为线性关系的权重项和偏置项，σ为SIGMOD激活函数。 输入门输入门（input gate）负责处理当前序列位置的输入，它的子结构如下图： 从图中可以看到输入门由两部分组成，第一部分使用了sigmoid激活函数，输出为i_{(t)},第二部分使用了tanh激活函数，输出为c_{(t)}, 两者的结果后面会相乘再去更新细胞状态。 SIGMOD层决定什么值需要更新。 Tanh层创建一个新的候选值向量$c_{(t)}$ 第二步还是为状态更新做准备。 数学表达式： i{(t)} = \sigma(W_ih_{t-1} + U_ix_t + b_i)\tilde c{(t)} =tanh(W_ah_{t-1} + U_ax_t + b_a)其中$W_i,U_i,b_i,W_a,U_a,b_a$，为线性关系的权重项和偏置项，σ为SIGMOD激活函数。 更新细胞在研究LSTM输出门之前，我们要先看看LSTM之细胞状态。前面的遗忘门和输入门的结果都会作用于细胞状态 C(t)，我们来看看细胞如何从C(t−1)到C(t): 由图可知：细胞状态C_t由两部分组成；第一部分是C_{t−1}和遗忘门输出f_t的乘积，第二部分是输入门的i_t和\tilde c_{(t)}的乘积，总结为如下三点： 更新C_{(t−1)}为C_{(t)}。 把C_{(t−1)}和$f_{(t)}$相乘，丢弃掉我们确定需要丢弃的信息。 加上$i(t) * \tilde c_{(t)}$。最后得到新的候选值，根据我们决定更新每个状态的程度进行变化。 数学表达式： C_{(t)} = C_{(t-1)} \odot f{(t)} + i_{(t)} \odot \tilde c_{(t)}其中，⨀为Hadamard积. 输出门有了新的隐藏细胞状态C(t)，我们就可以来看输出门了，子结构如下： 从图中可以看出：隐藏状态h_t的更新由两个部分组成：第一部分是o_t，它是由上一序列的隐藏状态h_{t−1}和本序列的x_t，以及激活函数SIGMOD得到的，第二部分是由隐藏状态C_t和Tanh激活函数组成，即： 最开始先运行一个SIGMOD层来确定细胞状态的哪个部分将输出。 接着用tanh处理细胞状态（得到一个-1到1之间的值），再将它和SIGMOD门的输出相乘。输出我们确定输出的那部分值。 数学表达式： o_t=\sigma(W_o[h_{t-1},x_t]+b_o)h_t=o_t*tanh(C_t)总结 LSTM变体 增加peephole connection 让门层也会接受细胞状态的输入。 数学表达式： f_t=\sigma(W_f \cdot[C_{t-1}, h_{t-1},x_t]+b_f)i_t=\sigma(W_i \cdot[C_{t-1}, h_{t-1},x_t]+b_i)o_t=\sigma(W_o \cdot[C_{t-1}, h_{t-1},x_t]+b_o) 通过使用coupled忘记和输入门 之前是分开确定需要忘记和添加的信息，然后一同做出决定。 数学表达式： C_t=f_t * C_{t-1}+(1-f_t) * \tilde C_tGRUGatad Reacurrent Unit (GRU)，2014年提出。 将忘记门和输入门合成了一个单一的更新门 混合了细胞状态和隐藏状态 比标准的LSTM简单 数学表达式： z_t=\sigma(W_z \cdot [h_{t-1},x_t])r_t=\sigma(W_r \cdot [h_{t-1},x_t])\tilde h_t= tanh(W \cdot [r_t*h_{t-1},x_t])h_t=(1-z_t) * h_{t-1} + z_t * \tilde h_tLSTM总结 实现ＬＳＴＭimport tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data# 导入数据mnist = input_data.read_data_sets(&apos;MNIST_data&apos;, one_hot=True)# 超参数设置lr = 0.001 # 学习率epochs = 100000 # 最大训练次数batch_size = 128 # 小批量样本数n_inputs = 28 # Mnist数据输入维度 (img shape: 28*28)n_steps = 28 # time stepsn_hidden_units = 128 # 隐层神经元n_classes = 10 # MNIST 分类 (0-9 digits)# x y placeholderx = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) # [None, 28, 28]y = tf.placeholder(tf.float32, [None, n_classes]) # [None,10]# 对 weights biases 初始值的定义weights = &#123; # shape (28, 128) &apos;in&apos;: tf.Variable(tf.random_normal([n_inputs, n_hidden_units])), # shape (128, 10) &apos;out&apos;: tf.Variable(tf.random_normal([n_hidden_units, n_classes]))&#125;biases = &#123; # shape (128, ) &apos;in&apos;: tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ])), # shape (10, ) &apos;out&apos;: tf.Variable(tf.constant(0.1, shape=[n_classes, ]))&#125;def RNN(X, weights, biases): # input layer # 原始的 X 是 3 维数据, 我们需要把它变成 2 维数据才能使用 weights 的矩阵乘法 # X ==&gt; (128 batches * 28 steps, 28 inputs) X = tf.reshape(X, [-1, n_inputs]) # X_in = W*X + b X_in = tf.matmul(X, weights[&apos;in&apos;]) + biases[&apos;in&apos;] # X_in ==&gt; (128 batches, 28 steps, 128 hidden) 换回3维 X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_units]) # cell计算 # 使用 basic LSTM Cell. cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units, forget_bias=1.0, state_is_tuple=True) init_state = cell.zero_state(batch_size, dtype=tf.float32) # 初始化全零 state # 使用tf.nn.dynamic_rnn(cell, inputs)，我们要确定inputs的格式。 # tf.nn.dynamic_rnn中的time_major参数会针对不同inputs格式有不同的值。 # 如果inputs为（批次，步骤，输入）==&gt; time_major=False; # 如果inputs为（步骤，批次，输入）==&gt; time_major=True; outputs, final_state = tf.nn.dynamic_rnn(cell, X_in, initial_state=init_state, time_major=False) # 输出层 results = tf.matmul(final_state[1], weights[&apos;out&apos;]) + biases[&apos;out&apos;] return resultspred = RNN(x, weights, biases)cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))train_optimizer = tf.train.AdamOptimizer(lr).minimize(cost)correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) step = 0 while step * batch_size &lt; epochs: batch_xs, batch_ys = mnist.train.next_batch(batch_size) batch_xs = batch_xs.reshape([batch_size, n_steps, n_inputs]) _ = sess.run([train_optimizer], feed_dict=&#123;x: batch_xs, y: batch_ys&#125;) if step % 20 == 0: # 计算批次数据的准确率 acc = sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys&#125;) # Calculate batch loss loss = sess.run(cost, feed_dict=&#123;x: batch_xs, y: batch_ys&#125;) print(&quot;Iter &quot; + str(step * batch_size) + &quot;, Minibatch Loss= &quot; + \ &quot;&#123;:.6f&#125;&quot;.format(loss) + &quot;, Training Accuracy= &quot; + \ &quot;&#123;:.5f&#125;&quot;.format(acc)) step += 1 print(&quot; Finished!&quot;) # 计算准确率 for 128 mnist test images test_len = 128 test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_inputs)) test_label = mnist.test.labels[:test_len] print(&quot;Testing Accuracy:&quot;, sess.run(accuracy, feed_dict=&#123;x: test_data, y: test_label&#125;)) GRUimport tensorflow as tf# 导入 MINST 数据集from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&quot;MNIST_data&quot;, one_hot=True)n_input = 28 # MNIST data 输入 (img shape: 28*28)n_steps = 28 # timestepsn_hidden = 128 # hidden layer num of featuresn_classes = 10 # MNIST 列别 (0-9 ，一共10类)tf.reset_default_graph()# tf Graph inputx = tf.placeholder(&quot;float&quot;, [None, n_steps, n_input])y = tf.placeholder(&quot;float&quot;, [None, n_classes])x1 = tf.unstack(x, n_steps, 1)# grugru = tf.contrib.rnn.GRUCell(n_hidden)outputs = tf.contrib.rnn.static_rnn(gru, x1, dtype=tf.float32)pred = tf.contrib.layers.fully_connected(outputs[-1], n_classes, activation_fn=None)learning_rate = 0.001training_iters = 100000batch_size = 128display_step = 10# Define loss and optimizercost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)# Evaluate modelcorrect_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))# 启动sessionwith tf.Session() as sess: sess.run(tf.global_variables_initializer()) step = 1 # Keep training until reach max iterations while step * batch_size &lt; training_iters: batch_x, batch_y = mnist.train.next_batch(batch_size) # Reshape data to get 28 seq of 28 elements batch_x = batch_x.reshape((batch_size, n_steps, n_input)) # Run optimization op (backprop) sess.run(optimizer, feed_dict=&#123;x: batch_x, y: batch_y&#125;) if step % display_step == 0: # 计算批次数据的准确率 acc = sess.run(accuracy, feed_dict=&#123;x: batch_x, y: batch_y&#125;) # Calculate batch loss loss = sess.run(cost, feed_dict=&#123;x: batch_x, y: batch_y&#125;) print(&quot;Iter &quot; + str(step * batch_size) + &quot;, Minibatch Loss= &quot; + \ &quot;&#123;:.6f&#125;&quot;.format(loss) + &quot;, Training Accuracy= &quot; + \ &quot;&#123;:.5f&#125;&quot;.format(acc)) step += 1 print(&quot; Finished!&quot;) # 计算准确率 for 128 mnist test images test_len = 128 test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input)) test_label = mnist.test.labels[:test_len] print(&quot;Testing Accuracy:&quot;, sess.run(accuracy, feed_dict=&#123;x: test_data, y: test_label&#125;))]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DCGAN]]></title>
    <url>%2F2019%2F01%2F21%2FDCGAN%2F</url>
    <content type="text"><![CDATA[简介DCGAN即使用卷积网络的对抗网络，其原理和GAN一样，只是把CNN的卷积技术用于GAN模式的网络里，G（生成器）网在生成数据时，使用反卷积的重构技术来重构原始图片。D（判别器）网用卷积技术来识别图片特征，进而作出判别。 https://arxiv.org/abs/1511.06434 架构 判别器 生成器在DCGAN中，生成式模型G(z)使用一个比较特殊的深度卷积网络来实现，如下图所示： 反卷积从前面两幅图中可以看出，DCGAN的生成式模型G(z)中出现了上采样（upsampling）。 卷积神经网络的下采样很好理解，加入pooling层即可，然而这里的上采样要如何实现呢？ 这里，DCGAN通过“微步幅卷积”（fractionally-strided convolution）进行上采样。 假设有一个3×3的输入，希望输出的尺寸比这要大，那么可以把这个3×3的输入通过在像素之间插入0的方式来进行扩展，如下图所示。当扩展到7×7的尺寸后，再进行卷积，就可以得到尺寸比原来大的输出。 特点 调优 https://github.com/hindupuravinash/the-gan- 实现深度卷积神经网络生成Mnist手写数据集—-DCGAN 导入环境 import numpy as npimport tensorflow as tfimport matplotlib.pyplot as pltfrom tensorflow.examples.tutorials.mnist import input_data 数据准备与超参数设置 mnist = input_data.read_data_sets(&apos;data&apos;)# 定义参数batch_size = 64noise_size = 100epochs = 5n_samples = 25learning_rate = 0.001 数据处理 def get_inputs(noise_dim, image_height, image_width, image_depth): # 真实数据 inputs_real = tf.placeholder(tf.float32, [None, image_height, image_width, image_depth], name=&apos;inputs_real&apos;) # 噪声数据 inputs_noise = tf.placeholder(tf.float32, [None, noise_dim], name=&apos;inputs_noise&apos;) return inputs_real, inputs_noise 构建DCGAN网络结构 生成器 def get_generator(noise_img, output_dim, is_train=True, alpha=0.01): with tf.variable_scope(&quot;generator&quot;, reuse=(not is_train)): # 100 x 1 to 4 x 4 x 512 # 全连接层 layer1 = tf.layers.dense(noise_img, 4 * 4 * 512) layer1 = tf.reshape(layer1, [-1, 4, 4, 512]) # batch normalization layer1 = tf.layers.batch_normalization(layer1, training=is_train) # Leaky ReLU layer1 = tf.maximum(alpha * layer1, layer1) # dropout layer1 = tf.nn.dropout(layer1, keep_prob=0.8) # 4 x 4 x 512 to 7 x 7 x 256 layer2 = tf.layers.conv2d_transpose(layer1, 256, 4, strides=1, padding=&apos;valid&apos;) layer2 = tf.layers.batch_normalization(layer2, training=is_train) layer2 = tf.maximum(alpha * layer2, layer2) layer2 = tf.nn.dropout(layer2, keep_prob=0.8) # 7 x 7 256 to 14 x 14 x 128 layer3 = tf.layers.conv2d_transpose(layer2, 128, 3, strides=2, padding=&apos;same&apos;) layer3 = tf.layers.batch_normalization(layer3, training=is_train) layer3 = tf.maximum(alpha * layer3, layer3) layer3 = tf.nn.dropout(layer3, keep_prob=0.8) # 14 x 14 x 128 to 28 x 28 x 1 logits = tf.layers.conv2d_transpose(layer3, output_dim, 3, strides=2, padding=&apos;same&apos;) # MNIST原始数据集的像素范围在0-1，这里的生成图片范围为(-1,1) # 因此在训练时，记住要把MNIST像素范围进行resize outputs = tf.tanh(logits) return outputs 判别器 def get_discriminator(inputs_img, reuse=False, alpha=0.01): with tf.variable_scope(&quot;discriminator&quot;, reuse=reuse): # 28 x 28 x 1 to 14 x 14 x 128 # 第一层不加入BN layer1 = tf.layers.conv2d(inputs_img, 128, 3, strides=2, padding=&apos;same&apos;) layer1 = tf.maximum(alpha * layer1, layer1) layer1 = tf.nn.dropout(layer1, keep_prob=0.8) # 14 x 14 x 128 to 7 x 7 x 256 layer2 = tf.layers.conv2d(layer1, 256, 3, strides=2, padding=&apos;same&apos;) layer2 = tf.layers.batch_normalization(layer2, training=True) layer2 = tf.maximum(alpha * layer2, layer2) layer2 = tf.nn.dropout(layer2, keep_prob=0.8) # 7 x 7 x 256 to 4 x 4 x 512 layer3 = tf.layers.conv2d(layer2, 512, 3, strides=2, padding=&apos;same&apos;) layer3 = tf.layers.batch_normalization(layer3, training=True) layer3 = tf.maximum(alpha * layer3, layer3) layer3 = tf.nn.dropout(layer3, keep_prob=0.8) # 4 x 4 x 512 to 4*4*512 x 1 flatten = tf.reshape(layer3, (-1, 4 * 4 * 512)) logits = tf.layers.dense(flatten, 1) outputs = tf.sigmoid(logits) return logits, outputs 计算损失值 def get_loss(inputs_real, inputs_noise, image_depth, smooth=0.1): g_outputs = get_generator(inputs_noise, image_depth, is_train=True) d_logits_real, d_outputs_real = get_discriminator(inputs_real) d_logits_fake, d_outputs_fake = get_discriminator(g_outputs, reuse=True) # 计算Loss g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_outputs_fake) * (1 - smooth))) d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_outputs_real) * ( 1 - smooth))) d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_outputs_fake))) d_loss = tf.add(d_loss_real, d_loss_fake) return g_loss, d_loss 初始化优化器 def get_optimizer(g_loss, d_loss, learning_rate=0.001): train_vars = tf.trainable_variables() g_vars = [var for var in train_vars if var.name.startswith(&quot;generator&quot;)] d_vars = [var for var in train_vars if var.name.startswith(&quot;discriminator&quot;)] # Optimizer with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars) d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars) return g_opt, d_opt 显示图片 def plot_images(samples): fig, axes = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True, figsize=(7, 7)) for img, ax in zip(samples, axes.flatten()): ax.imshow(img.reshape((28, 28)), cmap=&apos;Greys_r&apos;) ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) fig.tight_layout(pad=0) plt.show()def show_generator_output(sess, n_images, inputs_noise, output_dim): noise_shape = inputs_noise.get_shape().as_list()[-1] # 生成噪声图片 examples_noise = np.random.uniform(-1, 1, size=[n_images, noise_shape]) samples = sess.run(get_generator(inputs_noise, output_dim, False), feed_dict=&#123;inputs_noise: examples_noise&#125;) result = np.squeeze(samples, -1) return result 开始训练 def train(noise_size, data_shape, batch_size, n_samples): # 存储loss losses = [] steps = 0 inputs_real, inputs_noise = get_inputs(noise_size, data_shape[1], data_shape[2], data_shape[3]) g_loss, d_loss = get_loss(inputs_real, inputs_noise, data_shape[-1]) print(&quot;FUNCTION READY!!&quot;) g_train_opt, d_train_opt = get_optimizer(g_loss, d_loss, learning_rate) print(&quot;TRAINING....&quot;) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # 迭代epoch for e in range(epochs): for batch_i in range(mnist.train.num_examples // batch_size): steps += 1 batch = mnist.train.next_batch(batch_size) batch_images = batch[0].reshape((batch_size, data_shape[1], data_shape[2], data_shape[3])) # scale to -1, 1 batch_images = batch_images * 2 - 1 # noise batch_noise = np.random.uniform(-1, 1, size=(batch_size, noise_size)) # run optimizer sess.run(g_train_opt, feed_dict=&#123;inputs_real: batch_images, inputs_noise: batch_noise&#125;) sess.run(d_train_opt, feed_dict=&#123;inputs_real: batch_images, inputs_noise: batch_noise&#125;) if steps % 101 == 0: train_loss_d = d_loss.eval(&#123;inputs_real: batch_images, inputs_noise: batch_noise&#125;) train_loss_g = g_loss.eval(&#123;inputs_real: batch_images, inputs_noise: batch_noise&#125;) losses.append((train_loss_d, train_loss_g)) print(&quot;Epoch &#123;&#125;/&#123;&#125;....&quot;.format(e + 1, epochs), &quot;Discriminator Loss: &#123;:.4f&#125;....&quot;.format(train_loss_d), &quot;Generator Loss: &#123;:.4f&#125;....&quot;.format(train_loss_g)) if e % 1 == 0: # 显示图片 samples = show_generator_output(sess, n_samples, inputs_noise, data_shape[-1]) plot_images(samples)with tf.Graph().as_default(): train(noise_size, [-1, 28, 28, 1], batch_size, n_samples) print(&quot;OPTIMIZER END!!&quot;)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>DCGAN</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PG3 & IRL]]></title>
    <url>%2F2019%2F01%2F20%2FPG3%E5%92%8CIRL%2F</url>
    <content type="text"><![CDATA[参考资料Reinforcement Learning: An Introductionhttp://incompleteideas.net/book/the-book-2nd.htmlDave Silver强化学习课程http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html ＰＧ AC ＰＰＯ近端策略优化（Proximal Policy Optimization，PPO) https://spinningup.openai.com/en/latest/algorithms/ppo.html PPO的优点： VGP:在线采样，在线更新，采样完成的数据用来更新一次，因为更新过一次之后，策略就发生了改变（策略评估只能使用当下的策略生成数据），样本利用率低，效率低。PPO:在线采样，离线更新，采样完后的数据可以用来多次更新网络，样本利用率高，效率高。如何用之前的策略生成的数据评估当下的策略，重要性采样！ 重要性采样因为： 所以期望： 方差： 可见： 重要性采样（提高数据利用率）+约束策略变化幅度（减少方差）： PPO: TRPO: PPO-Clip 当优势值为正： 当优势值为负： ＩＴ正向强化学习中，所有的agent都是从头学习，其劣势有：1：需要由专家给出合理的奖励函数，很难对复杂的动作给出一个合适的奖励动作，例如飞机特技表演。2：比较耗时，需要训练成百上千个回合，并且有很多情况下，真实环境不具备这样的训练条件（不安全，价格昂贵），例如手术机器人学习动手术。怎么办？由专家进行演示，让学习者进行模仿模仿学习（Imitation Learning）： 1：直接法：直接学习策略监督式学习：行为克隆 Behavior Cloning2：间接法：学习奖励机制逆向强化学习（Inverse reinforcement learning） 直接法监督式学习：行为克隆+Data Augmentation 间接法学习奖励机制。 逆向强化学习IRL,从专家轨迹中推测专家这样做的动机。 Max-margin 分类器（SVM） http://www.andrew.cmu.edu/course/10-703/slides/Lecture_Imitation_supervised-Nov-5-2018.pdf Apprenticeship Learning学徒学习]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>PG3</tag>
        <tag>IRL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GAN理论]]></title>
    <url>%2F2019%2F01%2F20%2FGAN%E7%90%86%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[简介对抗神经网络其实是两个网络的组合，可以理解为一个网络生成模拟数据，另一个网络判断生成的数据是真实的还是模拟的。生成模拟数据的网络要不断优化自己让判别的网络判断不出来，判别的网络也要优化自己让自己判断得更准确。二者关系形成对抗，因此叫对抗生成神经网络。 GAN由generator（生成式模型）和discriminator（判别式模型）两部分构成。 $\bullet$ generator：主要是从训练数据中产生相同分布的samples，对于输入x，类别标签y，在生成式模型中估计其联合概率分布（两个及以上随机变量组成的随机向量的概率分布）。 $\bullet$ discriminator：判断输入是真实数据还是generator生成的数据，即估计样本属于某类的条件概率分布。它采用传统的监督学习的方法。 基本结构生成器生成式模型又叫生成器。它先用一个随机编码向量来输出一个模拟样本。 一般的生成模型, 必须先初始化一个“假设分布”，即后验分布， 通过各种抽样方法抽样这个后验分布，就能知道这个分布与真实分布之间究竟有多大差异。这里的差异就要通过构造损失函数(loss function)来估算。 判别器判别式模型又叫判别器。它的输入是一个样本（可以是真实样本也可以是模拟样本），输出一个判断该样本是真样本还是模拟样本（假样本）的结果 总结：判别器的目标是区分真假样本，生成器的目标是让判别器区分不出真假样本，两者目标相反，存在对抗。 我们可以把生成模型看作一个伪装者，而把判别模型看成一个警察。生成模型通过不断地学习来提高自己的伪装能力，从而使得生成出来的数据能够更好地“欺骗”判别模型。而判别模型则通过不断的训练来提高自己的判别能力，能够更准确地判断出数据的来源。GAN就是这样一个不断对抗的网络。 $\bullet$ 生成模型以随机变量作为输入，其输出是对真实数据分布的一个估计。$\bullet$ 生成数据和真实数据的采样都由判别模型进行判别，并给出真假性的判断和当前的损失。$\bullet$ 利用反向传播，GAN对生成模型和判别模型进行交替优化。 例子假设数据的概率分布为M，但是我们不知道具体的分布和构造是什么样的，就好像是一个黑盒子。为了了解这个黑盒子，我们就可以构建一个对抗生成网络： $\bullet$ 生成模型G：使用一种我们完全知道的概率分布N来不断学习成为我们不知道的概率分布M.$\bullet$ 判别模型D：用来判别这个不断学习的概率是我们知道的概率分布N还是我们不知道的概率分布M。 https://arxiv.org/abs/1406.2661 模型训练训练两个模型的方法：单独交替迭代训练判别模型： 希望真样本集尽可能输出1，假样本集输出0。对于判别网络，此时问题转换成一个有监督的二分类问题，直接送到神经网络模型中训练。生成网络：目的是生成尽可能逼真的样本。在训练生成网络的时候，需要联合判别网络才能达到训练的目的。 总结：首先固定G，单独训练D，为了让D得到充分训练，有的时候要迭代多次。D训练完毕后，固定D，训练G，如此循环。训练的方式是反向传播算法。 数学推导符号定义 $P_{data}(x)$：真实数据的分布 $P_z(Z)$：噪声数据 $P_g(x)$：生成模型生成的数据分布 D(X)：判别器 G(x)：生成器 定义生成器和判别器`E_{x \sim P_{data}}(x) \cdot logD(x) 由上式可知：当x \sim P_{data}(x) ,D(x)=1的时，E_{x \sim P_{data}}(x)取得最大值。 `E_{x \sim P_{z}}(z) \cdot log(1-D(G(z))) 由上式可知：当x \sim P_{z}(z) , D(G(z))=0时，E_{x \sim P_{z}}(z)取得最大值。 所以为了我们的判别模型越来越好，能力越来越强大，定义目标函数为： $V(G,D)= logD(x) + log(1-D(G(z)))$ 要使判别模型取得最好，所以需要使V(G,D)V(G,D)取得最大，即： $D = agrmax_DV(G,D)$ 当判别模型最好的时候，最好的生成模型就是目标函数取得最小的时候： $G=argmin_G(aggmax_D(V(G, D)))$ 所以经过这一系列的讨论，这个问题就变成了最大最小的问题，即： `min_Gmax_DV(G, D)=E_{x \sim P_{data}}(x) \cdot logD(x)+ E_{x \sim P_{z}}(z) \cdot log(1-D(G(z))) 最优判别模型最终的目标函数： `V(G,D)= \int_x P_{data}(x) \cdot logD(x) + P_g(x)log(1-D(G(z))) d(x) 令：$V(G,D)=f(y), P_{data}(x)=a, P_g(x)=b$ 所以：$f(y)=alogy+blog(1-y)$ 因为: $a+b \ne 0$ 所以最大值：$\frac{a}{a+b}$ 最后，我们得到的最优判别模型就是： `D(x)=\frac{P_{data}(X)}{P_{data}(X)+P_g(x)} 由于生成对抗网络的目的是：得到生成模型可以生成非常逼真的数据，也就是说是和真实数据的分布是一样的。因此最优的判别模型的输出为： `D(x)=\frac{P_{data}}{P_{data}+P_g}=\frac12 其中：P_g和P_{data}的数据分布是一样的。 也就是说当D输出为0.5时，说明鉴别模型已经完全分不清真实数据和GAN生成的数据了，此时就是得到了最优生成模型了。 特点优点： $\bullet$ 模型优化只用到了反向传播，而不需要马尔科夫链。$\bullet$ 训练时不需要对隐变量做推断。$\bullet$ 理论上，只要是可微分函数都能用于构建生成模型G和判别模型D，因而能够与深度神经网络结合–&gt;深度产生式模型。$\bullet$ 生成模型G的参数更新不是直接来自于数据样本，而是使用来自判别模型D的反向传播梯度。 缺点： $\bullet$ 可解释性差，生成模型的分布没有显示的表达。它只是一个黑盒子一样的映射函数：输入是一个随机变量，输出是我们想要的一个数据分布。$\bullet$ 比较难训练，生成模型D和判别模型G之间需要很好的同步。例如，在实际中我们常常需要 D 更新 K次， G 才能更新 1 次，如果没有很好地平衡这两个部件的优化，那么G最后就极大可能会坍缩到一个鞍点。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据集]]></title>
    <url>%2F2019%2F01%2F20%2F%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
    <content type="text"><![CDATA[一般数据集1、Kaggle：一个包含各种外部贡献数据集的数据科学网站。你可以在其主列表中找到各种合适的数据集，从拉面评级到篮球数据，甚至是西雅图宠物许可证，应有尽有。 https://www.kaggle.com/ 2、UCI 机器学习库：网络上最古老的数据集源之一，是寻找有趣的数据集的第一站。虽然这里的数据集是用户贡献的，因此清洁度不一，但绝大多数都是干净的。你可以直接从 UCI 机器学习库下载数据，无需注册。 http://mlr.cs.umass.edu/ml/ 政府公开数据集3、Data.gov：该网站可以从多个美国政府机构下载数据。数据范围从政府预算到学校绩效分数。但请注意：大部分数据有待进一步研究。 https://www.data.gov/ 4、食物环境地图集：包含当地食物选择如何影响美国饮食的数据。 https://catalog.data.gov/dataset/food-environment-atlas-f4a22 5、学校系统财务：对美国学校系统财务状况的调查。 https://catalog.data.gov/dataset/annual-survey-of-school-system-finances 6、慢性病数据：美国各地区慢性病指标数据。 https://catalog.data.gov/dataset/u-s-chronic-disease-indicators-cdi-e50c9 7、美国国家教育统计中心：来自美国和世界各地的教育机构和教育人口统计数据。 https://nces.ed.gov/ 8、英国数据服务：英国最大的社会、经济和人口数据集。 https://www.ukdataservice.ac.uk/ 9、Data USA：美国公共数据的全面可视化。 http://datausa.io/ 金融与经济10、Quandl：经济和金融数据很好的数据源，有助于建立预测经济指标或股票价格模型。 https://www.quandl.com/ 11、世界银行开放数据：涵盖全球人口统计数据和大量经济和发展指标的数据集。 https://data.worldbank.org/ 12、国际货币基金组织数据：国际货币基金组织公布的有关国际金融、债务利率、外汇储备、商品价格和投资的数据。 https://www.imf.org/en/Data 13、金融时报市场数据：来自世界各地的金融市场最新信息，包括股票价格指数、商品和外汇。 https://markets.ft.com/data/ 14、谷歌趋势：检查和分析世界各地的互联网搜索活动和热门新闻报道的数据。 https://trends.google.com/trends/?q=google&amp;ctab=0&amp;geo=all&amp;date=all&amp;sort=0 15、美国经济协会（AEA）：寻找美国宏观经济数据的良好来源。 https://www.aeaweb.org/resources/data/us-macro-regional 机器学习数据集16、Labelme：带图像标注的大型数据集。 http://labelme.csail.mit.edu/Release3.0/browserTools/php/dataset.php 17、ImageNet：业界最新算法图像数据集。根据 WordNet 层次结构进行组织，其中层次结构的每个节点由数百和数千个图像描述。 http://image-net.org/ 18、LSUN：有众多辅助任务的场景理解（房间布局估计、特点预测等） http://lsun.cs.princeton.edu/2016/ 19、MS COCO：通用图像理解和字幕。 http://mscoco.org/ 20、COIL100：100 个不同的物体，在 360 度旋转的每个角度成像。 http://www1.cs.columbia.edu/CAVE/software/softlib/coil-100.php 21、视觉基因组：非常详细的视觉知识库，带有~100K 图像的字幕。 http://visualgenome.org/ 22、谷歌的开放图像：在知识共享版权下的 900 万个图像网址集合，“超过 6000 个类别标签注释”。 https://ai.googleblog.com/2016/09/introducing-open-images-dataset.html 23、Labelled Faces in the Wild：13,000 张人脸标记图像，用于开发人脸识别应用程序。 http://vis-www.cs.umass.edu/lfw/ 24、斯坦福狗数据集：包含 20,580 张图片和 120 种不同的狗品种。 http://vision.stanford.edu/aditya86/ImageNetDogs/ 25、室内场景识别：一种非常特殊的数据集，因为大多数场景识别模型都最好建立在“室外”，这个数据集非常实用。包含 67 个室内类别，总共 15620 张图像。 http://web.mit.edu/torralba/www/indoor.html 情绪分析26、多域情绪分析数据集：一个有点老旧的数据集，其中包含来自亚马逊的产品评论。 http://www.cs.jhu.edu/~mdredze/datasets/sentiment/ 27、IMDB 评论：一个较旧的，相对较小的二元情绪分类数据集，包含 25,000 个电影评论。 http://ai.stanford.edu/~amaas/data/sentiment/ 28、斯坦福情绪树库：带有情感注释的标准情绪数据集。 http://nlp.stanford.edu/sentiment/code.html 29、Sentiment140：一个流行的数据集，使用 160,000 条预先删除表情符号的推文。 http://help.sentiment140.com/for-students/ 30、Twitter 美国航空公司情绪：2015 年 2 月美国航空公司的 Twitter 数据，分类为正面、负面和中性推文。 https://www.kaggle.com/crowdflower/twitter-airline-sentiment 自然语言处理31、安然数据集：来自安然高级管理层的电子邮件数据，以文件夹形式分类存放。 https://www.cs.cmu.edu/~./enron/ 32、亚马逊评论：包含亚马逊 18 年来约 3500 万条评论。数据包括产品和用户信息、评级和明文审核。 https://snap.stanford.edu/data/web-Amazon.html 33、Google Books Ngrams：Google 图书中的一系列文字。 https://aws.amazon.com/datasets/google-books-ngrams/ 34、Blogger Corpus：收集了来自 blogger.com 的 681288 篇博文。每个博客至少包含 200 个常用英语单词。 http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm 35、维基百科链接数据：维基百科全文。该数据集包含来自 400 多万篇文章的近 19 亿个单词。你可以按段落、短语或段落本身的一部分进行搜索。 https://code.google.com/archive/p/wiki-links/downloads 36、Gutenberg 电子书列表：Project Gutenberg 的电子书注释列表。 http://www.gutenberg.org/wiki/Gutenberg:Offline_Catalogs 37、加拿大议会议事录：来自第 36 届加拿大议会记录的 130 万对文本。 http://www.isi.edu/natural-language/download/hansard/ 38、Jeopardy：来自有奖竞猜节目 Jeopardy 的超过 200,000 个问题归档。 https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/ 39、英语短信垃圾邮件集：由 5574 条英文短信垃圾邮件组成的数据集。 http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ 40、Yelp 评论：Yelp 发布的一个开放数据集，包含超过 500 万条评论。 https://www.yelp.com/dataset 41、UCI 垃圾邮件集：一个大型垃圾邮件数据集，对垃圾邮件过滤非常有用。 https://archive.ics.uci.edu/ml/datasets/Spambase 更详细列表： https://gengo.ai/datasets/the-best-25-datasets-for-natural-language-processing/ 自动驾驶42、Berkeley DeepDrive BDD100k：目前是自动驾驶 AI 的最大数据集。包含超过 100000 个视频，包括一天中不同时段和天气条件下超过 1100 小时的驾驶体验。带注释的图像来自纽约和旧金山地区。 http://bdd-data.berkeley.edu/ 43、百度 Apolloscapes：大型数据集，定义了 26 种不同的语义项目，如汽车、自行车、行人、建筑物、路灯等。 http://apolloscape.auto/ 44、Comma.ai：超过 7 小时的高速公路驾驶数据。细节包括汽车的速度、加速度、转向角和 GPS 坐标。 https://archive.org/details/comma-dataset 45、牛津的机器人汽车：在英国牛津的同一条路线重复行驶 100 多次、耗时一年多收集的数据集。该数据集包含天气、交通和行人的不同组合，以及建筑和道路工程等长期变化。 http://robotcar-dataset.robots.ox.ac.uk/ 46、城市景观数据集：一个大型数据集，记录 50 个不同城市的城市街景。 https://www.cityscapes-dataset.com/ 47、CSSAD 数据集：此数据集对于自动驾驶车辆的感知和导航非常有用。但该数据集严重偏向发达国家的道路情况。 http://aplicaciones.cimat.mx/Personal/jbhayet/ccsad-dataset 48、KUL 比利时交通标志数据集：比利时法兰德斯地区数以千计的物理交通标志，有超过 10000 多个交通标志注释。 http://www.vision.ee.ethz.ch/~timofter/traffic_signs/ 49、麻省理工学院实验室：在 AgeLab 收集的 1000 多个小时多传感器驾驶数据集的样本。 http://lexfridman.com/automated-synchronization-of-driving-data-video-audio-telemetry-accelerometer/ 50、LISA：智能和安全汽车实验室，加州大学圣地亚哥分校数据集：该数据集包括交通标志、车辆检测、交通信号灯和轨迹模式。 http://cvrr.ucsd.edu/LISA/datasets.html]]></content>
      <categories>
        <category>数据集</category>
      </categories>
      <tags>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[21单元语法]]></title>
    <url>%2F2019%2F01%2F20%2F21%E5%8D%95%E5%85%83%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[～そうだ＜征兆、推测＞1、征兆接续：V-R（第一连用型）+ そうだ 样态助动词，表示征兆，是说话人对即将发生的动作，变化的征兆进行的描述，一般是说话人通过自身的感官判断或觉察到的。(主観)常与｢今にも｣连用。变形之后按二类形容词活用。 ✿汉语：就要~了、快要~了 1.今にも雨が降りそうです。 2.寒くて死にそうだ。3.ポケットから財布が落ちそうだよ。(2002年真题) 2、推测接续： $\bullet$ A1-词干(去い) + そうだ $\bullet$ A2-词干 + そうだ $\bullet$ VーR + そうだ 表示推断、猜测、发生的可能性，是说话人根据事物的外表、经验等判断某事态很有可能发生或某事物具有某性质。变形之后按二类形容词活用。 ✿汉语：看上去~；看起来~；好像~★：いい・ない: よさそうだ・なさそうだ 1.山田さんはいつも難しそうな本を読んでいる。(2006年真题) 2.石田さんは忙しそうだから、手伝おう。(2003年真题)3.あの様子では二人はもうすぐ結婚しそうです。 ３、そうだ的否定形式Vそうだ的否定为：｢Vそうにもない｣ 口语中常用｢そうもない｣或｢そうにない｣。即省略「に」或「も」表示发生某动作的可能性极小。 1.この本は売れそうもない。2.一人の力だけでは、とうていできそうにもない。 Aそうだ的否定为： $\bullet$ 「そうではない」 $\bullet$ 「A1-くなさそうだ」 $\bullet$ 「A2- ではなさそうだ」 1.最近、授業は少ないから、忙しそうではない。 最近、授業は少ないから、忙しくなさそうだ。2.彼は一週間以上も病気だから、体はあまり元気そうではない。 彼は一週間以上も病気だから、体はあまり元気ではなさそうだ。 4、そうだ修饰名词或动词修饰名词时：A/V +そう+な+ N修饰动词时：Aそう+に+ V 1.彼は悲しそうな顔をしている。2.彼は楽しそうに笑った。3.李さんは元気そうな声で話してくれた。4.雨が降りそうな天気だ。 授受动词1、あげる（自谦）：さしあげる接续：（赠与者）N1は∕が＋（接受者）N2に＋（所赠物品）N3を＋あげる。 当接受者的身份、地位、年龄高于赠与者时：あげるーさしあげる 先生が大学をおやめになる日、みんなで先生にアルバムをさしあげました。 2、くれるーくださる接续：（赠与者）N1は∕が＋（我或我这一方的人）に＋（所赠物品）N2を＋くれる。 当赠与者的身份、地位、年龄高于接受者时：くれるーくださる 先輩が妹にコンサートのチケットをくださいました。 3、もらう－いただく接续：（接受者）N1は＋（赠与者）N2に∕から＋（所得物品）N3を＋もらう。 当赠与者的身份、地位、年龄高于接受者时：もらう－いただく わたしは社長に会社のぺんをいただきました。 補助動詞・行为的授受$\bullet$ ～てあげる $\Rightarrow$ ～てさしあげる（謙譲語） $\bullet$ ～てもらう $\Rightarrow$ ～ていただく （謙譲語） $\bullet$ ～てくれる $\Rightarrow$ ～てくださる （尊敬語） ①Aは Bに Vてあげる Vて やる Vてさしあげる 表示A为B做某事。A是授予者，用「は」提示；(授予者为第一人时常省略）B是接受者，用「に」提示。 1.これ、貸してあげるよ。(2007年真题)2.この写真は家族にもぜひ見せてやりたい。3.吉田先生を駅まで車で送って差し上げた。 「～てやる」用于AB之间关系亲近，随意，或B身份、地位低于A时。「～てあげる」是「～てやる」的客气说法，「～てさしあげる」则比「～てあげる」更客气，常常用来叙述B是A的尊长时的授受关系。 ★：另外「～てあげる」会使行为接受者觉得这是对方施恩于自己而感到不快，用时应注意。 特殊接续： $\bullet$ 彼女の手を取ってあげる $\bullet$ 彼女の荷物を持ってあげる $\bullet$ 彼女を手伝ってあげる ②AはBに(から)Vてもらう Vていただく 表示A接受B所做的事。A是接受者，用「は」提示。B是授予者，用「に」或「から」提示。B为A所做的事用「て」前面的动词表示。★：「~ていただく」是「~てもらう」的谦语， 一般用于B比A身份高，或B是A所尊敬的人。 1.本田先生に貸していただいた本を家に忘れた。(2012年真题)2.この書類を広田さんに渡しておいてもらえないか。(2012年真题) ③Aは (私に) Vてくれる Vてくださる 表示A为我（我们，我方人员）做某事。与①相反。Ｂ在句中往往省略。★：「 Vていただく」与「 Vてくださる」的区别在于：前者常带有“该动作是受益者要求对方进行的” 1.友達がたくさん来てくれた。2.そちらの方が私の荷物を持ってくださいました。(2008年真题)3.友達が掃除を手伝ってくれた。(2006年真题) 变形总结 原型 授受敬语 行为的授受 行为的授受敬语 あげる さしあげる てあげる てさしあげる もらう いただく てもらう ていただく くれる くださる てくれる てくださる]]></content>
      <categories>
        <category>日语</category>
      </categories>
      <tags>
        <tag>语法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PG和TD3]]></title>
    <url>%2F2019%2F01%2F18%2FPG%E5%92%8CTD3%2F</url>
    <content type="text"><![CDATA[TD3TD3 = Twin Delayed DDPG：三点改进： 改进１Twin:有两个Q值预测网络，使用输出Q值较小的那个用作计算TD error的目标值； Double DQN: Double q learning(Q值来自于神经网络): Clipped Double Q-learning algorithm: 改进２Delayed：更新策略的频率要小于更新Q值，即训练actor网络的次数要小于训练critic网络； 在值网络估计不准确的情况下（TD error很大），更新策略会引发 在更新critic网络d次之后再更新actor网络 改进３目标策略平滑：Idea:相似的动作在同一个状态下的Q值也相似 Trick: 过程 效果 TD3 vs DDPG 参数设计 PG一个特定的回合内，其生成的轨迹概率轨迹: 概率： 重要性采样比率: 梯度公式： *\nabla_{\theta}logP(\tau|\theta)=\nabla \sum_{t=0}^T log\pi_{\theta}(a_t|s_t) 带入求导： 又： 所以： 所以： \nabla_{\theta}J(\pi_\theta)=\nabla E_{\tau\sim\pi_\theta} [R(\tau)] =\nabla_\theta \int_{\tau\sim\pi_\theta} P(\tau|\theta)R(\tau) = \int_{\tau\sim\pi_\theta} \nabla_\theta P(\tau|\theta)R(\tau) = \int_{\tau\sim\pi_\theta} P(\tau|\theta)\nabla_\theta logP(\tau|\theta)R(\tau) = E_{\tau\sim\pi_\theta}[\nabla_\theta logP(\tau|\theta)R(\tau)] = E_{a_t\sim\pi_\theta}[\nabla_\theta \sum_{t=0}^T log\pi_\theta(a_t|s_t)R(\tau)] 过程 蒙特卡洛估计方差太大，见下图：使用神经网络来估计Q值 从上图看出负的噪声影响很大，怎么办呢？ 可以增加一个b值补偿 推导： 方差公式和梯度公式： 梯度公式带入方差公式： 求导： 所以：]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>PG</tag>
        <tag>TD3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNN理论]]></title>
    <url>%2F2019%2F01%2F17%2FRNN%E7%90%86%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[循环神经网络（RNN）是一类神经网络，包括一层内的加权连接，与传统前馈神经网络相比，加权连接仅反馈到后续层。因为RNN包含循环，所以RNN就可以在处理输入信息的时候同时储存信息。这种记忆使得RNN非常适合处理必须考虑事先输入的任务（比如时序数据）。所以循环神经网络在自然语言处理领域非常适合。 传统神经网络（包含CNN），输入和输出都是互相独立的。RNN引入了“记忆”的概念 x：输入层的值U：输入层到隐层的权重参数s：隐层的值v：隐层到输出层的权重参数o：输出层的值W：递归神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重参数W就是隐藏层上一次的值作为这一次的输入的权重。 关键点：$St$的值不仅仅取决于$X_t$，还取决于$S{t−1}$(就是上一状态的隐层的值) 循环神经网络的计算公式： O_t=f(V \cdot S_t) \quad (1)输出层的计算公式，由于输出层是一个全连接层，所以说它每个节点都和隐层的节点相连。V是输出层的权重参数，f是激活函数。 S_t=f(U \cdot X_t+W \cdot S_{t-1}) \quad (2)隐层的计算公式，它是一个循环层，U是输入x的权重参数，W是上一次的值$S_{t−1}$作为这一次输入的权重参数，f是激活函数。 总结：从上面的公式中，我们可以看出，循环层和全连接层的区别就是循环层多了一个权重参数w。 扩展：如果反复的把（1）式带入 （2）式： ${O}_t=f(V\cdot{S}_t)$ `= V \cdot f(U \cdot X_t + W \cdot S_{t-1}) `= V \cdot f(U \cdot X_t+W \cdot f(U \cdot X_{t-1}+W \cdot S_{t-2})) `= V \cdot f(U \cdot X_t+W \cdot f(U \cdot X_{t-1}+W \cdot f(U \cdot X_{t-2}+W \cdot S_{t-3}))) `= V \cdot f(U \cdot X_t+W \cdot f(U \cdot X_{t-1}+W \cdot f(U \cdot X_{t-2}+W \cdot f(U \cdot X_{t-3}+…)))) 总结：从上面可以看出，递归神经网络的输出值$ot$，是受前面几次输入值$X_t、X{t−1}、X{t−2}、X{t−3}…$影响的，这也就是为什么递归神经网络可以往前看任意多个输入值的原因。 双向递归神经网络 从上图可以看出，双向递归神经网络的隐层是需要保持两个值： A：参与正向计算 A′：参与反向计算 所以$y_2$的值就取决于$A_2$和$A′_2$。计算方法： y_2=f(V \cdot A_2+V’ \cdot A_2’)$A_2和A_2′$则分别计算： A_2 = f(W \cdot A_1+U \cdot X_2)A_2’=f(W’ \cdot A_3’+U’ \cdot X_2)总结： 正向计算时：隐层的值S_t和S_{t−1}有关。 反向计算时：隐层的值S′_t和S′_{t−1}有关。 最终的输出取决于正向和反向计算的和。 扩展：我们仿照（1）和（2）那种方式： O_t =f(V \cdot S_t+V’ \cdot S_t’)S_t =f(U \cdot X_t+W \cdot S_{t-1})S_t’=f(U’ \cdot X_t+W’ \cdot S_{t+1}’)注意：从上面三个公式我们可以看到，正向计算和反向计算不共享权重，也就是说U和U’、W和W’、V和V’都是不同的权重矩阵。 深度递归神经网络 我们把第ii个隐层的值表示为$S_t^{(i)}、S_t’^{(i)}$ ,则深度递归神经网络的计算方式就可以表示为： {O}_t=f \cdot (V^{(i)} \cdot S_t^{(i)}+V’^{(i)} \cdot S_t’^{(i)})S_t^{(i)}=f(U^{(i)}\cdot S_t^{(i-1)}+W^{(i)}\cdot S_{t-1})S_t’^{(i)}=f(U’^{(i)}\cdot S_t’^{(i-1)}+W’^{(i)}\cdot S_{t+1}’)···S_t^{(1)}=f(U^{(1)} \cdot X_t+W^{(1)}\cdot S_{t-1})S_t’^{(1)}=f(U’^{(1)}\cdot X_t+W’^{(1)}\cdot S_{t+1}’)总结 从上图我们可以总结出： one to one：一个输入（单一标签）对应一个输出（单一标签） one to many：一个输入对应多个输出，即这个架构多用于图片的对象识别，即输入一个图片，输出一个文本序列。 many to one： 多个输入对应一个输出，多用于文本分类或视频分类，即输入一段文本或视频片段，输出类别。 many to many：这种结构广泛的用于机器翻译，输入一个文本，输出另一种语言的文本。 many to many：这种广泛的用于序列标注。 在众多的深度学习网络中，RNN由于能够接收序列输入，也能得到序列输出，在自然语言处理中取得了巨大的成功，并得到广泛的应用。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度下降]]></title>
    <url>%2F2019%2F01%2F17%2F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%2F</url>
    <content type="text"><![CDATA[梯度下降方法用负梯度作搜索方向，即令$\bigtriangleup x=-\bigtriangledown f(x)$, 是一种自然的选择。相应的方法就称梯度方法或者梯度下降方法。 梯度下降算法的概念梯度下降算法是一个被广泛使用的优化算法, 它可以用于寻找最小化成本函数的参数值. 也就是说: 当函数$ J(\theta)$取得最小值时, 求所对应的自变量θ的过程， 此处θ就是机器要学习的参数，$J(\theta)$就是用于参数估计的成本函数, 是关于θ的函数. 梯度下降的基本步骤梯度下降法的计算过程就是沿梯度下降的方向求解极小值（也可以沿梯度上升方向求解极大值） 给定 初始点:$x \in dom f$ 重复进行： $\bigtriangleup x :=-\bigtriangledown f(x)$ 直线搜索。通过精确或回溯直线搜索方法确实步长t. 修改 :$x =x+t\bigtriangleup x$ 直到：满足停止准则。 换种方式： 对成本函数进行微分, 得到其在给定点的梯度. 梯度的正负指示了成本函数值的上升或下降:$Δ(\theta)=\frac{∂J(\theta)}{∂\theta}$ 选择使成本函数值减小的方向, 即梯度负方向, 乘以学习率为 α 计算得参数的更新量, 并更新参数:$\theta=\theta−αΔ(\theta)$ 重复以上步骤, 直到取得最小的成本 批量梯度下降法（Batch Gradient Descent） 批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新，这个方法对应于线性回归的梯度下降算法，也就是说线性回归的梯度下降算法就是批量梯度下降法。 具体实现过程： 假设函数:h_\theta = \sum_{i=1}^n\theta_ix_i 成本函数:J(\theta)=\frac{1}{2m} \sum_{i=1}^n(h_\theta(x_i)-y_i)^2 对成本函数进行求偏导：对每一个参数\theta_j进行分别求偏导，得出各自的梯度。\frac{\partial J(\theta)}{\partial \theta}=-\frac1 m \sum_{i=1}^n(y_i-h_\theta(x_i))x_j^i 每个参数都按照梯度的负方向进行更新: \theta_j=\theta_j+\frac a m \sum_{i=1}^n(y_i-h_\theta(x_i))x_j^i BGD伪代码： repeat{ \theta_j=\theta_j+\frac a m \sum_{i=1}^n(y_i-h_\theta(x_i))x_j^i(for every j = 0, 1, .. n) } 总结： 优点：BGD 得到的是全局最优解, 因为它总是以整个训练集来计算梯度, 缺点：因此带来了巨大的计算量, 计算迭代速度很很慢. 随机梯度下降法（Stochastic Gradient Descent）随机梯度下降法，其实和批量梯度下降法原理类似，区别在于求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度。 具体实现过程： SGD 每次以一个样本, 而不是整个数据集来计算梯度. 因此, SGD 从成本函数开始, 就不必再求和了, 针对单个样例的成本函数可以写成: J(\theta)=\frac{1}{2} (h_\theta(x_i)-y_i)^2于是, SGD 的参数更新规则就可以写成 ： \theta_j=\theta_j+a (y_i-h_\theta(x_i))x_j^iSGD伪代码： repeat { for i = 1, .., m{ \theta_j=\theta_j+a (y_i-h_\theta(x_i))x_j^i (for every j = 0, 1, .. n) } } 总结： SGD 的关键点在于以随机顺序选取样本. 因为 SGD 存在局部最优困境, 若每次都以相同的顺序选取样本, 其有很大的可能会在相同的地方陷入局部最优解困境, 或者收敛减缓. 因此, 欲使 SGD 发挥更好的效果, 应充分利用随机化带来的优势: 可以在每次迭代之前 (伪代码中最外围循环), 对训练集进行随机排列. 缺点：因为每次只取一个样本来进行梯度下降, SGD 的训练速度很快, 但会引入噪声, 使准确度下降 优点：可以使用在线学习. 也就是说, 在模型训练好之后, 只要有新的数据到来, 模型都可以利用新的数据进行再学习, 更新参数,以适应新的变化. ＢＧＤ vs ＳＧＤ随机梯度下降法和批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。 MBGD就综合了这两种方法的优点。 小批量梯度下降法（Mini-batch Gradient Descent）MBGD 是为解决 BGD 与 SGD 各自缺点而发明的折中算法, 或者说它利用了 BGD 和 SGD 各自优点. 其基本思想是: 每次更新参数时, 使用 n 个样本, 既不是全部, 也不是 1. (SGD 可以看成是 n=1 的 MBGD 的一个特例) MBGD 的成本函数或其求导公式或参数更新规则公式基本同 BGD 。 MBGD 的伪代码： say b=10, m=1000, repeat { for i = 1, 11, 21, .., 991 { \theta_j=\theta_j+\frac a {10} \sum_{i=1}^{i+9}(y_i-h_\theta(x_i))x_j^i (for every j = 0, 1, .. n) } } 梯度下降算法总结]]></content>
      <categories>
        <category>优化算法</category>
      </categories>
      <tags>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DDQN & DDPG]]></title>
    <url>%2F2019%2F01%2F17%2FDouble-QDN%2F</url>
    <content type="text"><![CDATA[ε-贪婪（greedy）策略目的：探索与利用ε∈(0,1)，随着时间的推移逐渐减小直至0产生一个(0,1)的随机数m如果ε&gt;m 采取随机策略，例如一共4个动作，那么选每一个动作的概率都是 0.25如果ε&lt;m 采取贪婪策略，计算当前网络所有输出值Q(St,a)，选择使得 Q(St,a)最大的那个at值作为下一步的动作 玻尔兹曼softmax$q_t(a)$为t时刻，采取动作a的Q值大小 τ表示是一个衰减系数（类似于模拟退火算法的温度项），随着训练次数的增加而逐渐减少，与ε相对应。随着τ的减小，选择使Q值最大的那个动作a值的概率也越来越高。 ε-贪婪 VS 玻尔兹曼` DDQNDQN: Double Q learning: Double DQN: PRIORITIZED EXPERIENCE REPLAY(优先化记忆回放)每一个rollout的被采样概率: 其中: importance-sampling (IS) weights（重要性采样权重）: SumTree: DDQN+PER: Dueling DQNDueling Network Architectures for Deep Reinforcement Learningvalue和Q value： 优势值（Advantage function） 结合方式： DQN性能 DDPGＤＱＮ的问题： 动作空间必须是离散的能不能将DQN的思想应用到连续的动作空间？ DDPG 是一种离策略算法DDPG仅可以用于连续动作空间的问题]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>DDPG</tag>
        <tag>DDQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DQN]]></title>
    <url>%2F2019%2F01%2F16%2FDQN%2F</url>
    <content type="text"><![CDATA[DQN的背景传统强化学习的局限性，无法很好的解决状态空间或者动作空间很大的实际问题举例：小车使用相机进行导航，动作为向左，向前，向右，3种 100 x 100的灰度图片，状态数： 256^{10000}如果使用q-learning，q(s,a)的个数为3\times256^{10000}以现在的存储与计算能力，不可能完成 首先解决状态空间很大的问题 能不能根据现在的状态来估计Q(s,a)的值？ 价值函数估计假设近似器参数为w,注意有些公式给的是θ，两者是一个意思 回归器的选择: 特征线性组合 神经网络 决策树 最近邻 傅里叶/小波基 DQN VS Q_learning深度Q网络(Deep Q-Network,DQN): Q-learning(离策略（Off-policy）TD控制): Q learning学习目标： Q函数近似的学习目标: θ可以是任何回归器的参数，如果特指深度神经网络，那么我们也称之为深度Q网络 深度Q网络(Deep Q-Network,dqn)1、如何通过神经网络进行近似端到端的形式输入：状态或者观测输出：Q值２、与监督学习的异同？ 不用人工标注,神经网络生成 目标值 １、数据怎么来？使用当下策略生成。2.、有没有问题？相邻两次的更新使用的样本是是相关的Q（s1,a1）=0.9, 估计成了1.0, s2与s1很相似Q(s2,a1) = 0.05+1*0.99=1.04，s3与s2很相似……3 、在训练时，打散训练样本的顺序 经验回放定义一个replay buffer，RB, 记录下前N次的rollouts在训练的时候，随机采样，进行训练 DQN with experience replay ：]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>DQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP传输图片]]></title>
    <url>%2F2019%2F01%2F16%2FTCP%E4%BC%A0%E8%BE%93%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[import socketimport pickleimport numpy as npfrom PIL import Imageimport ioimport sysimport threadingdef main(img):# start_svc = datetime.datetime.now() try: img = img.convert(&apos;L&apos;) img = img.resize((256,256)) # print(img.mode) # print(img.size) arr = np.asarray(img, dtype=&quot;float32&quot;) # arr = arr.flatten() arr = arr.reshape(1,-1) #data.append(arr) # name = &quot;&quot; + job_name + &quot;.pkl&quot; # name = &quot;AAA.pkl&quot; #end_svc = datetime.datetime.now() pca_data = pca.transform(arr) y_test_pred = svc.predict(pca_data) #time = end_svc - start_svc #print(time) return y_test_pred except Exception as err: return str(err)len_rev=0def tcp_connected(s,addr): print(&apos;Accept new connection from %s:%s...&apos; % addr) while 1: data =s.recv(1600000) len_rev=len(data) # print(len_rev) if len_rev&gt;=1000000: image = Image.open(io.BytesIO(data)) result=main(image)[0] s.send(result.encode()) print(result) elif len_rev&lt;=100 and data.decode()== &apos;close&apos;: sock.close() sys.exit(0)arg1 = &quot;&quot; + sys.argv[1]s = socket.socket() # 创建 socket 对象all_port=[6001,42683]arg=int(arg1)port=all_port[arg]print(port)#path_to_watch=r&apos;image/Cam&#123;&#125;/&apos;.format(arg+1)s.bind((&apos;127.0.0.1&apos;, port)) # 绑定端口s.listen(2) # 监听连接,传入连接请求的最大数5 # 接受一个新连接sock,ad=s.accept() # 创建新线程来处理TCP连接name = &quot;SVC_PCA.pkl&quot;pca, svc = pickle.load(open(name, &apos;rb&apos;))t = threading.Thread(tcp=tcp_connected(sock, ad))]]></content>
      <categories>
        <category>图片分类</category>
      </categories>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时序差分学习]]></title>
    <url>%2F2019%2F01%2F16%2F%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[时序差分学习（Temporal-Difference Learning, TD learning）是强化学习中最核心与最著名的思想 ‘If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning.’ —Richard S. Sutton&amp; Andrew G. BartoTD = DP + MCTD, DP都是使用下一时刻的状态函数来估计当前时刻的状态函数。TD,MC都是通过经历一次一次与环境互动，产生多个episode来估计状态函数。 ＴＤ： V(S_t)\leftarrow V(S_t)+\alpha(G_t-V(S_t))ＭＣ： V(S_t)\leftarrow V(S_t)+\alpha(R_{t+1}+V(S_{t+1})-V(S_t))Sarsa在策略（On policy）ＴＤ控制 Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\gamma(R_{t+1}+Q(S_{t+1},A_{t+1})-Q(S_t,A_t)) Expected Sarsa离策略（Off-policy）TD控制 Q-learning离策略（Off-policy）TD控制 Q-learning vs Sarsa例子：悬崖行走（固定的ε=0.1） Q-learning的问题过估计（overestimate），因此产生了Double Q learning Double Q learning Q learning vs. Double Q learning训练参数：ε=0.1，α=0.1, γ=1]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>TD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[蒙特卡洛方法]]></title>
    <url>%2F2019%2F01%2F16%2F%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[MC如何在没有模型的情况下评估一个策略？ 如何计算Ｖ（ｓ）和Ｑ（ｓ）？ 通过采样的方式 如何得到数据？ On policy: 使用当下的策略生成的数据进行策略评估 Off policy: 使用其他策略生成的数据进行策略评估 首次访问蒙特卡洛预测（评估）： Every-Visit Monte-Carlo Policy Evaluation: Incremental Mean: Incremental Monte-Carlo Updates 随机策略在预测完成当前策略下的V和Q之后，我们需要对当下的策略进行改进可以采用完全贪婪的策略提升吗？ s4-&gt;s3; s3-&gt;s2;s2-&gt;s2;s2-&gt;s1;s4-&gt;s3; s3-&gt;s2;s2-&gt;s1; V(s)，Q(s,a)0,1,1,1,0,0,0Q(s2,a=左)= Q(s3,a=左)= Q(s4,a=左)=1，Q(s4,a=右)=0一直向左走？ ε-贪婪（greedy）策略目的： Exploration（探索）与Exploitation（利用） ε∈(0,1)，随着时间的推移逐渐减小直至0 产生一个(0,1)的随机数m 如果ε&gt;m 采取随机策略，例如一共4个动作，那么选每一个动作的概率都是 0.25如果ε&lt;m 采取贪婪策略，计算当前网络所有输出值Q(St,a)，选择使得Q(St,a)最大的那个at值作为下一步的动作 On-pokicy first-visit 蒙特卡洛方法： 重要性采样 一个特定的回合内，其生成的轨迹概率： 轨迹： 重要性采样比率： 使用重要性采样的蒙特卡洛方法：]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>MC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图片分类-pca_svc]]></title>
    <url>%2F2019%2F01%2F15%2F%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E4%B9%8Bpca-svc%2F</url>
    <content type="text"><![CDATA[PCA_SVC训练import osfrom PIL import Imageimport numpy as npfrom sklearn.decomposition import PCAfrom sklearn.svm import SVCimport timeimport datetimedef load_Img(imgDir): lable = os.listdir(imgDir) #print(lable) OK_name=os.listdir(imgDir+&apos;/&apos;+lable[0]) NG_name=os.listdir(imgDir+&apos;/&apos;+lable[1]) #print(NG_name) label=[] data=[] for i in range(len(OK_name)): start = datetime.datetime.now() OK_path = imgDir + &quot;/&quot; + lable[0]+&apos;/&apos;+OK_name[i-1] OK_img = Image.open(OK_path) OK_img=OK_img.convert(&apos;L&apos;) # print(OK_img.size) end = datetime.datetime.now() print(end - start) OK_img = OK_img.resize((64,64)) OK_arr = np.asarray(OK_img, dtype=&quot;float32&quot;) OK_arr = OK_arr.flatten() data.append(OK_arr) label.append(lable[0]) # label.append(1) for j in range(len(NG_name)): NG_path = imgDir + &apos;/&apos; + lable[1] + &apos;/&apos; + NG_name[j - 1] NG_img = Image.open(NG_path) NG_img=NG_img.convert(&apos;L&apos;) # print(NG_img.size) NG_img = NG_img.resize((64,64)) NG_arr = np.asarray(NG_img, dtype=&quot;float32&quot;) NG_arr = NG_arr.flatten() data.append(NG_arr) # label.append(0) label.append(lable[1]) return label ,datacraterDir = &quot;E:\image\Result&quot;label,data = load_Img(craterDir)#print(label)#print(data)#将数据分割训练数据与测试数据from sklearn.model_selection import train_test_split# 随机采样20%的数据构建测试样本，其余作为训练样本X_train, X_test, y_train, y_test = train_test_split(data,label , random_state=33, test_size=0.2)# 一个参数点（PCA维数为n）的模型训练和测试，得到该参数下模型在校验集上的预测性能def n_component_analysis(n,C,gamma, X_train, y_train, X_val, y_val): #start = time.time() start = datetime.datetime.now() pca = PCA(n_components=n) print(&quot;PCA begin with n_components: &#123;&#125;&quot;.format(n)); pca.fit(X_train) # 在训练集和测试集降维 X_train_pca = pca.transform(X_train) X_val_pca = pca.transform(X_val) # 利用SVC训练 print(&apos;SVC begin&apos;) clf1 = SVC(C=C,gamma=gamma) # clf1 = SVC(C=C,kernel=&apos;rbf&apos;,gamma=gamma) clf1.fit(X_train_pca, y_train) # 返回accuracy accuracy = clf1.score(X_val_pca, y_val) end = datetime.datetime.now() # end = time.time() print(&quot;accuracy: &#123;&#125;,C:&#123;&#125;,gamma:&#123;&#125; time elaps:&#123;&#125;&quot;.format(accuracy,C,gamma ,end - start)) return accuracy# 设置超参数（PCA维数）搜索范围n_s = np.linspace(0.70, 0.85, num=3)#需要调优的参数C_s = np.logspace(4,6, 3)# logspace(a,b,N)把10的a次方到10的b次方区间分成N份gamma_s = np.logspace(-8, -6, 3)accuracy = []if __name__==&apos;__main__&apos;: for n in n_s: for i, oneC in enumerate(C_s): for j, gamma in enumerate(gamma_s): tmp = n_component_analysis(n, oneC, gamma,X_train, y_train, X_test, y_test) accuracy.append(tmp) 保存模型# coding=utf-8import osimport sysfrom PIL import Imageimport numpy as npfrom sklearn.svm import SVCimport picklefrom sklearn.decomposition import PCA#将数据分割训练数据与测试数据from sklearn.model_selection import train_test_split# 随机采样20%的数据构建测试样本，其余作为训练样本def Gain_Img(imgDir): lable = os.listdir(imgDir) OK_name=os.listdir(imgDir+&apos;/&apos;+lable[0]) NG_name=os.listdir(imgDir+&apos;/&apos;+lable[1]) print(lable) for i in range(len(OK_name)): OK_path = imgDir + &quot;/&quot; + lable[0]+&apos;/&apos;+OK_name[i-1] OK_img = Image.open(OK_path) OK_img=OK_img.convert(&apos;L&apos;) OK_img = OK_img.resize((256,256)) out1 = OK_img.rotate(90) # 逆时针旋转45度 if not os.path.exists(&apos;rotation/&#123;&#125;&apos;.format(lable[0])): os.makedirs(&apos;rotation/&#123;&#125;&apos;.format(lable[0])) if not os.path.exists(&apos;rotation/&#123;&#125;&apos;.format(lable[1])): os.makedirs(&apos;rotation/&#123;&#125;&apos;.format(lable[1])) out1.save(&quot;rotation\\&#123;&#125;\\&#123;&#125;_90_&#123;&#125;.bmp&quot;.format(lable[0],lable[0],i)) OK_img.save(&quot;rotation\\&#123;&#125;\\&#123;&#125;_&#123;&#125;.bmp&quot;.format(lable[0],lable[0],i)) for j in range(len(NG_name)): NG_path = imgDir + &apos;/&apos; + lable[1] + &apos;/&apos; + NG_name[j - 1] NG_img = Image.open(NG_path) NG_img = NG_img.convert(&apos;L&apos;) NG_img = NG_img.resize((256,256)) out2 = NG_img.rotate(90) # 逆时针旋转45度 out2.save(&quot;rotation\\&#123;&#125;\\&#123;&#125;_90_&#123;&#125;.bmp&quot;.format(lable[1],lable[1],j)) NG_img.save(&quot;rotation\\&#123;&#125;\\&#123;&#125;_&#123;&#125;.bmp&quot;.format(lable[1],lable[1],j))def load_Img(imgDir): lable = os.listdir(imgDir) OK_name=os.listdir(imgDir+&apos;/&apos;+lable[0]) NG_name=os.listdir(imgDir+&apos;/&apos;+lable[1]) label=[] data=[] for i in range(len(OK_name)): OK_path = imgDir + &quot;/&quot; + lable[0]+&apos;/&apos;+OK_name[i-1] OK_img = Image.open(OK_path) # OK_img = OK_img.convert(&apos;L&apos;) # OK_img = OK_img.resize((64, 64)) OK_arr = np.asarray(OK_img, dtype=&quot;float32&quot;) OK_arr = OK_arr.flatten() data.append(OK_arr) label.append(lable[0]) for j in range(len(NG_name)): NG_path = imgDir + &apos;/&apos; + lable[1] + &apos;/&apos; + NG_name[j - 1] NG_img = Image.open(NG_path) # NG_img = NG_img.convert(&apos;L&apos;) # NG_img = NG_img.resize((64, 64)) NG_arr = np.asarray(NG_img, dtype=&quot;float32&quot;) NG_arr = NG_arr.flatten() data.append(NG_arr) label.append(lable[1]) return label ,datadef main(path,job_name): try: label, data = load_Img(path) X_train, X_test, y_train, y_test = train_test_split(data,label , random_state=33, test_size=0.2) pca = PCA(n_components=0.7) pca.fit(X_train) # 在训练集和测试集降维 X_train_pca = pca.transform(X_train) X_test_pca = pca.transform(X_test) SVC4=SVC(C=10000,gamma=1e-06) SVC4 = SVC4.fit(X_train_pca, y_train) accuracy = SVC4.score(X_test_pca, y_test) #保存模型 name=&quot;&quot;+job_name+&quot;.pkl&quot; pickle.dump((pca,SVC4),open(name, &apos;wb&apos;)) return str(accuracy) except Exception as err: return str(err)if __name__ == &apos;__main__&apos;: # print(main(&quot;E:\image\SVM&quot;)) arg1=&quot;&quot;+sys.argv[1] arg2=&quot;&quot;+sys.argv[2] gain = Gain_Img(arg1) print(main(&quot;rotation&quot;,arg2)) 实时测试见ＴＣＰ传输图片]]></content>
      <categories>
        <category>图片分类</category>
      </categories>
      <tags>
        <tag>SVC</tag>
        <tag>PCA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PIL简单应用]]></title>
    <url>%2F2019%2F01%2F15%2FPIL%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[PIL简介PIL是python自带的图像处理库 安装： pip install pillow 扩充数据－旋转准备：新建Train文件夹，将ＮＧ和ＯＫ图片分别放在命名为ＮＧ和ＯＫ的文件夹，新建rotation文件夹，或者修改路径 import osfrom PIL import Imageimport numpy as npdef load_Img(imgDir): lable = os.listdir(imgDir) OK_name=os.listdir(imgDir+&apos;/&apos;+lable[0]) NG_name=os.listdir(imgDir+&apos;/&apos;+lable[1]) print(lable) for i in range(len(OK_name)): OK_path = imgDir + &quot;/&quot; + lable[0]+&apos;/&apos;+OK_name[i-1] OK_img = Image.open(OK_path) OK_img=OK_img.convert(&apos;L&apos;) OK_img = OK_img.resize((256,256)) out1 = OK_img.rotate(90) # 逆时针旋转90度 out1.save(&quot;E:\\image\\rotation\\&#123;&#125;\\&#123;&#125;_90_&#123;&#125;.bmp&quot;.format(lable[0],lable[0],i)) OK_img.save(&quot;E:\\image\\rotation\\&#123;&#125;\\&#123;&#125;_&#123;&#125;.bmp&quot;.format(lable[0],lable[0],i)) for j in range(len(NG_name)): NG_path = imgDir + &apos;/&apos; + lable[1] + &apos;/&apos; + NG_name[j - 1] NG_img = Image.open(NG_path) NG_img = NG_img.convert(&apos;L&apos;)#L为灰度 NG_img = NG_img.resize((256,256))#改变大小 out2 = NG_img.rotate(90) # 逆时针旋转90度 out2.save(&quot;E:\\image\\rotation\\&#123;&#125;\\&#123;&#125;_90_&#123;&#125;.bmp&quot;.format(lable[1],lable[1],j)) NG_img.save(&quot;E:\\image\\rotation\\&#123;&#125;\\&#123;&#125;_&#123;&#125;.bmp&quot;.format(lable[1],lable[1],j))craterDir = &quot;E:\image\Train&quot;rotation=load_Img(craterDir)]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>PIL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[强化学习基础]]></title>
    <url>%2F2019%2F01%2F15%2F%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[强化学习的作用Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal.强化学习是学习做什么（决策），即基于当前的场景，学习如何做出一个可以最大化回报的动作。 深度学习与强化学习的关系multiple layers of nonlinear processing units for feature extraction and transformation.深度学习（DL）:用多层非线性处理单元学习从输入到输出的特征提取和变换 深度强化学习（DRL）：在强化学习的框架下，用深度神经网络来近似策略 基本结构 状态集：s ∈ S 动作集：a ∈ A 策略π： s =&gt; a 转移函数：Ｔ(s,a,s’) 或者：转移概率： P(s’|s,a) 奖励函数： R(s,a,s’) MDP策略：在状态 s 下采取什么动作 a ，找到一个最优策略 π* π（a|s）:表示策略 π 下，在状态 s 下采取行动 a 的概率 方式：通过定义每一个状态的好坏，以及或者该状态下采取某一个动作后的好坏，来寻找最优策略 价值函数状态s，在策略π下的价值函数： v_\pi(s)=E_\pi[G_t|S_t=s]=E_\pi[\sum_{k=0}^\infty\gamma^kR_{t+k+1}|S_t=s]状态s,在执行动作a情况下，策略π的价值函数： q_\pi(s,a)=E_\pi[G_t|S_t=s,A_t=a]=E_\pi[\sum_{k=0}^\infty\gamma^kR_{t+k+1}|S_t=s,A_t=a]贝尔曼方程 描述了当前状态下的价值函数与其下一时刻状态下的价值函数的关系 DP“The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP).”动态规划是在给定模型情况下求解最优策略的马尔科夫决策过程的一系列算法的统称。 动态规划主要分为：策略迭代与值迭代（Policy iteration vs Value iteration） 前提条件：转移概率p(s’,r|s,a)已知 贝尔曼最优性方程： 值迭代 实例 运用公式： 结果： 策略迭代策略迭代＝策略评估＋策略提升 策略评估目标：通过执行策略π，计算每个状态对应的状态函数值 实例： 策略提升在策略评估之后，采用贪婪策略进行策略更新 v_\pi=E_{a\sim\pi}(q_\pi(s,a))将策略改成： \pi'=argmax_a(q_\pi(s,a))则： v_\pi'>v_\pi VI和PI的联系策略评估： 策略提升： 值迭代： ＰＭＤＰＭＤＰ假设中，状态是完全已知的。实际生活中，由于传感器的局限性。往往难以得到当前状态的准确状态值。 但我们可以估计当前的状态分布belief: b(s) 更新belief: 实例]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络理论]]></title>
    <url>%2F2019%2F01%2F15%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[卷积层任务：对输入的图像进行特征提取 用一个小的权重矩阵去覆盖输入数据，对应位置元素加权相乘，其和作为结果的一个像素点。 这个权重在输入数据上滑动，形成一张新的矩阵 这个权重矩阵就被称为卷积核（convolution kernel） 其覆盖的位置称为感受野（receptive fileld ） 生成的新矩阵叫做特征图（feature map） 如下图所示： 步长（stride）： 步长为1，表示跳过1个像素 步长为2，就表示跳过2个像素 补齐（padding）方式: valid方式 same方式（会在图像的边缘用0补齐） 输出维度计算公式： VALID: W-F+1/S SAME: W/S 注意：彩色图像的卷积核是三阶的，所有的通道的结果要做累加。 实例： padding=same；步长设置为2 池化层｀任务`：对特征进行采样，即用一个数值替代一块区域，主要是为了降低网络训练参数及模型的过拟合程度。以及降低计算量。 池化/采样的方式通常有以下两种： 最大池化（Max Pooling: 选择Pooling窗口中的最大值作为采样值； 均值池化（Mean Pooling）: 将Pooling窗口中的所有值相加取平均，以平均值作为采样值 高斯池化：借鉴高斯模糊的方法。不常用。 可训练池化：使用一个训练函数y=f(x)y=f(x)。不常用。 全链接层任务：全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。 dropout层任务：在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了。主要是为了防止过拟合。 激活层任务：卷积后的结果压缩到某一个固定的范围做非线性映射，这样可以一直保持一层一层下去的数值范围是可控的。 激活函数： Sigmoid Tanh（双曲正切） ReLU Leaky ReLU ELU Maxout 卷积神经网络一般采用的激活函数是ReLU(The Rectified Linear Unit/修正线性单元)，它的特点是收敛快，求梯度简单，但较脆弱，图像如下： 激活层的实践经验： 不要用sigmoid！不要用sigmoid！不要用sigmoid！ 首先试RELU，因为快，但要小心点 、 如果2失效，请用Leaky ReLU或者Maxout 某些情况下tanh倒是有不错的结果，但是很少]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典卷积神经网络]]></title>
    <url>%2F2019%2F01%2F15%2F%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[LeNet这是最早用于数字识别的CNN LeNet5特征能够总结为如下几点： 1）卷积神经网络使用三个层作为一个系列： 卷积，池化，非线性 2） 使用卷积提取空间特征 3）使用映射到空间均值下采样（subsample） 4）双曲线（tanh）或S型（sigmoid）形式的非线性 5）多层神经网络（MLP）作为最后的分类器 6）层与层之间的稀疏连接矩阵避免大的计算成本 AlexNet2012 ILSVRC比赛远超第2名的CNN，比 LeNet更深，用多层小卷积层叠加替换单大卷积层。 AlexNet的结构模型如下： VGGNet2014 ILSVRC比赛中的模型，图像识别略差于GoogLeNet，但是在很多图像转化学习问题(比如object detection)上效果奇好 VGG各版本结构如下： 经典卷积网络实现详见：卷积神经网络实现]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>VGG</tag>
        <tag>LeNet</tag>
        <tag>AlexNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反向传播]]></title>
    <url>%2F2019%2F01%2F14%2F%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[定义损失函数： E_{total}=\frac12 (y-outo)^2定义激活函数： \sigma(x)=sigmod(x)前向传播第一层(输入层)： x_1\ \ \ x_2\ \ \ b_1加权和： net h_1=x_1w_1+x_2w_2+b_1 第二层(隐层)： outh_1=sigmod(neth_1) 加权和： neto_1=outh_1w_3+outh_2w_4+b_2 第三层(输出层)： outo_1=sigmod(neto_1) 计算误差值： Eo_1 = \frac12 (y_1-outo_1)^2 Eo_2 = \frac12 (y_2-outo_2)^2 E_{total}=Eo_1+Eo_2 总结：要是使误差值最小，就需要误差反向传播算法，更新得到最小误差的权重参数w和b。 反向传播须知：我们需要反向传递回去更新每一层对应的权重参数w和b。 我们使用链式法则来反向模式求导。 更新第三层（输出层）的权重参数：更新参数w： \frac{\partial E_{total}}{\partial w_3}=\frac{\partial E_{total}}{\partial outo_1} \cdot \frac{\partial outo_1}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial w_3} = \frac{\partial \frac12(y_1-outo_1)^2}{\partial outo_1} \cdot \frac{\partial sigmod(neto_1)}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial w_3} =(outo_1-y_1)\cdot outo_1(1-outo_1)\cdot outh_1 w_{3new}=w_{3old}-\eta \frac{\partial E_{total}}{\partial w_3}，η是学习率更新参数b： \frac{\partial E_{total}}{\partial b_2}=\frac{\partial E_{total}}{\partial outo_1} \cdot \frac{\partial outo_1}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial b_2} = \frac{\partial \frac12(y_1-outo_1)^2}{\partial outo_1} \cdot \frac{\partial sigmod(neto_1)}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial b_2} =(outo_1-y_1)\cdot outo_1(1-outo_1) b_{2new}=b_{2old}-\eta \frac{\partial E_{total}}{\partial b_2}, η是学习率同理可得：w4：也就是同一层的w都可以用这种方式更新。 更新上一层(隐层)的权重参数：更新权重参数w和b： \frac{\partial E_{total}}{\partial w_1}=\frac{\partial E_{total}}{\partial outh_1} \cdot \frac{\partial outh_1}{\partial neth_1} \cdot \frac{\partial neth_1}{\partial w_1} \frac{\partial E_{total}}{\partial b_1}=\frac{\partial E_{total}}{\partial outh_1} \cdot \frac{\partial outh_1}{\partial neth_1} \cdot \frac{\partial neth_1}{\partial b_1}其中： \frac{\partial E_{total}}{\partial outh_1} = \frac{\partial Eo_1}{\partial outh_1}+ \frac{\partial Eo_2}{\partial outh_1} \frac{\partial Eo_1}{\partial outh_1} = \frac{\partial Eo_1}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial outh_1} \frac{\partial Eo_1}{\partial neto_1} = \frac{\partial E_{o_1}}{\partial outo_1} \cdot \frac{\partial outo_1}{\partial neto_1} = (outo_1-y_1)\cdot outo_1(1-outo_1) \frac{\partial neto_1}{\partial outh_1} = \frac{\partial (outh_1w_3+outo_2w_4+b_2)}{\partial outh_1} = w_3同理可得： \frac{\partial Eo_2}{\partial outh_1} = \frac{\partial Eo_2}{\partial neto_2} \cdot \frac{\partial neto_2}{\partial outh_1} \frac{\partial Eo_2}{\partial neto_2} = \frac{\partial E_{o_2}}{\partial outo_2} \cdot \frac{\partial outo_2}{\partial neto_2} = (outo_2-y_2)\cdot outo_2(1-outo_2) \frac{\partial neto_2}{\partial outh_1} = w_5（outh1连接outo2的权重，暂定为w5）综合上式： \frac{\partial E_{total}}{\partial w_1}= [w_3 (outo_1-y_1)\cdot outo_1(1-outo_1) + w_5(outo_2-y_2)\cdot outo_2(1-outo_2)] \cdot outh_1(1-outh_1) \cdot x_1 \frac{\partial E_{total}}{\partial b_1}= [w_3 (outo_1-y_1)\cdot outo_1(1-outo_1) +w_5(outo_2-y_2)\cdot outo_2(1-outo_2)] \cdot outh_1(1-outh_1)更新： w_{1new}=w_{1old}-\eta \frac{\partial E_{total}}{\partial w_1} b_{1new}=b_{1old}-\eta \frac{\partial E_{total}}{\partial b_1}同理可得：w2：也就是同一层的w都可以用这种方式更新。 推广 我们定义第L层的第i个神经元更新权重参数时(上标表示层数，下标表示神经元)： \frac{\partial E_{total}}{\partial net_i^{(L)}} = \delta_i^{(L)} \frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}，其中w_{ij}^{(l)}表示第l层的第i个神经元连接第l−1层的第j的神经元的相连的权重参数w。如下图所示： 推广总结根据前面所定义的： E_{total}=\frac12 (y-outo)^2 \sigma(x)=sigmod(x) \frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)} \delta_i^{(L)}=\frac{\partial E_{total}}{\partial net_i^{(L)}} = \frac{\partial E_{total}}{\partial outh_i} \cdot \frac{\partial outh_i}{\partial net_i^{(L)}} = \bigtriangledown_{out} E_{total} \times \sigma^{\prime}(net_i^{(L)})对于第ll层： \delta^{(l)}=\frac{\partial E_{total}}{\partial net^{(l)}} = \frac{\partial E_{total}}{\partial net^{(l+1)}} \cdot \frac{\partial net^{(l+1)}}{\partial net^{(l)}} = \delta^{(l+1)} \times \frac{\partial net^{(l+1)}}{\partial net^{(l)}} = \delta^{(l+1)} \times \frac{\partial (w^{(l+1)}\sigma (net^{(l)}))}{\partial net^{(l)}} = \delta^{(l+1)} w^{(l+1)} \sigma^{\prime}(net^{(L)})对于偏置项bias： \frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}四项基本原则基本形式 \delta_i^{(L)}= \bigtriangledown_{out} E_{total} \times \sigma^{\prime}(net_i^{(L)}) \delta^{(l)} = \sum_j \delta_j^{(l+1)} w_{ji}^{(l+1)} \sigma^{\prime}(net_i^{(l)}) \frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)} \frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}矩阵形式 \delta_i^{(L)}= \bigtriangledown_{out} E_{total} \bigodot \sigma^{\prime}(net_i^{(L)})其中 ⨀是Hadamard乘积（对应位置相乘） \delta^{(l)} = (w^{(l+1)})^T \delta^{(l+1)} \bigodot \sigma^{\prime}(net^{(l)}) \frac{\partial E_{total}}{\partial bias^{(l)}}=\delta^{(l)} \frac{\partial E_{total}}{\partial w^{(l)}}=\delta^{(l)}(outh^{(l-1)})^T实例 因为： \delta_i^{(L)}= \bigtriangledown_{out} E_{total} \bigodot \sigma^{\prime}(net_i^{(L)})所以： \delta^{(1)} = (w^{(2)})^T \delta^{(2)} \bigodot \sigma^{\prime}(net^{(1)}) =(\begin{bmatrix} 0.6 & 0.8 \\ 0.7 & 0.9\end{bmatrix}^T \cdot \begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix}) \bigodot \begin{bmatrix} 0.20977282 \\ 0.19661193\end{bmatrix} =\begin{bmatrix} 0.01074218\\ 0.01287516\end{bmatrix}因为： \frac{\partial E_{total}}{\partial w^{(l)}}=\delta^{(l)}(outh^{(l-1)})^T所以： \Delta w^{(2)} = \delta^{(2)}(outh^{(1)})^T =\begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix} \cdot \begin{bmatrix} 0.70056714\\ 0.73105858 \end{bmatrix}^T = \begin{bmatrix} -0.00869356 & -0.00907194 \\ 0.5870176 & 0.612567 \end{bmatrix} \Delta w^{(1)} = \delta^{(1)}x^T =\begin{bmatrix} 0.01074218\\ 0.01287516\end{bmatrix} \cdot \begin{bmatrix} 0.5\\ 1\end{bmatrix}^T = \begin{bmatrix} 0.00537109& 0.01074218\\ 0.00643758 & 0.01287516 \end{bmatrix}权重更新： w_{new}^2 = w_{old}^2-\Delta w^{(2)} = {\begin{bmatrix} 0.6 & 0.8 \\ 0.7 & 0.9\end{bmatrix}}-\begin{bmatrix} -0.00869356 & 0.00907194 \\ 0.5870176 & 0.612567 \end{bmatrix} = \begin{bmatrix} 0.60869356 & 0.80907194 \\ 0.64129824& 0.8387433 \end{bmatrix} b_{new}^2=b_{old}^2-\Delta b^2 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}-\begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix} =\begin{bmatrix} 1.01240932\\ 0.91620823\end{bmatrix} w_{new}^1= w_{old}^1-\Delta w^{(1)} =\begin{bmatrix} 0.1 & 0.3 \\ 0.2 & 0.4\end{bmatrix} - \begin{bmatrix} 0.00537109& 0.01074218\\ 0.00643758 & 0.01287516 \end{bmatrix} = \begin{bmatrix} 0.09462891& 0.28925782\\ 0.19356242& 0.38712484\end{bmatrix} b_{new}^1=b_{old}^1-\Delta b^1 =\begin{bmatrix} 0.5 \\ 0.5 \end{bmatrix} - \begin{bmatrix} 0.01074218\\ 0.01287516\end{bmatrix} =\begin{bmatrix} 0.48925782\\ 0.48712484\end{bmatrix}权重初始化]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>反向传播</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图床]]></title>
    <url>%2F2019%2F01%2F14%2F%E5%9B%BE%E5%BA%8A%2F</url>
    <content type="text"><![CDATA[安装picgo下载地址: https://github.com/Molunerfinn/PicGo/releases linux下载AppImage文件 右键属性，将权限设为允许为启动程序 配置以阿里云为例： 点击右上角头像，找到accessKeyId和accessKeySecret 创建对象存储，类型设为公共，记住存储空间名和地域（比如华南为oss-cn-shenzhen） 右键点击picgo，选择主窗口；将以上对应信息配置到图床配置，保存并应用]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>图床</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[感知器与激活函数]]></title>
    <url>%2F2019%2F01%2F14%2F%E6%84%9F%E7%9F%A5%E5%99%A8%E4%B8%8E%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[定义感知器是激活函数为阶跃函数的神经元。感知器的模型如下：· 输入(inputs)：一个感知器可以接收多个输入(x1,x2,…,xn|xi∈R)· 权值(weights)：每一个输入上都有一个权值wi∈R，此外还有一个偏置项b∈R，也就是上图的w0。· 加权和(weighted sum)：就是输入权值 x x 权值 w + 偏置项 b的总和。· 激活函数(step function)：感知器的激活函数： f(x) = \begin{cases} 0, & \text{x>0} \\ 1, & \text{x≤0} \end{cases}· 输出(output)：感知器的输出由加权值用激活函数做非线性变换。也就是这个公式：y=f(w⋅x+b)我们使用unit激活函数结合上图就有： y=f(w⋅x+b)=f(w1x1+w2x2+w3x3+bias) 其中f(x)就是激活函数 f(x)={1x&gt;0 0x≤0 ，图像如下图所示: 实例 x_1=[-1.0, 3.0, 2.0] \\ x_2=[2.0, -1.0, 5.0] \\ x_3=[-2.0, 0.0, 3.0 ] \\ x_4=[4.0, 1.0, 6.0] \\ w=[4.0, -3.0, 5.0 ] \\ b=2.0则： X=\begin{bmatrix} -1.0 & 3.0 & 2.0 \\ 2.0 & -1.0& 5.0 \\ -2.0& 0.0& 3.0 \\ 4.0& 1.0 & 6.0 \end{bmatrix} w^T =\begin{bmatrix} 4.0 \\ -3.0 \\ 5.0 \end{bmatrix}所以： logits = X\cdot w^T + b = \begin{bmatrix} -1.0 & 3.0 & 2.0 \\ 2.0 & -1.0& 5.0 \\ -2.0& 0.0& 3.0 \\ 4.0& 1.0 & 6.0 \end{bmatrix} \cdot \begin{bmatrix} 4.0 \\ -3.0 \\ 5.0 \end{bmatrix} + 2.0 \\ =[-1.0 \ \ \ 38.0 \ \ \ 7.0 \ \ \ 43.0 ]带入激活函数： output = f(x)=[0\ \ \ 1 \ \ \ 1 \ \ \ 1 ]隐层 激活函数unit激活函数： f(x) = \begin{cases} 0, & \text{x>0} \\ 1, & \text{x≤0} \end{cases} sigmod激活函数： f(x)=sigmod(x)=\frac{1}{1+e^{-x}} tanh激活函数： f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}} relu激活函数： f(x) = \begin{cases} x, & \text{x>0} \\ 0, & \text{x≤0} \end{cases} 激活函数的作用 引入非线性因素。 在我们面对线性可分的数据集的时候，简单的用线性分类器即可解决分类问题。但是现实生活中的数据往往不是线性可分的，面对这样的数据，一般有两个方法：引入非线性函数、线性变换。 线性变换就是把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类 激活函数的特点 unit：线性分界 – 几乎已经不用了 sigmoid：非线性分界 – 两端软饱和，输出为 (0,1)区间 – 两端有梯度消失问题 – 因为输出恒正，可能有 zig现象 tanh：非线性分界 ：非线性分界 – 两端软饱和，输出为 (-1, 1) 区间 – 仍然存在梯度消失问题 – 没有 zig，收敛更快 (LeCun 1989) ReLU：非线性分界 – 左侧硬饱和，右无输出为 [0,+∞)区间 – 左侧会出现梯度一直为 0的情况，导致神经元 不再更新（死亡） – 改善了梯度弥散 – 同样存在 zig]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>感知器</tag>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络实现]]></title>
    <url>%2F2019%2F01%2F13%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[net新建netWork.py,添加以下代码 准备import tensorflow as tfimport config# 卷积操作def conv2d(name, l_input, w, b): return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;), b), name=name)# 最大下采样操作def max_pool(name, l_input, k): return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=&apos;SAME&apos;, name=name)# 归一化操作def norm(name, l_input, lsize=4): return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name) LeNetdef LeNet(inputs): mu = 0 sigma = 0.1 print(inputs.shape) # TODO: 第一层卷积：输入=32x32x3, 输出=28x28x6 conv1_w = tf.Variable(tf.truncated_normal(shape=[5, 5, 3, 6], mean=mu, stddev=sigma)) conv1_b = tf.Variable(tf.zeros(6)) conv1 = tf.nn.conv2d(inputs, conv1_w, strides=[1, 1, 1, 1], padding=&apos;VALID&apos;) + conv1_b print(conv1.shape) # 激活函数 conv1_out = tf.nn.relu(conv1) # 池化层， 输入=28x28x6, 输出=14x14x6 pool_1 = tf.nn.max_pool(conv1_out, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;VALID&apos;) print(pool_1.shape) # TODO: 第二层卷积： 输入=14x14x6， 输出=10x10x16 conv2_w = tf.Variable(tf.truncated_normal(shape=[5, 5, 6, 16], mean=mu, stddev=sigma)) conv2_b = tf.Variable(tf.zeros(16)) conv2 = tf.nn.conv2d(pool_1, conv2_w, strides=[1, 1, 1, 1], padding=&apos;VALID&apos;) + conv2_b print(conv2.shape) # 激活函数 conv2_out = tf.nn.relu(conv2) # 池化层， 输入=10x10x16, 输出=5x5x16 pool_2 = tf.nn.max_pool(conv2_out, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;VALID&apos;) print(pool_2.shape) # Flatten 输入=5x5x16， 输出=400 pool_2_flat = tf.reshape(pool_2, [-1, 400]) # TODO: 第三层全连接层， 输入=400， 输出=120 fc1_w = tf.Variable(tf.truncated_normal(shape=[400, 120], mean=mu, stddev=sigma)) fc1_b = tf.Variable(tf.zeros(120)) fc1 = tf.matmul(pool_2_flat, fc1_w) + fc1_b # 激活函数 fc1_out = tf.nn.relu(fc1) print(fc1_out.shape) # TODO: 第四层全连接层： 输入=120， 输出=84 fc2_w = tf.Variable(tf.truncated_normal(shape=[120, 84], mean=mu, stddev=sigma)) fc2_b = tf.Variable(tf.zeros(84)) fc2 = tf.matmul(fc1_out, fc2_w) + fc2_b # 激活函数 fc2_out = tf.nn.relu(fc2) print(fc2_out.shape) # TODO: 第五层全连接层： 输入=84， 输出=10 fc3_w = tf.Variable(tf.truncated_normal(shape=[84, 10], mean=mu, stddev=sigma)) fc3_b = tf.Variable(tf.zeros(10)) fc3_out = tf.matmul(fc2_out, fc3_w) + fc3_b print(fc3_out.shape) return fc3_out alex_netdef alex_net(_X, _weights, _biases, _dropout): # 向量转为矩阵 # _X = tf.reshape(_X, shape=[-1, 28, 28, 3]) print(_X.shape) # TODO: 第一层卷积： conv1 = conv2d(&apos;conv1&apos;, _X, _weights[&apos;wc1&apos;], _biases[&apos;bc1&apos;]) # 下采样层 pool1 = max_pool(&apos;pool1&apos;, conv1, k=2) # 归一化层 norm1 = norm(&apos;norm1&apos;, pool1, lsize=4) print(norm1.shape) # TODO: 第二层卷积： conv2 = conv2d(&apos;conv2&apos;, norm1, _weights[&apos;wc2&apos;], _biases[&apos;bc2&apos;]) # 下采样 pool2 = max_pool(&apos;pool2&apos;, conv2, k=2) # 归一化 norm2 = norm(&apos;norm2&apos;, pool2, lsize=4) print(norm2.shape) # TODO: 第三层卷积： conv3 = conv2d(&apos;conv3&apos;, norm2, _weights[&apos;wc3&apos;], _biases[&apos;bc3&apos;]) # 归一化 norm3 = norm(&apos;norm3&apos;, conv3, lsize=4) print(norm3.shape) # TODO: 第四层卷积 # 卷积 conv4 = conv2d(&apos;conv4&apos;, norm3, _weights[&apos;wc4&apos;], _biases[&apos;bc4&apos;]) # 归一化 norm4 = norm(&apos;norm4&apos;, conv4, lsize=4) print(norm4.shape) # TODO: 第五层卷积 # 卷积 conv5 = conv2d(&apos;conv5&apos;, norm4, _weights[&apos;wc5&apos;], _biases[&apos;bc5&apos;]) # 下采样 pool5 = max_pool(&apos;pool5&apos;, conv5, k=2) # 归一化 norm5 = norm(&apos;norm5&apos;, pool5, lsize=4) print(norm5.shape) # TODO: 第六层全连接层 # 先把特征图转为向量 dense1 = tf.reshape(norm5, [-1, _weights[&apos;wd1&apos;].get_shape().as_list()[0]]) dense1 = tf.nn.relu(tf.matmul(dense1, _weights[&apos;wd1&apos;]) + _biases[&apos;bd1&apos;], name=&apos;fc1&apos;) dense1 = tf.nn.dropout(dense1, _dropout) print(dense1.shape) # TODO: 第七层全连接层： dense2 = tf.nn.relu(tf.matmul(dense1, _weights[&apos;wd2&apos;]) + _biases[&apos;bd2&apos;], name=&apos;fc2&apos;) # Relu activation dense2 = tf.nn.dropout(dense2, _dropout) print(dense2.shape) # TODO: 第八层全连接层： # 网络输出层 out = tf.matmul(dense2, _weights[&apos;out&apos;]) + _biases[&apos;out&apos;] print(out.shape) return out cnn_1def CNN_1(inputs): # (32x32x3)--&gt;(32x32x64) with tf.name_scope(&apos;conv1&apos;): h_conv1 = tf.layers.conv2d(inputs, 64, [2, 2], padding=&apos;SAME&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(h_conv1.shape) # 构建池化层--采用最大池化 # (32X32X64)--&gt;(16X16X64) with tf.name_scope(&apos;pool1&apos;): h_pool1 = tf.layers.max_pooling2d(h_conv1, pool_size=[2, 2], strides=[2, 2], padding=&apos;SAME&apos;) print(h_pool1.shape) # 构建第二层卷积计算层--(16x16x64)--&gt;(16x16x128). with tf.name_scope(&apos;conv2&apos;): h_conv2 = tf.layers.conv2d(h_pool1, 128, [4, 4], padding=&apos;SAME&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(h_conv2.shape) # 构建第二个池化层(16x16x128)--&gt;(8x8x128) with tf.name_scope(&apos;pool2&apos;): h_pool2 = tf.layers.max_pooling2d(h_conv2, pool_size=[2, 2], strides=[2, 2], padding=&apos;SAME&apos;) print(h_pool2.shape) # 构建全连接层--(8x8x128)--&gt;(1024) with tf.name_scope(&apos;fc1&apos;): h_pool2_flat = tf.layers.flatten(h_pool2) h_fc1 = tf.layers.dense(h_pool2_flat, 1024, activation=tf.nn.relu) print(h_fc1.shape) # Dropout--防止过拟合 with tf.name_scope(&apos;dropout&apos;): # keep_prob = tf.placeholder(tf.float32) h_fc_drop = tf.nn.dropout(h_fc1, keep_prob=config.keep_prob) # 构建全连接层--1024--&gt;512 with tf.name_scope(&apos;fc2&apos;): fc2 = tf.layers.dense(h_fc_drop, 512, activation=tf.nn.relu) print(fc2.shape) # 构建全连接层--512--&gt;10 with tf.name_scope(&apos;fc3&apos;): out = tf.layers.dense(fc2, 10, activation=None) print(out.shape) return out VGG16def VGG16(inputs): print(inputs.shape) # (32x32x3) --&gt; (32x32x64) with tf.name_scope(&apos;conv_1&apos;): conv_1_out = tf.layers.conv2d(inputs, 64, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_1_out.shape) # (32x32x64) --&gt; (32x32x64) with tf.name_scope(&apos;conv_2&apos;): conv_2_out = tf.layers.conv2d(conv_1_out, 64, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_2_out.shape) # (32x32x64) --&gt; (16x16x64) with tf.name_scope(&apos;pool_1&apos;): pool_1_out = tf.layers.max_pooling2d(conv_2_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;) print(pool_1_out.shape) # (16x16x64) --&gt; (16x16x128) with tf.name_scope(&apos;conv_3&apos;): conv_3_out = tf.layers.conv2d(pool_1_out, 128, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_3_out.shape) # (16x16x128) --&gt; (16x16x128) with tf.name_scope(&apos;conv_4&apos;): conv_4_out = tf.layers.conv2d(conv_3_out, 128, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_4_out.shape) # (16x16x128) --&gt; (8x8x128) with tf.name_scope(&apos;pool_2&apos;): pool_2_out = tf.layers.max_pooling2d(conv_4_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;) print(pool_2_out.shape) # (8x8x128) --&gt; (8x8x256) with tf.name_scope(&apos;conv_5&apos;): conv_5_out = tf.layers.conv2d(pool_2_out, 256, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_5_out.shape) # (8x8x256) --&gt; (8x8x256) with tf.name_scope(&apos;conv_6&apos;): conv_6_out = tf.layers.conv2d(conv_5_out, 256, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_6_out.shape) # (8x8x256) --&gt; (8x8x256) with tf.name_scope(&apos;conv_7&apos;): conv_7_out = tf.layers.conv2d(conv_6_out, 256, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_7_out.shape) # (8x8x256) --&gt; (4x4x256) with tf.name_scope(&apos;pool_3&apos;): pool_3_out = tf.layers.max_pooling2d(conv_7_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;) print(pool_3_out.shape) # (4x4x256) --&gt; (4x4x512) with tf.name_scope(&apos;conv_8&apos;): conv_8_out = tf.layers.conv2d(pool_3_out, 512, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_8_out.shape) # (4x4x512) --&gt; (4x4x512) with tf.name_scope(&apos;conv_9&apos;): conv_9_out = tf.layers.conv2d(conv_8_out, 512, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_9_out.shape) # (4x4x512) --&gt; (4x4x512) with tf.name_scope(&apos;conv_10&apos;): conv_10_out = tf.layers.conv2d(conv_9_out, 512, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_10_out.shape) # (4x4x512) --&gt; (2x2x512) with tf.name_scope(&apos;pool_4&apos;): pool_4_out = tf.layers.max_pooling2d(conv_10_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;) print(pool_4_out.shape) # (2x2x512) --&gt; (2x2x512) with tf.name_scope(&apos;conv_11&apos;): conv_11_out = tf.layers.conv2d(pool_4_out, 512, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_11_out.shape) # (2x2x512) --&gt; (2x2x512) with tf.name_scope(&apos;conv_12&apos;): conv_12_out = tf.layers.conv2d(conv_11_out, 512, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_12_out.shape) # (2x2x512) --&gt; (2x2x512) with tf.name_scope(&apos;conv_13&apos;): conv_13_out = tf.layers.conv2d(conv_12_out, 512, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_13_out.shape) # (2x2x512) --&gt; (1x1x512) with tf.name_scope(&apos;pool_5&apos;): pool_5_out = tf.layers.max_pooling2d(conv_13_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;) print(pool_5_out.shape) # (1x1x512) --&gt; 512 with tf.name_scope(&apos;fc_1&apos;): pool_5_outz_flat = tf.layers.flatten(pool_5_out) fc_1_out = tf.layers.dense(pool_5_outz_flat, 512, activation=tf.nn.relu) fc_1_drop = tf.nn.dropout(fc_1_out, keep_prob=config.keep_prob) print(fc_1_drop.shape) # 512 --&gt; 512 with tf.name_scope(&apos;fc_2&apos;): fc_2_out = tf.layers.dense(fc_1_drop, 512, activation=tf.nn.relu) fc_2_drop = tf.nn.dropout(fc_2_out, keep_prob=config.keep_prob) print(fc_2_drop.shape) # 512 --&gt; 10 with tf.name_scope(&apos;fc_3&apos;): fc_3_out = tf.layers.dense(fc_2_drop, 10, activation=None) print(fc_3_out.shape) return fc_3_out vgg19def VGG19(inputs): print(inputs.shape) # (32x32x3) --&gt; (32x32x64) with tf.name_scope(&apos;conv_1&apos;): conv_1_out = tf.layers.conv2d(inputs, 64, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_1_out.shape) # (32x32x64) --&gt; (32x32x64) with tf.name_scope(&apos;conv_2&apos;): conv_2_out = tf.layers.conv2d(conv_1_out, 64, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_2_out.shape) # (32x32x64) --&gt; (16x16x64) with tf.name_scope(&apos;pool_1&apos;): pool_1_out = tf.layers.max_pooling2d(conv_2_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;) print(pool_1_out.shape) # (16x16x64) --&gt; (16x16x128) with tf.name_scope(&apos;conv_3&apos;): conv_3_out = tf.layers.conv2d(pool_1_out, 128, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_3_out.shape) # (16x16x128) --&gt; (16x16x128) with tf.name_scope(&apos;conv_4&apos;): conv_4_out = tf.layers.conv2d(conv_3_out, 128, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_4_out.shape) # (16x16x128) --&gt; (8x8x128) with tf.name_scope(&apos;pool_2&apos;): pool_2_out = tf.layers.max_pooling2d(conv_4_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;) print(pool_2_out.shape) # (8x8x128) --&gt; (8x8x256) with tf.name_scope(&apos;conv_5&apos;): conv_5_out = tf.layers.conv2d(pool_2_out, 256, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_5_out.shape) # (8x8x256) --&gt; (8x8x256) with tf.name_scope(&apos;conv_6&apos;): conv_6_out = tf.layers.conv2d(conv_5_out, 256, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_6_out.shape) # (8x8x256) --&gt; (8x8x256) with tf.name_scope(&apos;conv_7&apos;): conv_7_out = tf.layers.conv2d(conv_6_out, 256, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_7_out.shape) # (8x8x256) --&gt; (8x8x256) with tf.name_scope(&apos;conv_8&apos;): conv_8_out = tf.layers.conv2d(conv_7_out, 256, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_8_out.shape) # (8x8x256) --&gt; (4x4x256) with tf.name_scope(&apos;pool_3&apos;): pool_3_out = tf.layers.max_pooling2d(conv_8_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;) print(pool_3_out.shape) # (4x4x256) --&gt; (4x4x512) with tf.name_scope(&apos;conv_9&apos;): conv_9_out = tf.layers.conv2d(pool_3_out, 512, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_9_out.shape) # (4x4x512) --&gt; (4x4x512) with tf.name_scope(&apos;conv_10&apos;): conv_10_out = tf.layers.conv2d(conv_9_out, 512, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_10_out.shape) # (4x4x512) --&gt; (4x4x512) with tf.name_scope(&apos;conv_11&apos;): conv_11_out = tf.layers.conv2d(conv_10_out, 512, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_11_out.shape) # (4x4x512) --&gt; (4x4x512) with tf.name_scope(&apos;conv_12&apos;): conv_12_out = tf.layers.conv2d(conv_11_out, 512, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_12_out.shape) # (4x4x512) --&gt; (2x2x512) with tf.name_scope(&apos;pool_4&apos;): pool_4_out = tf.layers.max_pooling2d(conv_12_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;) print(pool_4_out.shape) # (2x2x512) --&gt; (2x2x512) with tf.name_scope(&apos;conv_13&apos;): conv_13_out = tf.layers.conv2d(pool_4_out, 512, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_13_out.shape) # (2x2x512) --&gt; (2x2x512) with tf.name_scope(&apos;conv_14&apos;): conv_14_out = tf.layers.conv2d(conv_13_out, 512, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_14_out.shape) # (2x2x512) --&gt; (2x2x512) with tf.name_scope(&apos;conv_15&apos;): conv_15_out = tf.layers.conv2d(conv_14_out, 512, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_15_out.shape) # (2x2x512) --&gt; (2x2x512) with tf.name_scope(&apos;conv_16&apos;): conv_16_out = tf.layers.conv2d(conv_15_out, 512, [3, 3], padding=&apos;same&apos;, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1)) print(conv_16_out.shape) # (2x2x512) --&gt; (1x1x512) with tf.name_scope(&apos;pool_5&apos;): pool_5_out = tf.layers.max_pooling2d(conv_16_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;) print(pool_5_out.shape) # (1x1x512) --&gt; 512 with tf.name_scope(&apos;fc_1&apos;): pool_5_outz_flat = tf.layers.flatten(pool_5_out) fc_1_out = tf.layers.dense(pool_5_outz_flat, 512, activation=tf.nn.relu) fc_1_drop = tf.nn.dropout(fc_1_out, keep_prob=config.keep_prob) print(fc_1_drop.shape) # 512 --&gt; 512 with tf.name_scope(&apos;fc_2&apos;): fc_2_out = tf.layers.dense(fc_1_drop, 512, activation=tf.nn.relu) fc_2_drop = tf.nn.dropout(fc_2_out, keep_prob=config.keep_prob) print(fc_2_drop.shape) # 512 --&gt; 10 with tf.name_scope(&apos;fc_3&apos;): fc_3_out = tf.layers.dense(fc_2_drop, 10, activation=None) print(fc_3_out.shape) return fc_3_out 读取数据加载数据import pickleimport numpy as npfrom sklearn.preprocessing import MinMaxScaler, LabelBinarizerdef load_cifar10_batch(path, batch_id): &quot;&quot;&quot; 加载batch的数据 :param path: 数据存储的目录 :param batch_id:batch的编号 :return:features and labels &quot;&quot;&quot; with open(path + &apos;/data_batch_&apos; + str(batch_id), mode=&apos;rb&apos;) as file: batch = pickle.load(file, encoding=&apos;latin1&apos;) # features and labels features = batch[&apos;data&apos;].reshape((len(batch[&apos;data&apos;]), 3, 32, 32)).transpose(0, 2, 3, 1) labels = batch[&apos;labels&apos;] return features, labels 数据预处理def pre_processing_data(x_train, y_train, x_test, y_test): # features minmax = MinMaxScaler() # 重塑数据 # (50000, 32, 32, 3) --&gt; (50000, 32*32*3) x_train_rows = x_train.reshape(x_train.shape[0], 32*32*3) # (10000, 32, 32, 3) --&gt; (10000, 32*32*3) x_test_rows = x_test.reshape(x_test.shape[0], 32*32*3) # 归一化 x_train_norm = minmax.fit_transform(x_train_rows) x_test_norm = minmax.fit_transform(x_test_rows) # 重塑数据 x_train = x_train_norm.reshape(x_train_norm.shape[0], 32, 32, 3) x_test = x_test_norm.reshape(x_test_norm.shape[0], 32, 32, 3) # labels # 对标签进行one-hot n_class = 10 label_binarizer = LabelBinarizer().fit(np.array(range(n_class))) y_train = label_binarizer.transform(y_train) y_test = label_binarizer.transform(y_test) return x_train, y_train, x_test, y_test 数据准备新建Read_date.pydef cifar10_data(): # 加载训练数据 cifar10_path = &apos;data&apos; # 一共是有5个batch的训练数据 x_train, y_train = load_cifar10_batch(cifar10_path, 1) for n in range(2, 6): features, labels = load_cifar10_batch(cifar10_path, n) x_train = np.concatenate([x_train, features]) y_train = np.concatenate([y_train, labels]) # 加载测试数据 with open(cifar10_path + &apos;/test_batch&apos;, mode=&apos;rb&apos;) as file: batch = pickle.load(file, encoding=&apos;latin1&apos;) x_test = batch[&apos;data&apos;].reshape((len(batch[&apos;data&apos;]), 3, 32, 32)).transpose(0, 2, 3, 1) y_test = batch[&apos;labels&apos;] x_train, y_train, x_test, y_test = pre_processing_data(x_train, y_train, x_test, y_test) return x_train, y_train, x_test, y_test config新建config.py；复制以下代码 初始化卷积神经网络参数import tensorflow as tfimport matplotlib.pyplot as pltkeep_prob = 0.8epochs = 20batch_size = 128n_classes = 10 # 总共10类 定义placeholderinputs = tf.placeholder(tf.float32, [None, 32, 32, 3], name=&apos;inputs&apos;)targets = tf.placeholder(tf.float32, [None, 10], name=&apos;logits&apos;)learning_rate = 0.001 显示图片def show_images(images): fig, axes = plt.subplots(nrows=3, ncols=3, sharex=True, sharey=True, figsize=(9, 9)) img = images[: 60] for image, row in zip([img[: 20], img[20: 40], img[40: 60]], axes): for img, ax in zip(image, row): ax.imshow(img) ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) fig.tight_layout(pad=0.1) # plt.show() 存储网络参数(alexnet)weights = &#123; &apos;wc1&apos;: tf.Variable(tf.random_normal(shape=[11, 11, 3, 96])), &apos;wc2&apos;: tf.Variable(tf.random_normal(shape=[5, 5, 96, 256])), &apos;wc3&apos;: tf.Variable(tf.random_normal(shape=[3, 3, 256, 384])), &apos;wc4&apos;: tf.Variable(tf.random_normal(shape=[3, 3, 384, 384])), &apos;wc5&apos;: tf.Variable(tf.random_normal(shape=[3, 3, 384, 256])), &apos;wd1&apos;: tf.Variable(tf.random_normal(shape=[4*4*256, 4096])), &apos;wd2&apos;: tf.Variable(tf.random_normal(shape=[4096, 1024])), &apos;out&apos;: tf.Variable(tf.random_normal(shape=[1024, n_classes]))&#125;biases = &#123; &apos;bc1&apos;: tf.Variable(tf.random_normal([96])), &apos;bc2&apos;: tf.Variable(tf.random_normal([256])), &apos;bc3&apos;: tf.Variable(tf.random_normal([384])), &apos;bc4&apos;: tf.Variable(tf.random_normal([384])), &apos;bc5&apos;: tf.Variable(tf.random_normal([256])), &apos;bd1&apos;: tf.Variable(tf.random_normal([4096])), &apos;bd2&apos;: tf.Variable(tf.random_normal([1024])), &apos;out&apos;: tf.Variable(tf.random_normal([n_classes]))&#125;​```## 测试模型新建TestModel.py,复制以下代码### 模型评估​```import tensorflow as tfimport configimport Read_datadef evaluate(X_data, y_data, inputs, logits, targets): batch_size = config.batch_size correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(targets, 1)) accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) num_examples = len(X_data) total_accuracy = 0 sess = tf.get_default_session() for offset in range(0, num_examples, batch_size): batch_x, batch_y = X_data[offset:offset+batch_size], y_data[offset:offset+batch_size] accuracy = sess.run(accuracy_operation, feed_dict=&#123;inputs: batch_x, targets: batch_y&#125;) total_accuracy += (accuracy * len(batch_x)) return total_accuracy / num_examples​```### 读取模型​```def run(inputs, logits, targets): print(&apos;TESTING....&apos;) x_train, y_train, x_test, y_test = Read_data.cifar10_data() saver = tf.train.Saver() with tf.Session() as sess: print(&quot;Evaluate The Model&quot;) # TODO: 读取模型 saver.restore(sess, &apos;./model/cifar.model&apos;) test_accuracy = evaluate(x_test, y_test, inputs, logits, targets) print(&quot;Test Accuracy = &#123;:.3f&#125;&quot;.format(test_accuracy))​```## 训练创建TrainModel.py,复制以下代码​```from sklearn.model_selection import train_test_splitimport tensorflow as tfimport Read_dataimport configimport TestModeldef run(logits): # 读取数据 global train_loss x_train, y_train, x_test, y_test = Read_data.cifar10_data() print(y_train.shape) # 构造验证集和训练集 train_rate = 0.8 x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, train_size=train_rate) # 初始化卷积神经网络参数 epochs = config.epochs batch_size = config.batch_size # 定义输入和标签的placeholder inputs = config.inputs targets = config.targets # TODO: 计算损失值并初始化optimizer learning_rate = config.learning_rate cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=targets) loss_operation = tf.reduce_mean(cross_entropy) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) training_operation = optimizer.minimize(loss_operation) # TODO: 初始化变量 init = tf.global_variables_initializer() print(&quot;FUNCTION READY!!&quot;) # TODO: 保存模型 saver = tf.train.Saver() # TODO: 训练模型 with tf.Session() as sess: sess.run(init) num_examples = len(x_train) print(&quot;Training.....&quot;) for n in range(epochs): for offset in range(0, num_examples, batch_size): batch_x, batch_y = x_train[offset:offset+batch_size], y_train[offset:offset+batch_size] train_loss, _ = sess.run([loss_operation, training_operation], feed_dict=&#123;inputs: batch_x, targets: batch_y&#125;) print(&quot;EPOCH &#123;&#125; ...&quot;.format(n + 1)) print(&quot;Train Loss &#123;:.4f&#125;&quot; .format(train_loss)) print(&quot;Validation Accuracy = &#123;:.3f&#125;&quot;.format(TestModel.evaluate(x_validation, y_validation, inputs, logits, targets))) saver.save(sess, &apos;./model/cifar.model&apos;) print(&quot;Model saved&quot;)​```## 测试图片新建testPhoto.py​```import numpy as npimport tensorflow as tffrom PIL import Imageimport Networkimport configdef main(): with tf.Session() as sess: photo_classes = &#123;0: &apos;airplane&apos;, 1: &apos;automobile&apos;, 2: &apos;bird&apos;, 3: &apos;cat&apos;, 4: &apos;deer&apos;, 5: &apos;dog&apos;, 6: &apos;frog&apos;, 7: &apos;horse&apos;, 8: &apos;ship&apos;, 9: &apos;truck&apos;&#125; logits = Network.VGG16(config.inputs) x = config.inputs saver = tf.train.Saver() saver.restore(sess, &apos;./model/cifar.model&apos;) # input im = Image.open(&apos;image/dog-3.jpg&apos;) # im.show() im = im.resize((32, 32)) # print(im.size, im.mode) im = np.array(im).astype(np.float32) im = np.reshape(im, [-1, 32*32*3]) im = (im - (255 / 2.0)) / 255 batch_xs = np.reshape(im, [-1, 32, 32, 3]) output = sess.run(logits, feed_dict=&#123;x: batch_xs&#125;) print(output) print(&apos;the out put is :&apos;, photo_classes[np.argmax(output)])if __name__ == &apos;__main__&apos;: main()​```## 主程序新建main.py​```import TrainModelimport TestModelimport configimport Network# logits = Network.LeNet(Setting.inputs)# logits = Network.alex_net(Setting.inputs, Setting.weights, Setting.biases, Setting.keep_prob)logits = Network.CNN_1(config.inputs)# logits = Network.VGG16(config.inputs)# logits = Network.VGG19(Setting.inputs)TrainModel.run(logits)TestModel.run(config.inputs, logits, config.targets)​```]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keras实现CNN]]></title>
    <url>%2F2019%2F01%2F13%2Fkeras%E5%AE%9E%E7%8E%B0CNN%2F</url>
    <content type="text"><![CDATA[0.导入环境import osfrom tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tffrom keras.layers.core import Dense, Flattenfrom keras.layers.convolutional import Conv2Dfrom keras.layers.pooling import MaxPooling2Dfrom keras.objectives import categorical_crossentropyfrom keras import backend as KK.image_data_format()os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;] = &apos;2&apos; 1.数据准备# 使用tensorflow自带的工具加载MNIST手写数字集合mnist = input_data.read_data_sets(&apos;data&apos;, one_hot=True)# 查看数据的维度和target的维度print(mnist.train.images.shape)print(mnist.train.labels.shape) 2.准备好palceholderx = tf.placeholder(tf.float32, [None, 784])y = tf.placeholder(tf.float32, [None, 10])learnRate = tf.placeholder(tf.float32) 3.构建网络计算图结构# 把输入数据reshape--28x28=784, 单通道， -1表示Nonewith tf.name_scope(&apos;reshape&apos;): x_image = tf.reshape(x, [-1, 28, 28, 1])# 构建第一层卷积计算层--将一个灰度图像映射到32个feature maps, 卷积核为5x5net = Conv2D(32, kernel_size=[5, 5], strides=[1, 1], activation=&apos;relu&apos;, padding=&apos;same&apos;, input_shape=[28, 28, 1])(x_image)# 构建池化层--采用最大池化net = MaxPooling2D(pool_size=[2, 2])(net)# 构建第二层卷积计算层--maps 32 feature maps to 64.net = Conv2D(64, kernel_size=[5, 5], strides=[1, 1], activation=&apos;relu&apos;, padding=&apos;same&apos;)(net)# 构建第二层池化层--采用最大池化net = MaxPooling2D(pool_size=[2, 2])(net)# 构建全连接层--经过的两层的下采样（池化），28x28x1的图像--&gt;7x7x64，然后映射到1024个特征net = Flatten()(net)net = Dense(1024, activation=&apos;relu&apos;)(net)# 构建第二层全连接层--将1024个特性映射到10个类，每个类对应一个数字net = Dense(10, activation=&apos;softmax&apos;)(net) 4.计算损失值并初始化optimizercross_entropy = tf.reduce_mean(categorical_crossentropy(y, net))l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])total_loss = cross_entropy + 7e-5*l2_losstrain_step = tf.train.AdamOptimizer(learnRate).minimize(total_loss) 5.初始化变量init = tf.global_variables_initializer()print(&quot;FUNCTION READY!!&quot;) 6.在会话中执行网络定义的运算with tf.Session() as sess: sess.run(init) for step in range(3000): batch_xs, batch_ys = mnist.train.next_batch(100) lr = 0.01 _, loss, l2_loss_value, total_loss_value = sess.run( [train_step, cross_entropy, l2_loss, total_loss], feed_dict=&#123;x: batch_xs, y: batch_ys, learnRate: lr&#125;) if (step + 1) % 100 == 0: print(&quot;step %d, entropy loss: %f, l2_loss: %f, total loss: %f&quot; % (step + 1, loss, l2_loss_value, total_loss_value)) # 验证训练的模型 correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) print(&quot;Train accuracy:&quot;, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)) if (step + 1) % 1000 == 0: print(&quot;Text accuracy:&quot;, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys&#125;))]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度卷积对抗生成网络-DCGAN]]></title>
    <url>%2F2019%2F01%2F13%2F%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E5%AF%B9%E6%8A%97%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C-DCGAN%2F</url>
    <content type="text"><![CDATA[导入环境import numpy as npimport tensorflow as tfimport matplotlib.pyplot as pltfrom tensorflow.examples.tutorials.mnist import input_data 数据准备运行程序下面代码，mnist数据集会自动下载mnist = input_data.read_data_sets(&apos;data&apos;) 获得输入数据def get_inputs(noise_dim, image_height, image_width, image_depth): # 真实数据 inputs_real = tf.placeholder(tf.float32, [None, image_height, image_width, image_depth], name=&apos;inputs_real&apos;) # 噪声数据 inputs_noise = tf.placeholder(tf.float32, [None, noise_dim], name=&apos;inputs_noise&apos;) return inputs_real, inputs_noise 生成器def get_generator(noise_img, output_dim, is_train=True, alpha=0.01): with tf.variable_scope(&quot;generator&quot;, reuse=(not is_train)): # 100 x 1 to 4 x 4 x 512 # 全连接层 layer1 = tf.layers.dense(noise_img, 4 * 4 * 512) layer1 = tf.reshape(layer1, [-1, 4, 4, 512]) # batch normalization layer1 = tf.layers.batch_normalization(layer1, training=is_train) # Leaky ReLU layer1 = tf.maximum(alpha * layer1, layer1) # dropout layer1 = tf.nn.dropout(layer1, keep_prob=0.8) # 4 x 4 x 512 to 7 x 7 x 256 layer2 = tf.layers.conv2d_transpose(layer1, 256, 4, strides=1, padding=&apos;valid&apos;) layer2 = tf.layers.batch_normalization(layer2, training=is_train) layer2 = tf.maximum(alpha * layer2, layer2) layer2 = tf.nn.dropout(layer2, keep_prob=0.8) # 7 x 7 256 to 14 x 14 x 128 layer3 = tf.layers.conv2d_transpose(layer2, 128, 3, strides=2, padding=&apos;same&apos;) layer3 = tf.layers.batch_normalization(layer3, training=is_train) layer3 = tf.maximum(alpha * layer3, layer3) layer3 = tf.nn.dropout(layer3, keep_prob=0.8) # 14 x 14 x 128 to 28 x 28 x 1 logits = tf.layers.conv2d_transpose(layer3, output_dim, 3, strides=2, padding=&apos;same&apos;) # MNIST原始数据集的像素范围在0-1，这里的生成图片范围为(-1,1) # 因此在训练时，记住要把MNIST像素范围进行resize outputs = tf.tanh(logits) return outputs 判别器def get_discriminator(inputs_img, reuse=False, alpha=0.01): with tf.variable_scope(&quot;discriminator&quot;, reuse=reuse): # 28 x 28 x 1 to 14 x 14 x 128 # 第一层不加入BN layer1 = tf.layers.conv2d(inputs_img, 128, 3, strides=2, padding=&apos;same&apos;) layer1 = tf.maximum(alpha * layer1, layer1) layer1 = tf.nn.dropout(layer1, keep_prob=0.8) # 14 x 14 x 128 to 7 x 7 x 256 layer2 = tf.layers.conv2d(layer1, 256, 3, strides=2, padding=&apos;same&apos;) layer2 = tf.layers.batch_normalization(layer2, training=True) layer2 = tf.maximum(alpha * layer2, layer2) layer2 = tf.nn.dropout(layer2, keep_prob=0.8) # 7 x 7 x 256 to 4 x 4 x 512 layer3 = tf.layers.conv2d(layer2, 512, 3, strides=2, padding=&apos;same&apos;) layer3 = tf.layers.batch_normalization(layer3, training=True) layer3 = tf.maximum(alpha * layer3, layer3) layer3 = tf.nn.dropout(layer3, keep_prob=0.8) # 4 x 4 x 512 to 4*4*512 x 1 flatten = tf.reshape(layer3, (-1, 4 * 4 * 512)) logits = tf.layers.dense(flatten, 1) outputs = tf.sigmoid(logits) return logits, outputs 目标函数def get_loss(inputs_real, inputs_noise, image_depth, smooth=0.1): g_outputs = get_generator(inputs_noise, image_depth, is_train=True) d_logits_real, d_outputs_real = get_discriminator(inputs_real) d_logits_fake, d_outputs_fake = get_discriminator(g_outputs, reuse=True) # 计算Loss g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_outputs_fake) * (1 - smooth))) d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_outputs_real) * ( 1 - smooth))) d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_outputs_fake))) d_loss = tf.add(d_loss_real, d_loss_fake) return g_loss, d_loss 优化器def get_optimizer(g_loss, d_loss, learning_rate=0.001): train_vars = tf.trainable_variables() g_vars = [var for var in train_vars if var.name.startswith(&quot;generator&quot;)] d_vars = [var for var in train_vars if var.name.startswith(&quot;discriminator&quot;)] # Optimizer with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): g_opt = tf.train.RMSPropOptimizer(learning_rate).minimize(g_loss, var_list=g_vars) d_opt = tf.train.RMSPropOptimizer(learning_rate).minimize(d_loss, var_list=d_vars) return g_opt, d_opt 显示图片def plot_images(samples): fig, axes = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True, figsize=(7, 7)) for img, ax in zip(samples, axes.flatten()): ax.imshow(img.reshape((28, 28)), cmap=&apos;Greys_r&apos;) ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) fig.tight_layout(pad=0) plt.show()def show_generator_output(sess, n_images, inputs_noise, output_dim): noise_shape = inputs_noise.get_shape().as_list()[-1] # 生成噪声图片 examples_noise = np.random.uniform(-1, 1, size=[n_images, noise_shape]) samples = sess.run(get_generator(inputs_noise, output_dim, False), feed_dict=&#123;inputs_noise: examples_noise&#125;) result = np.squeeze(samples, -1) return result 开始训练# 定义参数batch_size = 64noise_size = 100epochs = 5n_samples = 25learning_rate = 0.001def train(noise_size, data_shape, batch_size, n_samples): # 存储loss losses = [] steps = 0 inputs_real, inputs_noise = get_inputs(noise_size, data_shape[1], data_shape[2], data_shape[3]) g_loss, d_loss = get_loss(inputs_real, inputs_noise, data_shape[-1]) print(&quot;FUNCTION READY!!&quot;) for _ in range(6): g_train_opt, d_train_opt = get_optimizer(g_loss, d_loss, learning_rate) print(&quot;TRAINING....&quot;) #exit() with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # 迭代epoch for e in range(epochs): for batch_i in range(mnist.train.num_examples // batch_size): steps += 1 batch = mnist.train.next_batch(batch_size) batch_images = batch[0].reshape((batch_size, data_shape[1], data_shape[2], data_shape[3])) # scale to -1, 1 batch_images = batch_images * 2 - 1 # noise batch_noise = np.random.uniform(-1, 1, size=(batch_size, noise_size)) # run optimizer sess.run(g_train_opt, feed_dict=&#123;inputs_real: batch_images, inputs_noise: batch_noise&#125;) sess.run(d_train_opt, feed_dict=&#123;inputs_real: batch_images, inputs_noise: batch_noise&#125;) if steps % 101 == 0: train_loss_d = d_loss.eval(&#123;inputs_real: batch_images, inputs_noise: batch_noise&#125;) train_loss_g = g_loss.eval(&#123;inputs_real: batch_images, inputs_noise: batch_noise&#125;) losses.append((train_loss_d, train_loss_g)) print(&quot;Epoch &#123;&#125;/&#123;&#125;....&quot;.format(e + 1, epochs), &quot;Discriminator Loss: &#123;:.4f&#125;....&quot;.format(train_loss_d), &quot;Generator Loss: &#123;:.4f&#125;....&quot;.format(train_loss_g)) if e % 1 == 0: # 显示图片 samples = show_generator_output(sess, n_samples, inputs_noise, data_shape[-1]) plot_images(samples)with tf.Graph().as_default(): train(noise_size, [-1, 28, 28, 1], batch_size, n_samples) print(&quot;OPTIMIZER END!!&quot;)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>DCGAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNN实践之写文章]]></title>
    <url>%2F2019%2F01%2F13%2FRNN%2F</url>
    <content type="text"><![CDATA[文章下载下载文章或重新找一篇文章：https://pan.baidu.com/s/1-dZd1oKZSawCN0R7LQWz1g 导入环境import numpy as npimport tensorflow as tffrom tensorflow.contrib import rnnimport randomimport timefrom collections import Counterstart_time = time.time()tf.reset_default_graph()train_file = &apos;words.txt&apos; 简单时间处理def str_time(sec): if sec &lt; 60: return str(sec) + &quot; sec&quot; elif sec &lt; (60 * 60): return str(sec / 60) + &quot; min&quot; else: return str(sec / (60 * 60)) + &quot; hour&quot; 处理汉字def get_char(txt_file): labels = str() with open(file=txt_file, mode=&apos;rb&apos;) as f: for label in f: labels = label.decode(&quot;utf-8&quot;) return labels 处理多个中文文件def readfile(files): labels = list() for txt_file in files: target = get_char(txt_file) labels.append(target) return labels 将文本数组转换为向量def char_vector(files, num_map, label=None): word_size = len(num_map) vector = lambda word: num_map.get(word, word_size) if files: label = get_char(files) labels_vector = list(map(vector, label)) return labels_vector 样本预处理train_data = get_char(train_file)print(&quot;Loading training data...&quot;)print(len(train_data))counter = Counter(train_data)words = sorted(counter)words_size = len(words)words_num_map = dict(zip(words, range(words_size)))print(&quot;字表大小：&quot;, words_size)word_label = char_vector(train_file, words_num_map) 超参数设置learning_rate = 0.001epochs = 100000display_step = 1000n_input = 4 # 每次输入4个汉字， 预测第5个汉字# 隐层神经元n_hidden1 = 256n_hidden2 = 512n_hidden3 = 512keep_prob=0.8layer_num=3batch_size=1# 定义X, Y的placeholderx = tf.placeholder(&quot;float&quot;, [None, n_input, 1])y = tf.placeholder(&quot;float&quot;, [None, words_size])# 对 weights biases 初始值的定义weights = &#123; &apos;in&apos;: tf.Variable(tf.random_normal([n_input,n_hidden1])), &apos;out&apos;: tf.Variable(tf.random_normal([n_hidden2,words_size]))&#125;biases = &#123; # shape (128, ) &apos;in&apos;: tf.Variable(tf.constant(0.1, shape=[n_hidden1,])), # shape (10, ) &apos;out&apos;: tf.Variable(tf.constant(0.1, shape=[words_size, ]))&#125; 定义网络结构def lstm_call(): cell = tf.nn.rnn_cell.LSTMCell(num_units=n_hidden1, reuse=tf.get_variable_scope().reuse) return tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_prob)def RNN(x, weights, biases): x = tf.reshape(x, [batch_size, n_input, 1]) # (1,4,1) 相当于batch =1 # rnn cell = tf.contrib.rnn.BasicLSTMCell(n_hidden2) init_state = cell.zero_state(batch_size, dtype=tf.float32) # final_state 的维度是 batch * n_hidden --&gt; 1 * 512 # outputs 的维度是 batch * n_input(time_step) * n_hidden --&gt; 1 * 4 * 512 outputs, final_state = tf.nn.dynamic_rnn(cell, x, initial_state=init_state, time_major=False) # print (&quot;before unstack , output shape : &quot;,outputs.shape) # output shape : (1,3,512) (batch,time_step,cell_n_hidden) # unstack 更改维度 outputs = tf.unstack(tf.transpose(outputs, [1, 0, 2])) # 这个时候 outputs 变成了list # print (&quot;output shape[-1] 2: &quot;,outputs[-1].shape) # output shape : (3,1,512), outputs[-1] shape (1,512) results = tf.matmul(outputs[-1], weights[&apos;out&apos;]) + biases[&apos;out&apos;] # (1,112) 这个的表示意义是一个(1,112)的onehot，112表示字典里面总共有112个词汇 return results 计算损失值并初始化optimizerpredicted = RNN(x,weights,biases)# Loss optimizerloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predicted, labels=y))optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)# Model evaluationcorrect_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(y, 1))accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))# 保存模型save_dir = &quot;model/&quot;saver = tf.train.Saver(max_to_keep=1)# 初始化所有变量init = tf.global_variables_initializer() 训练及测试模型with tf.Session() as sess: sess.run(init) # 每训练一次，取后面四个文字向量当做输入，第五个文字向量当做标签用作计算loss offset = random.randint(0, n_input + 1) end_offset = n_input + 1 step = 0 loss_total = 0. acc_total = 0. # 恢复模型并继续训练 model = tf.train.latest_checkpoint(save_dir) print(&quot;model-ckpt:&quot;, model) start_epoch = 0 if model: saver.restore(sess, model) ind = model.find(&quot;-&quot;) start_epoch = int(model[ind + 1:]) print(start_epoch) step = start_epoch while step &lt; epochs: # 随机选择一个位置 if offset &gt; (len(train_data) - end_offset): offset = random.randint(0, n_input + 1) # 按照指定的位置获取后四个文字向量，当做输入 in_words = [[word_label[word]] for word in range(offset, offset + n_input)] in_words = np.reshape(np.array(in_words), [-1, n_input, 1]) out_onehot = np.zeros([words_size], dtype=float) out_onehot[word_label[offset + n_input]] = 1.0 # 所有的字都变成onehot out_onehot = np.reshape(out_onehot, [1, -1]) _, acc, loss_val, onehot_pred = sess.run([optimizer, accuracy, loss, predicted], feed_dict=&#123;x: in_words, y: out_onehot&#125;) loss_total += loss_val acc_total += acc if (step + 1) % display_step == 0: print(&quot;Iter= &quot; + str(step + 1) + &quot;, Average Loss= &quot; + &quot;&#123;:.6f&#125;&quot;.format(loss_total / display_step) + &quot;, Average Accuracy= &quot; + &quot;&#123;:.2f&#125;%&quot;.format(100 * acc_total / display_step)) acc_total = 0. loss_total = 0. in2 = [words[word_label[i]] for i in range(offset, offset + n_input)] out2 = words[word_label[offset + n_input]] out_pred = words[int(tf.argmax(onehot_pred, 1).eval())] print(&quot;%s - [%s] vs [%s]&quot; % (in2, out2, out_pred)) saver.save(sess, save_dir + &quot;CharRNN.cpkt&quot;, global_step=step) # 中间隔了一个，作为预测 offset += (n_input + 1) step += 1 print(&quot;Finished!&quot;) saver.save(sess, save_dir + &quot;CharRnn.cpkt&quot;, global_step=step) print(&quot;Elapsed time: &quot;, str_time(time.time() - start_time)) # 测试模型 while True: prompt = &quot;请输入%s个字: &quot; % n_input sentence = input(prompt) input_word = sentence.strip() if len(input_word) != n_input: print(&quot;您输入的字符长度为：&quot;, len(input_word), &quot;请输入4个字&quot;) continue try: input_word = char_vector(None, words_num_map, input_word) for i in range(100): keys = np.reshape(np.array(input_word), [-1, n_input, 1]) onehot_pred = sess.run(predicted, feed_dict=&#123;x: keys&#125;) onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval()) sentence = &quot;%s%s&quot; % (sentence, words[onehot_pred_index]) input_word = input_word[1:] input_word.append(onehot_pred_index) print(sentence) except: print(&quot;该字我还没学会&quot;)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[循环神经网络RNN-写诗]]></title>
    <url>%2F2019%2F01%2F13%2F%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN-%E5%86%99%E8%AF%97%2F</url>
    <content type="text"><![CDATA[数据下载下载地址：https://pan.baidu.com/s/19fAqY0_ajkTiKfOBbpY_Sg 训练数据预处理import collectionsimport numpy as npimport tensorflow as tfpoetry_file = &apos;data/poetry.txt&apos;# 数据清洗，生成诗集poetrys = []with open(poetry_file, &quot;r&quot;, encoding=&apos;utf-8&apos;) as f: for line in f: try: line = line.strip(u&apos;\n&apos;) #strip() 方法用于移除字符串头尾指定的字符 title, content = line.strip(u&apos; &apos;).split(u&apos;:&apos;) content = content.replace(u&apos; &apos;, u&apos;&apos;) if u&apos;_&apos; in content or u&apos;(&apos; in content or u&apos;（&apos; in content or u&apos;《&apos; in content or u&apos;[&apos; in content: continue if len(content) &lt; 5 or len(content) &gt; 79: continue content = u&apos;[&apos; + content + u&apos;]&apos; poetrys.append(content) except Exception as e: pass# print(poetrys[0])# 按诗的字数排序poetrys = sorted(poetrys, key=lambda lines: len(lines))print(&apos;唐诗总数: &apos;, len(poetrys))# 统计每个字出现次数all_words = []for poetry in poetrys: all_words += [word for word in poetry]counter = collections.Counter(all_words) #Counter是一个无序的容器类型，以字典的键值对形式存储，其中元素作为key，其计数作为value。count_pairs = sorted(counter.items(), key=lambda x: -x[1])words, _ = zip(*count_pairs)# 取前多少个常用字words = words[:len(words)] + (&apos; &apos;,)# 每个字映射为一个数字IDword_num_map = dict(zip(words, range(len(words))))# 把诗转换为向量形式.trans_to_num = lambda word: word_num_map.get(word, len(words))poetrys_vector = [list(map(trans_to_num, poetry)) for poetry in poetrys]class DataSet(object): def __init__(self, data_size): self._data_size = data_size self._epochs_completed = 0 self._index_in_epoch = 0 self._data_index = np.arange(data_size) def next_batch(self, batch_size): start = self._index_in_epoch if start + batch_size &gt; self._data_size: np.random.shuffle(self._data_index) self._epochs_completed = self._epochs_completed + 1 self._index_in_epoch = batch_size full_batch_features, full_batch_labels = self.data_batch(0, batch_size) return full_batch_features, full_batch_labels else: self._index_in_epoch += batch_size end = self._index_in_epoch full_batch_features, full_batch_labels = self.data_batch(start, end) if self._index_in_epoch == self._data_size: self._index_in_epoch = 0 self._epochs_completed = self._epochs_completed + 1 np.random.shuffle(self._data_index) return full_batch_features, full_batch_labels def data_batch(self, start, end): batches = [] for i in range(start, end): batches.append(poetrys_vector[self._data_index[i]]) length = max(map(len, batches)) xdata = np.full((end - start, length), word_num_map[&apos; &apos;], np.int32) for row in range(end - start): xdata[row, :len(batches[row])] = batches[row] ydata = np.copy(xdata) ydata[:, :-1] = xdata[:, 1:] return xdata, ydata 构建RNN网络计算图# 每次取64首诗进行训练batch_size = 64n_chunk = len(poetrys_vector) // batch_sizeinput_data = tf.placeholder(tf.int32, [batch_size, None])output_targets = tf.placeholder(tf.int32, [batch_size, None])# 定义RNNdef neural_network(model=&apos;lstm&apos;, rnn_size=128, num_layers=2): global cell_fun if model == &apos;rnn&apos;: cell_fun = tf.nn.rnn_cell.BasicRNNCell elif model == &apos;gru&apos;: cell_fun = tf.nn.rnn_cell.GRUCell elif model == &apos;lstm&apos;: cell_fun = tf.nn.rnn_cell.BasicLSTMCell cell = cell_fun(rnn_size, state_is_tuple=True) cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True) initial_state = cell.zero_state(batch_size, tf.float32) with tf.variable_scope(&apos;rnnlm&apos;): softmax_w = tf.get_variable(&quot;softmax_w&quot;, [rnn_size, len(words)]) softmax_b = tf.get_variable(&quot;softmax_b&quot;, [len(words)]) with tf.device(&quot;/cpu:0&quot;): embedding = tf.get_variable(&quot;embedding&quot;, [len(words), rnn_size]) inputs = tf.nn.embedding_lookup(embedding, input_data) outputs, last_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state, scope=&apos;rnnlm&apos;) output = tf.reshape(outputs, [-1, rnn_size]) logits = tf.matmul(output, softmax_w) + softmax_b probs = tf.nn.softmax(logits) return logits, last_state, probs, cell, initial_state 训练模型def load_model(sess, saver, ckpt_path): latest_ckpt = tf.train.latest_checkpoint(ckpt_path) if latest_ckpt: print(&apos;resume from&apos;, latest_ckpt) saver.restore(sess, latest_ckpt) return int(latest_ckpt[latest_ckpt.rindex(&apos;-&apos;) + 1:]) else: print(&apos;building model from Training....&apos;) sess.run(tf.global_variables_initializer()) return -1# 训练def train_neural_network(): logits, last_state, _, _, _ = neural_network() targets = tf.reshape(output_targets, [-1]) loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [targets], [tf.ones_like(targets, dtype=tf.float32)], len(words))##这个函数用于计算所有examples（假设一句话有n个单词，一个单词及单词所对应的label就是一个example,所有example就是一句话中所有单词）的加权交叉熵损失 cost = tf.reduce_mean(loss) tf.summary.scalar(&apos;loss&apos;, tf.reshape(cost, []))##画损失图 learning_rate = tf.Variable(0.0, trainable=False) tvars = tf.trainable_variables()##返回的是需要训练的变量列表 grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 5)##tf.gradients：计算梯度；tf.clip_by_global_norm（t_list 是梯度张量， clip_norm 是截取的比率）让权重的更新限制在一个合适的范围 optimizer = tf.train.AdamOptimizer(learning_rate) train_op = optimizer.apply_gradients(zip(grads, tvars)) Session_config = tf.ConfigProto(allow_soft_placement=True) Session_config.gpu_options.allow_growth = True trainds = DataSet(len(poetrys_vector)) with tf.Session(config=Session_config) as sess: merged = tf.summary.merge_all()##tensorflow的可视化是使用summary和tensorboard合作完成的.###########tf.summary.merge_all: 将之前定义的所有summary op整合到一起 log_writer = tf.summary.FileWriter(&quot;logs&quot;, sess.graph) sess.run(tf.initialize_all_variables()) saver = tf.train.Saver(tf.all_variables()) last_epoch = load_model(sess, saver, &apos;model/&apos;) for epoch in range(last_epoch + 1, 1000): sess.run(tf.assign(learning_rate, 0.002 * (0.97 ** epoch))) #tf.assign(A, new_number): 这个函数的功能主要是把A的值变为new_number all_loss = 0.0 for batche in range(n_chunk): x, y = trainds.next_batch(batch_size) train_loss, _, _, merged_summary = sess.run([cost, last_state, train_op, merged], feed_dict=&#123;input_data: x, output_targets: y&#125;) all_loss = all_loss + train_loss if batche % 50 == 1: log_writer.add_summary(merged_summary, batche) print(&quot;epoch:&#123;&#125; \n&quot;.format(epoch), &quot;batch:&#123;&#125; \n&quot;.format(batche), &quot;Learning_rate:&#123;&#125; \n&quot;.format(0.002 * (0.97 ** epoch)), &quot;train_loss:&#123;&#125; \n&quot;.format(train_loss)) print(epoch, &apos; Loss: &apos;, all_loss * 1.0 / n_chunk) saver.save(sess, &apos;model/poetry.module-%d&apos; % epoch) log_writer.close()train_neural_network() 生成古诗数据预处理import collectionsimport numpy as npimport tensorflow as tfpoetry_file = &apos;data/poetry.txt&apos;# 诗集poetrys = []with open(poetry_file, &quot;r&quot;, encoding=&apos;utf-8&apos;) as f: for line in f: try: line = line.strip(u&apos;\n&apos;) title, content = line.strip(u&apos; &apos;).split(u&apos;:&apos;) content = content.replace(u&apos; &apos;, u&apos;&apos;) if u&apos;_&apos; in content or u&apos;(&apos; in content or u&apos;（&apos; in content or u&apos;《&apos; in content or u&apos;[&apos; in content: continue if len(content) &lt; 5 or len(content) &gt; 79: continue content = u&apos;[&apos; + content + u&apos;]&apos; poetrys.append(content) except Exception as e: pass # 按诗的字数排序poetrys = sorted(poetrys, key=lambda line: len(line))print(&apos;唐诗总数: &apos;, len(poetrys))# 统计每个字出现次数all_words = []for poetry in poetrys: all_words += [word for word in poetry]counter = collections.Counter(all_words)count_pairs = sorted(counter.items(), key=lambda x: -x[1])words, _ = zip(*count_pairs)# 取前多少个常用字words = words[:len(words)] + (&apos; &apos;,)# 每个字映射为一个数字IDword_num_map = dict(zip(words, range(len(words))))# 把诗转换为向量形式to_num = lambda word: word_num_map.get(word, len(words))poetrys_vector = [list(map(to_num, poetry)) for poetry in poetrys]# 每次取64首诗进行训练batch_size = 1n_chunk = len(poetrys_vector) // batch_size# ---------------------------------------RNN--------------------------------------#input_data = tf.placeholder(tf.int32, [batch_size, None])output_targets = tf.placeholder(tf.int32, [batch_size, None])# 定义RNNdef neural_network(model=&apos;lstm&apos;, rnn_size=128, num_layers=2): global cell_fun if model == &apos;rnn&apos;: cell_fun = tf.nn.rnn_cell.BasicRNNCell elif model == &apos;gru&apos;: cell_fun = tf.nn.rnn_cell.GRUCell elif model == &apos;lstm&apos;: cell_fun = tf.nn.rnn_cell.BasicLSTMCell cell = cell_fun(rnn_size, state_is_tuple=True) cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True) initial_state = cell.zero_state(batch_size, tf.float32) with tf.variable_scope(&apos;rnnlm&apos;): softmax_w = tf.get_variable(&quot;softmax_w&quot;, [rnn_size, len(words)]) softmax_b = tf.get_variable(&quot;softmax_b&quot;, [len(words)]) with tf.device(&quot;/cpu:0&quot;): embedding = tf.get_variable(&quot;embedding&quot;, [len(words), rnn_size]) inputs = tf.nn.embedding_lookup(embedding, input_data) outputs, last_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state, scope=&apos;rnnlm&apos;) output = tf.reshape(outputs, [-1, rnn_size]) logits = tf.matmul(output, softmax_w) + softmax_b probs = tf.nn.softmax(logits) return logits, last_state, probs, cell, initial_state 用训练完成的模型生成古诗def gen_head_poetry(heads, type): if type != 5 and type != 7: print(&apos;The second para has to be 5 or 7!&apos;) return def to_word(weights): t = np.cumsum(weights)#a = np.array([[1,2,3], [4,5,6]])### np.cumsum(a)###array([ 1, 3, 6, 10, 15, 21])####array([1，1+2=3，1+2+3=6，1+2+3+4=10，1+2+3+4+5=15，1+2+3+4+5+6=21]） s = np.sum(weights) sample = int(np.searchsorted(t, np.random.rand(1) * s))##np.random.rand(3,2)##括号中为shape##np.searchsorted:寻找某个数应该插在数组的什么位置上，这个数组必须是按序排列的 return words[sample] _, last_state, probs, cell, initial_state = neural_network() Session_config = tf.ConfigProto(allow_soft_placement=True) Session_config.gpu_options.allow_growth = True with tf.Session(config=Session_config) as sess: with tf.device(&apos;/gpu:1&apos;): sess.run(tf.initialize_all_variables()) saver = tf.train.Saver(tf.all_variables()) saver.restore(sess, &apos;model/poetry.module-99&apos;) poem = &apos;&apos; for head in heads: flag = True while flag: state_ = sess.run(cell.zero_state(1, tf.float32)) x = np.array([list(map(word_num_map.get, u&apos;[&apos;))]) [probs_, state_] = sess.run([probs, last_state], feed_dict=&#123;input_data: x, initial_state: state_&#125;) sentence = head x = np.zeros((1, 1)) x[0, 0] = word_num_map[sentence] [probs_, state_] = sess.run([probs, last_state], feed_dict=&#123;input_data: x, initial_state: state_&#125;) word = to_word(probs_) sentence += word while word != u&apos;。&apos;: x = np.zeros((1, 1)) x[0, 0] = word_num_map[word] [probs_, state_] = sess.run([probs, last_state], feed_dict=&#123;input_data: x, initial_state: state_&#125;) word = to_word(probs_) sentence += word if len(sentence) == 2 + 2 * type: sentence += u&apos;\n&apos; poem += sentence flag = False return poemdef gen_poetry(): def to_word(weights): t = np.cumsum(weights) s = np.sum(weights) sample = int(np.searchsorted(t, np.random.rand(1) * s)) return words[sample] _, last_state, probs, cell, initial_state = neural_network() with tf.Session() as sess: sess.run(tf.initialize_all_variables()) saver = tf.train.Saver(tf.all_variables()) saver.restore(sess, &apos;model/poetry.module-99&apos;) state_ = sess.run(cell.zero_state(1, tf.float32)) x = np.array([list(map(word_num_map.get, &apos;[&apos;))]) [probs_, state_] = sess.run([probs, last_state], feed_dict=&#123;input_data: x, initial_state: state_&#125;) word = to_word(probs_) poem = &apos;&apos; while word != &apos;[&apos;: poem += word x = np.zeros((1, 1)) x[0, 0] = word_num_map[word] [probs_, state_] = sess.run([probs, last_state], feed_dict=&#123;input_data: x, initial_state: state_&#125;) word = to_word(probs_) return poem#print(gen_poetry())print(gen_head_poetry(u&apos;言叶之庭&apos;, 5))]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow-基础语法]]></title>
    <url>%2F2019%2F01%2F12%2Ftensorflow-%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[数学公式API：https://github.com/tensorflow/docs/blob/master/site/en/api_guides/python constanta = tf.constant(0, name=&apos;B&apos;)b = tf.constant(1) 常量x = tf.zeros([2, 3], tf.int32)y = tf.zeros_like(x, optimize=True) 变量with tf.variable_scope(&apos;meh&apos;) as scope: a = tf.get_variable(&apos;a&apos;, [10]) b = tf.get_variable(&apos;b&apos;, [100])writer = tf.summary.FileWriter(&apos;./graphs/test&apos;, tf.get_default_graph())writer.close() placeholdder占位符input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)output = tf.multiply(input1, input2)with tf.Session() as sess: print(sess.run([output], feed_dict=&#123;input1:[7.], input2:[2.]&#125;)) 类型转换tf.cast(tf.constant(2.0), tf.int32) 把numpy转换成Tensorimport numpy as npa = np.zeros((3,3))print(a)print(&apos;----------------&apos;)ta = tf.convert_to_tensor(a)with tf.Session() as sess: print(sess.run(ta))]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>语法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow-可视化]]></title>
    <url>%2F2019%2F01%2F12%2Ftensorflow-%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[导入包import numpy as npimport osimport tensorflow as tfimport matplotlib.pyplot as plt 设置生成的图像尺寸和去除警告os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;] = &apos;2&apos;plt.rcParams[&quot;figure.figsize&quot;] = (14, 8) # 生成的图像尺寸 随机生成一个线性的数据n_observations = 100xs = np.linspace(-3, 3, n_observations) #生成-3到3的n为100等差数列ys = 0.8*xs + 0.1 + np.random.uniform(-0.5, 0.5, n_observations)plt.scatter(xs, ys) #画图plt.show() #画图 准备placeholderX = tf.placeholder(tf.float32, name=&apos;X&apos;)Y = tf.placeholder(tf.float32, name=&apos;Y&apos;) 初始化参数/权重W = tf.Variable(tf.random_normal([1]), name=&apos;weight&apos;)tf.summary.histogram(&apos;weight&apos;, W) #画图b = tf.Variable(tf.random_normal([1]), name=&apos;bias&apos;)tf.summary.histogram(&apos;bias&apos;, b)#画图 计算预测结果Y_pred = tf.add(tf.multiply(X, W), b) 计算损失值loss = tf.square(Y - Y_pred, name=&apos;loss&apos;) #tf.square:平方tf.summary.scalar(&apos;loss&apos;, tf.reshape(loss, []))#画图 初始化optimizerlearning_rate = 0.01optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) 指定迭代次数，并在session里执行graphn_samples = xs.shape[0]init = tf.global_variables_initializer()with tf.Session() as sess: # 记得初始化所有变量 sess.run(init) merged = tf.summary.merge_all()#画图 log_writer = tf.summary.FileWriter(&quot;./logs/linear_regression&quot;, sess.graph) # 训练模型 for i in range(50): total_loss = 0 for x, y in zip(xs, ys): # 通过feed_dic把数据灌进去 _, loss_value, merged_summary = sess.run([optimizer, loss, merged], feed_dict=&#123;X: x, Y: y&#125;) total_loss += loss_value if i % 5 == 0: print(&apos;Epoch &#123;0&#125;: &#123;1&#125;&apos;.format(i, total_loss / n_samples)) log_writer.add_summary(merged_summary, i)#画图 # 关闭writer log_writer.close()#画图 # 取出w和b的值 W, b = sess.run([W, b])print(W, b)print(&quot;W:&quot;+str(W[0]))print(&quot;b:&quot;+str(b[0])) 画出线性回归线plt.plot(xs, ys, &apos;bo&apos;, label=&apos;Real data&apos;)plt.plot(xs, xs * W + b, &apos;r&apos;, label=&apos;Predicted data&apos;)plt.legend()plt.show() Tensorboard查看图形数据tensorboard --logdir path/to/logs(你保存文件所在位置) 如：（log_writer = tf.summary.FileWriter(“./logs/linear_regression”, sess.graph)保存的地址）： tensorboard —logdir ./logs/linear_regression 输出：TensorBoard x.x.x at http://(你的用户名):6006 (Press CTRL+C to quit) 然后打开网页：http://localhost:6006]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20单元语法]]></title>
    <url>%2F2019%2F01%2F11%2F20%E5%8D%95%E5%85%83%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[N + なら（ば）＜凸显、条件＞用法一接在体言后面，凸显、强调所指事物，作提示助词，提出主题，并用自信、有把握的语气进行叙述，如果以此为主题的话。接续：N+ ✿汉语：就~方面来说、~的话 1、お金なら心配は要りません。2、花なら桜です。 用法2以假设的形式提出话题，前项为前提，后项为说话人判断、决定或建议；接续： N ・ A ・ V ・ Na 直接裸接 なら ✿汉语：要是~的话~1、私ならそんなことを言いませんよ。2、海が静かならいいですが。3、彼が出席するなら、私は行きません。 ～場合は＜假设＞表示假设的情况。当假设出现了前项情况时，后项一般为针对此情况所采取的方法或对策。接续：N ・ A ・ V ・ Na 连体形+場合は ✿汉语：当~时、在~的情况下 1、王さんの都合が悪い場合は、ほかの日にしましょう。2、電話が通じない場合はどうしたらいいですか。3、雨が降った場合は、運動会を中止します。 Vたらどうですか＜建议＞表示建议或劝诱的惯用表达， ✿汉语：~怎么样、~如何★：礼貌表达方式为：~たらどうですか。 ~たらいかがですか。 ~たらいかがでしょうか1、ABC病院に行ってみたらどうでしょう。(2011年真题)2、朝からずっと勉強していますね。少し休んだらどうですか。3、ネクタイでも買ってあげたらどう？(2010年真题) くらい＜程度＞接在分句后面表示程度。举出具体的事例来说明其程度。也可写做ぐらい。基本可与「ほど」互换。接续：裸接分句后 1、怖くて怖くて、大声で叫びたいくらいだった。(2009年真题)2、涙が出るくらい痛いです。 「ほど」VS「くらい」在表示某种程度时: 如果说话人心目中对其程度没有进行高低取向时，くらい和ほど有时可以互换使用，表示相同的意思。 1、日曜は足が痛くなる**ぐらい**（〇ほど）歩いた。 如果有高低取向，则くらい表示低，而ほど表示高， 1、彼**くらい**（Ｘほど）のレベルでは通訳はできない。2、党の御恩は山**ほど**（Ｘくらい）高く、海**ほど**（Ｘくらい）深い。3、死ぬ**ほど**（×くらい）疲れた。 Vてくださいませんか＜客气的请求＞表示请求别人做某事。比｢Vてくれませんか｣更加委婉、客气，是一种尊他，客气的表达。 ✿ 汉语：能不能请您（为我做)～ １、先生のお写真を見せてくださいませんか。２、もう少し説明してくださいませんか。 VてしまったVてしまった＜感慨＞表示说话人对意外发生的事（无法挽回的事情、消极的结果等）感到很遗憾、后悔的语气。常与副词「もう」搭配１、バスで財布を落としてしまった。２、急いで来たから、財布を忘れてしまった。(2005年真题) Vてしまった＜完了＞表示动作过程的完了。用于表示持续动作的动词时，与｢V第一连用－おわる｣意思相近。１、この宿題をしてしまったら、遊びに行ける。２、この本はもう読んでしまったから、図書館に返します。 N+の＋うち＜范围＞表示限定范围。 ✿ 汉语：~当中、~之中★在表示从某范围中挑选某事物时，可与｢Ｎのなか｣替换。１、三人のうち、林さんが一番若いです。２、クラスメートのうち、6人が男性です。３、相撲とサッカーと野球のうちで、一番人気があるのはやはり野球だそうだ。 N1 または N2＜选择＞表示两者择一，多用于书面语，表示要求、指示等场合。 ✿ 汉语：~或是~1、3番の部屋、または4番の部屋に行ってください。(2008年真题)2、漢字または仮名で書いてください。]]></content>
      <categories>
        <category>日语</category>
      </categories>
      <tags>
        <tag>语法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[19单元语法]]></title>
    <url>%2F2019%2F01%2F11%2F19%E5%8D%95%E5%85%83%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Nとは～という意味だ汉语：~是~的意思★口语：Ｎというのは、～✿帰省とは故郷に帰るという意味です。✿下水とは台所などで使った汚れた水のことである。✿進入禁止とは入ってはいけないという意味です。 ~うちに＜时段＞前接表示状态的词，表示在该 状态持续期间 内，发生了某件事或做某件事（有尽快进行该动作的语感）。接续： ✿ N + の + うちに ✿ Na + な + うちに ✿ A - い + うちに ✿ V - る / V-ている V-ない + うちに 汉语 ：趁着~、~时候、在~之内✿どうぞ、温かいうちに食べてください。（2008年真题）✿父が元気なうちに、一度一緒に温泉に行きたいと思います。 V-（よ）うか＜犹豫＞用于简体的会话。 自言自语 或是 与对方商量 的语气。表示说话人对是否要做某动作而 犹豫不决、踌躇不定 的心情。接续：动词意志形+か✿もう時間だから、行こうか。✿結果はどうなるかわからないけど、やってみようか。✿いくら考えてもわからないから、しばらく休んで、後にしようか。 ても・でも＜让步＞表示让步的条件。就算前项从句成立，后项主句的结果也不会改变。（同19课2单元）接续： ✿N・Na + でも ✿A-く + ても ✿V-て + ても汉语：即使~也~、就算~都~ ✿あの美術館はいつ行っても人がたくさんいる。(2007年真题)✿学校を卒業しても、日本語の勉強を続けていくつもりだ。 (2005年真题)✿今必要だから、高くても買う。✿先生でもわからないかもしれません V-ると～た＜契机＞表示说话人在 前面的事情成立 的情况下，重新认识后项事物，是一些 新的发现、认识 等，具有意外性。或以此为契机 发生了后项的事物 。 ✿五月に入ると、急に暑くなった。✿外に出ると、雨が降っていた。✿友達が怪我で入院したと聞き、慌てて病院に行ってみると思っていたより元気で安心した。 「～たら～た」 VS 「～と～た」相同点：表示“以~为契机发现了~”这一用法时，两者一般可以替换使用。 不同点：1、“と”常用于小说或故事等，而“たら”则多用于说话人表述自己直接的经历。 2、当前后两个句子表示为 同一人物的意志可控制 的连续动词时，只可以用“と”。✿男は部屋に入ると、友達に電話した。 3、当表示 说话人身体的感觉 时，只可以用“たら”，不能用“と”。✿昨夜、この薬を飲んだら、よく効いた。 でも＜极端的情况＞助词でも除了表示“示例”以外，更多的是接在名词（或者部分副词、助词）后，用于举出极端的事例。 中文：“就连~都~”“即使~也~”“尽管~也~”✿この店は日本料理が本格的ですが。日本人でもこの味に満足している。 ✿先生でもわからないかもしれない。✿この仕事は病気でも休めません✿今度の日曜日、雨でもサッカーの試合を行います。 ～し～(し)＜并列＞连接两个或两个以上的分句，列举。多用罗列于原因理由接续：分句+し翻译：“既~又~”“又~又~”✿お金もないし、時間もないから、遊びに行けない。(2008年真题)✿アパートは綺麗だし、広いし、駅からも近い。(2005年真题) ～Ｖばいい・よい&lt;建议＞常用在表示提议时。汉语：只要~就可、~就好 ✿ A:どうすればいいですか。 B:ちゃんと謝ればいいですよ。✿ お金がなければ、お父さんに借りればいいでしょう。 のに＜转折＞ ✿ V-る・V-た +のに ✿ A-い・A-かった +のに ✿ N・Na な+のに 位于句中起逆接作用，是接续助词。连接起来的句子往往都有意外、不满、埋怨等语感汉语：可是~、却~✿雨が降っているのに、傘を持たないで出かけた。✿知っているのに知らないと言った。 置于句末是终助词，表示事与愿违时的遗憾、惋惜、后悔等心情，一般多用口语。可以跟在｢ばいい｣后面。✿この部屋がもう少し広げればいいのに。✿注意していたのに。 たら＜条件＞表示 假设，属于动词的另一种条件形。接续上和动词的过去式｢た｣是一样的。表 一次性的，特定 的依存关系。表示主句的实现，建立在从句动作或变化完成的基础上。汉语： ~之后就~ ~以后~✿仕事が終わったら、お茶でも飲みにいきましょう。✿そんなにたくさん食べたら、おなかを壊しますよ。✿大学を卒業したらどんな仕事をしますか。 V-て/V-ないで&lt;伴随状态&gt;表示（没有）在前项的伴随状态下进行后项主体动作。 ✿マスクをして出かけました。✿ネクタイを締めないで会社に行きます。 V-て（は）いられない&lt;状态难以持续&gt;表示因为在紧迫的情况下，不能继续那种状态而要急于想付诸另一种行动之意。汉语：不能~，哪能~ ✿もう時間がないから、遅れてくる人を待っていられない。すぐ始めよう。✿こんなに忙しいときに、寝ていられないよ。 Nによって&lt;原因&gt;表示“那就是原因”之意，后续表示结果的词句。讲述已经发生的事情，谓语一般为过去式，多用于书面语。 ✿私の不注意な発言によって、彼を傷つけた。✿交通事故によって、電車は三時間も遅れた。 N として（は・も・の）&lt;资格性质&gt;表示动作主体进行某动作时的身份、资格、立场、性质等。汉语：作为~、以~身份、以~立场、以~资格 ✿通訳として、一緒に行く。(2011年真题)✿私としては賛成ですが、ほかの人の意見も聞いてみないと決められない。✿彼女は母としても妻としても完璧な素晴らしい女性です。✿私には私としての考えがあります。 と总结 必然结果，自然现象 ✿春になると、花が咲く✿雨だと明日の試合は中止になります。✿右に曲がると、大きな建物が見える。 契机，发现（ｖたら～た也有该用法） ✿デパートに行くと、休みだった。✿うちへ帰ると、友達が待っていた。 习惯动作 ✿起きると、すぐ顔を洗う✿彼は家に帰ると、パソコンに向かっています。 ば总结 必然结果，自然现象（と～也有该用法） ✿春になれば、花が咲く。 假定条件 ✿このごろ日本へ行けば、桜が見える。 ★注意：1、当假定式为动作或者变化时，后项不能使用ください、たい、ましょう雨が降れば、窓を閉めてください。× 2、当假定式为状态或存在时，后项可以使用ください、たい暑ければ、エアコンをつけてください。〇 3、主句一般不能使用过去式窓を開けば、富士山が見えた。× たら总结 假定（ば也有该用法） ✿安かったら、買う✿困ったら、電話してね。 契机，发现（と也有该用法） ✿窓を開けたら、海が見えた。 在～之后 ✿本を読んだら貸してください。✿大阪に着いたら電話してください。 ★注意：1、たら含有明显的完成之意，特别是前后都是动词时，一定是前项先发生，后项再发生。2、たら后项可使用命令、劝诱、依赖等表达。]]></content>
      <categories>
        <category>日语</category>
      </categories>
      <tags>
        <tag>语法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wine安装qq]]></title>
    <url>%2F2019%2F01%2F11%2Fwine%E5%AE%89%E8%A3%85qq%2F</url>
    <content type="text"><![CDATA[安装wine下载地址： https://github.com/wszqkzqk/deepin-wine-ubuntu解压后安装：sudo sh ./install.sh 安装QQ、微信wine应用下载地址： http://mirrors.aliyun.com/deepin/pool/non-free/d/常用下载应用：QQ： http://mirrors.aliyun.com/deepin/pool/non-free/d/deepin.com.qq.im/微信： http://mirrors.aliyun.com/deepin/pool/non-free/d/deepin.com.wechat/Foxmail: http://mirrors.aliyun.com/deepin/pool/non-free/d/deepin.com.foxmail/ 异常说明微信无法发送图片：sudo apt install libjpeg62:i386 卸载：sudo apt remove 软件包名 比如deepin.com.qq.office_2.0.0deepin4_i386.deb的卸载命令：sudo apt remove deepin.com.qq.office 托盘图标安装icons-plus扩展sudo apt-get install gnome-shell-extension-top-icons-plus gnome-tweaks 然后在gnome-tweaks里设置]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>qq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-图片处理及绘图]]></title>
    <url>%2F2019%2F01%2F11%2Fopencv-%E5%9B%BE%E7%89%87%E5%A4%84%E7%90%86%E5%8F%8A%E7%BB%98%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[原始图片： 彩色图片灰度化方式1：import cv2 # 导入cv库img = cv2.imread(&apos;image2.jpg&apos;,0)cv2.imwrite(&apos;gray_image.jpg&apos;,img) 方式2：import cv2img = cv2.imread(&apos;image2.jpg&apos;,1)dst = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)# 颜色空间转换 1 data 2 BGR graycv2.imshow(&apos;dst&apos;,dst) 方式3#方法4 gray = r*0.299+g*0.587+b*0.114import cv2import numpy as npimg = cv2.imread(&apos;image0.jpg&apos;,1)imgInfo = img.shapeheight = imgInfo[0]width = imgInfo[1]dst = np.zeros((height,width,3),np.uint8)for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] b = int(b) g = int(g) r = int(r) gray = r*0.299+g*0.587+b*0.114 dst[i,j] = np.uint8(gray)cv2.imshow(&apos;dst&apos;,dst) 马赛克import cv2 # 导入cv库import numpy as npimg = cv2.imread(&apos;image2.jpg&apos;,1) # 读取图片文件， 1：彩色， 0：灰色#cv2.imshow(&apos;src&apos;,img)imgInfo = img.shape # 获取图片的维度height = imgInfo[0]width = imgInfo[1]for m in range(200,400): for n in range(400,500): # pixel -&gt;10*10 if m%10 == 0 and n%10==0: for i in range(0,10): for j in range(0,10): (b,g,r) = img[m,n] img[i+m,j+n] = (b,g,r)cv2.imwrite(&apos;msk.jpg&apos;,img) 边缘检测方式1：import cv2import numpy as npimport randomimg = cv2.imread(&apos;image2.jpg&apos;,1)imgInfo = img.shapeheight = imgInfo[0]width = imgInfo[1]#cv2.imshow(&apos;src&apos;,img)#canny 1 gray 2 高斯 3 cannygray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)imgG = cv2.GaussianBlur(gray,(3,3),0)dst = cv2.Canny(img,50,50) #图片卷积——》th#cv2.imshow(&apos;dst&apos;,dst)cv2.imwrite(&apos;canny.jpg&apos;,dst) 方式2：import cv2import numpy as npimport randomimport mathimg = cv2.imread(&apos;image2.jpg&apos;, 1)imgInfo = img.shapeheight = imgInfo[0]width = imgInfo[1]#cv2.imshow(&apos;src&apos;, img)# sobel 1 算子模版 2 图片卷积 3 阈值判决# [1 2 1 [ 1 0 -1# 0 0 0 2 0 -2# -1 -2 -1 ] 1 0 -1 ]# [1 2 3 4] [a b c d] a*1+b*2+c*3+d*4 = dst# sqrt(a*a+b*b) = f&gt;thgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)dst = np.zeros((height, width, 1), np.uint8)for i in range(0, height - 2): for j in range(0, width - 2): gy = gray[i, j] * 1 + gray[i, j + 1] * 2 + gray[i, j + 2] * 1 - gray[i + 2, j] * 1 - gray[i + 2, j + 1] * 2 - \ gray[i + 2, j + 2] * 1 gx = gray[i, j] + gray[i + 1, j] * 2 + gray[i + 2, j] - gray[i, j + 2] - gray[i + 1, j + 2] * 2 - gray[ i + 2, j + 2] grad = math.sqrt(gx * gx + gy * gy) if grad &gt; 50: dst[i, j] = 255 else: dst[i, j] = 0cv2.imwrite(&apos;sobel.jpg&apos;,dst) 颜色风格变化import cv2import numpy as npimg = cv2.imread(&apos;image2.jpg&apos;,1)#cv2.imshow(&apos;src&apos;,img)imgInfo = img.shapeheight = imgInfo[0]width = imgInfo[1]#rgb -》RGB new “蓝色”# b=b*1.5# g = g*1.3dst = np.zeros((height,width,3),np.uint8)for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] b = b*1.5 g = g*1.3 if b&gt;255: b = 255 if g&gt;255: g = 255 dst[i,j]=(b,g,r)#cv2.imshow(&apos;dst&apos;,dst)cv2.imwrite(&apos;dst2.jpg&apos;,dst) 油画特效import cv2import numpy as npimg = cv2.imread(&apos;image2.jpg&apos;,1)#cv2.imshow(&apos;src&apos;,img)imgInfo = img.shapeheight = imgInfo[0]width = imgInfo[1]gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)dst = np.zeros((height,width,3),np.uint8)for i in range(4,height-4): for j in range(4,width-4): array1 = np.zeros(8,np.uint8) for m in range(-4,4): for n in range(-4,4): p1 = int(gray[i+m,j+n]/32) array1[p1] = array1[p1]+1 currentMax = array1[0] l = 0 for k in range(0,8): if currentMax&lt;array1[k]: currentMax = array1[k] l = k # 简化 均值 for m in range(-4,4): for n in range(-4,4): if gray[i+m,j+n]&gt;=(l*32) and gray[i+m,j+n]&lt;=((l+1)*32): (b,g,r) = img[i+m,j+n] dst[i,j] = (b,g,r)cv2.imwrite(&apos;dst3.jpg&apos;,dst) 线段绘制import cv2import numpy as npnewImageInfo = (500,500,3)dst = np.zeros(newImageInfo,np.uint8)# line# 绘制线段 1 dst 2 begin 3 end 4 colorcv2.line(dst,(100,100),(400,400),(0,0,255))# 5 line wcv2.line(dst,(100,200),(400,200),(0,255,255),20)# 6 line typecv2.line(dst,(100,300),(400,300),(0,255,0),20,cv2.LINE_AA)cv2.imwrite(&apos;line.jpg&apos;,dst) 绘制矩形、圆形import cv2import numpy as npnewImageInfo = (500,500,3)dst = np.zeros(newImageInfo,np.uint8)# 1 2 左上角 3 右下角 4 5 fill -1 &gt;0 line wcv2.rectangle(dst,(50,100),(200,300),(255,0,0),5)# 2 center 3 rcv2.circle(dst,(300,100),(50),(0,255,0),2)# 2 center 3 轴(a,b) 4 angle 5 begin 6 end 7cv2.ellipse(dst,(256,350),(150,100),30,0,360,(255,255,0),-1)points = np.array([[350,50],[140,140],[200,170],[250,250],[350,50]],np.int32)print(points.shape)points = points.reshape((-1,1,2))print(points.shape)cv2.polylines(dst,[points],True,(0,0,255))cv2.imwrite(&apos;dst4.jpg&apos;,dst) 添加文字import cv2import numpy as npimg = cv2.imread(&apos;image2.jpg&apos;,1)font = cv2.FONT_HERSHEY_SIMPLEXcv2.rectangle(img,(400,300),(950,900),(0,255,0),3)# 1 dst 2 文字内容 3 坐标 4 5 字体大小 6 color 7 粗细 8 line typecv2.putText(img,&apos;this is flower&apos;,(500,500),font,2,(200,100,255),3,cv2.LINE_AA)cv2.imwrite(&apos;word.jpg&apos;,img)]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>图像美化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-图片几何变换]]></title>
    <url>%2F2019%2F01%2F11%2Fopencv-%E5%9B%BE%E7%89%87%E5%87%A0%E4%BD%95%E5%8F%98%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[原始图片： 图片缩放一import cv2 # 导入cv库img = cv2.imread(&apos;image1.jpg&apos;,1) # 读取图片文件， 1：彩色， 0：灰色imgInfo = img.shape # 获取图片的维度print(imgInfo)height = imgInfo[0] width = imgInfo[1]mode = imgInfo[2]# 1 放大 缩小 2 等比例 非 2:3 dstHeight = int(height*0.5)dstWidth = int(width*0.5)#最近临域插值 双线性插值 像素关系重采样 立方插值dst = cv2.resize(img,(dstWidth,dstHeight))#cv2.imshow(&apos;image&apos;,dst)cv2.imwrite(&apos;resize_image.jpg&apos;,dst) 图片缩放二import cv2 # 导入cv库import numpy as npimg = cv2.imread(&apos;image1.jpg&apos;,1) # 读取图片文件， 1：彩色， 0：灰色#cv2.imshow(&apos;src&apos;,img)imgInfo = img.shape # 获取图片的维度height = imgInfo[0]width = imgInfo[1]matScale = np.float32([[0.5,0,0],[0,0.5,0]]) # 定义缩放矩阵dst = cv2.warpAffine(img,matScale,(int(width/2),int(height/2))) # 原始数据，缩放矩阵，目标的宽高信息cv2.imwrite(&apos;warp_image.jpg&apos;,dst) 图片剪切import cv2 # 导入cv库img = cv2.imread(&apos;image1.jpg&apos;,1) # 读取图片文件， 1：彩色， 0：灰色#imgInfo = img.shapeprint(img.shape)dst = img[100:600,250:800] # 获取宽度100-600， 高度250-800的图像cv2.imwrite(&apos;cut_image.jpg&apos;,dst) 图片镜像import cv2 # 导入cv库import numpy as npimg = cv2.imread(&apos;image1.jpg&apos;,1) # 读取图片文件， 1：彩色， 0：灰色cv2.imshow(&apos;src&apos;,img)imgInfo = img.shape # 获取图片的维度height = imgInfo[0]width = imgInfo[1]deep = imgInfo[2]newImgInfo = (height*2,width,deep) # 新图片的维度dst = np.zeros(newImgInfo,np.uint8)#uint8 # 目标图片的数据维度# 刷新图片的数据for i in range(0,height): for j in range(0,width): dst[i,j] = img[i,j] #x y = 2*h - y -1 dst[height*2-i-1,j] = img[i,j]for i in range(0,width): # 添加分割线 dst[height,i] = (0,0,255)#BGRcv2.imshow(&apos;dst&apos;,dst) 图片旋转import cv2 # 导入cv库import numpy as npimg = cv2.imread(&apos;image1.jpg&apos;,1) # 读取图片文件， 1：彩色， 0：灰色cv2.imshow(&apos;src&apos;,img)imgInfo = img.shape # 获取图片的维度height = imgInfo[0]width = imgInfo[1]# 2*3 定义旋转矩阵--旋转的中心点，旋转的角度， 缩放系数matRotate = cv2.getRotationMatrix2D((height*0.5,width*0.5),45,1)# mat rotate 1 center 2 angle 3 scale#100*100 25dst = cv2.warpAffine(img,matRotate,(height,width)) # 仿射方法#cv2.imshow(&apos;dst&apos;,dst)cv2.imwrite(&apos;rotate_image.jpg&apos;,dst) 图片仿射变换import cv2 # 导入cv库import numpy as npimg = cv2.imread(&apos;image1.jpg&apos;,1) # 读取图片文件， 1：彩色， 0：灰色cv2.imshow(&apos;src&apos;,img)imgInfo = img.shape # 获取图片的维度height = imgInfo[0]width = imgInfo[1]#src 3-&gt;dst 3 (左上角 左下角 右上角)matSrc = np.float32([[0,0],[0,height-1],[width-1,0]]) # 获取原图片三个点坐标matDst = np.float32([[50,50],[300,height-200],[width-300,100]]) # 三个点的新坐标#把两个矩阵组合matAffine = cv2.getAffineTransform(matSrc,matDst) # 获取矩阵的组合，dst = cv2.warpAffine(img,matAffine,(width,height)) # 仿射变换方法#cv2.imshow(&apos;dst&apos;,dst)cv2.imwrite(&apos;aft_image.jpg&apos;,dst)]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>图像美化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown]]></title>
    <url>%2F2019%2F01%2F09%2Fmarkdown%2F</url>
    <content type="text"><![CDATA[粗体、斜体*这是斜体***这是粗体*****这是粗体+斜体*** 删除线~~就像这样~~ 引用通过在行首加上大于号&gt;来添加引用格式。&gt; This is the first level of quoting.&gt;&gt; &gt; This is nested blockquote.&gt;&gt; Back to the first level. 列表无序列表使用星号、加号或是减号作为列表标记：* Red+ Green- Blue 分隔线* * *********- - ---------------------------------------- 链接[an example](http://example.com/)[an example](http://example.com/ &quot;Optional Title&quot;) 图像普通方式![Alt text](/path/to/img.jpg)![Alt text](/path/to/img.jpg &quot;Optional Title&quot;) 通过管理文件夹&#123;% asset_path slug %&#125;&#123;% asset_img slug [title] %&#125;&#123;% asset_link slug [title] %&#125; 通过图床引用&lt;figure class=&quot;half&quot;&gt; &lt;img src=&quot;http://address.com/images/image.png&quot; title=&quot;title1&quot;/&gt; &lt;img src=&quot;http://path/image.png&quot; title=&quot;title2&quot;/&gt;&lt;/figure&gt; 表格| Item | Value | Qty || :------- | ----: | :---: || Computer | $1600 | 5 || Phone | $12 | 12 || Pipe | $1 | 234 | TeX公式更换渲染器： $ npm uninstall hexo-renderer-marked --save$ npm install hexo-renderer-kramed --save 插入公式形式： $$\Gamma(z) = \int_0^\infty t^&#123;z-1&#125;e^&#123;-t&#125;dt\,.$$ 公式说明文档： https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference typora编辑器#for Linux# or run:# sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys BA300B7755AFCFAEwget -qO - https://typora.io/linux/public-key.asc | sudo apt-key add -# add Typora&apos;s repositorysudo add-apt-repository &apos;deb https://typora.io/linux ./&apos;sudo apt-get update# install typorasudo apt-get install typora]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv图像美化]]></title>
    <url>%2F2019%2F01%2F09%2Fopencv%E5%9B%BE%E5%83%8F%E7%BE%8E%E5%8C%96%2F</url>
    <content type="text"><![CDATA[直方图均衡化-灰度import cv2import numpy as npimg = cv2.imread(&apos;image2.jpg&apos;,1)gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # 图片灰度化cv2.imshow(&apos;gray&apos;,gray)cv2.imwrite(&apos;gray2.jpg&apos;,gray)hist = cv2.equalizeHist(gray) # api 完成直方图均衡化cv2.imshow(&apos;hist&apos;,hist)cv2.imwrite(&apos;hist2.jpg&apos;,hist)cv2.waitKey(0) 直方图均衡化-彩色import cv2import numpy as npimg = cv2.imread(&apos;image2.jpg&apos;,1)cv2.imshow(&apos;src&apos;,img)(b,g,r) = cv2.split(img) # 通道分解# 图片单通道处理bH = cv2.equalizeHist(b)gH = cv2.equalizeHist(g)rH = cv2.equalizeHist(r)result = cv2.merge((bH,gH,rH))# 通道合成cv2.imshow(&apos;dst&apos;,result)cv2.imwrite(&apos;dst2.jpg&apos;,result)cv2.waitKey(0) 直方图均衡化-YUVimport cv2import numpy as npimg = cv2.imread(&apos;image2.jpg&apos;,1)imgYUV = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb) #cv2.imshow(&apos;src&apos;,img)channelYUV = cv2.split(imgYUV) # 图片分解channelYUV[0] = cv2.equalizeHist(channelYUV[0]) # 直方图均衡化channels = cv2.merge(channelYUV) # 合成result = cv2.cvtColor(channels,cv2.COLOR_YCrCb2BGR)cv2.imshow(&apos;result2&apos;,result)cv2.imwrite(&apos;result2.jpg&apos;,result)cv2.waitKey(0) 图片修补import cv2import numpy as np#制作一张损坏的图片img = cv2.imread(&apos;image2.jpg&apos;,1) # 读取图片文件， 1：彩色， 0：灰色for i in range(1,100): # 总共一百个像素点 img[500+i,500] = (255,255,255) # 写入标准的白色cv2.imwrite(&apos;damaged.jpg&apos;,img)#修补损坏的图片img = cv2.imread(&apos;damaged.jpg&apos;,1)cv2.imshow(&apos;src&apos;,img)imgInfo = img.shapeheight = imgInfo[0]width = imgInfo[1]paint = np.zeros((height,width,1),np.uint8)# 描绘图片坏的数组；进行修补for i in range(500,600): paint[i,500] = 255cv2.imshow(&apos;paint&apos;,paint)#1 src 2 maskimgDst = cv2.inpaint(img,paint,3,cv2.INPAINT_TELEA)cv2.imshow(&apos;image&apos;,imgDst)cv2.imwrite(&apos;imgDst.jpg&apos;,imgDst)cv2.waitKey(0) 亮度增强import cv2import numpy as npimg = cv2.imread(&apos;image2.jpg&apos;,1)imgInfo = img.shapeheight = imgInfo[0]width = imgInfo[1]cv2.imshow(&apos;src&apos;,img)dst = np.zeros((height,width,3),np.uint8)for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] bb = int(b)+40 gg = int(g)+40 rr = int(r)+40 if bb&gt;255: bb = 255 if gg&gt;255: gg = 255 if rr&gt;255: rr = 255 dst[i,j] = (bb,gg,rr)cv2.imshow(&apos;dst3&apos;,dst)cv2.imwrite(&apos;dst3.jpg&apos;,dst) 高斯滤波import cv2import numpy as npimg = cv2.imread(&apos;image1.jpg&apos;,1)cv2.imshow(&apos;src&apos;,img)dst = cv2.GaussianBlur(img,(5,5),1.5)cv2.imwrite(&apos;dst4.jpg&apos;,dst)cv2.imshow(&apos;dst4&apos;,dst)cv2.waitKey(0) 均值滤波# 均值 6*6 1 。 * 【6*6】/36 = mean -》Pimport cv2import numpy as npimg = cv2.imread(&apos;image1.jpg&apos;, 1)#cv2.imshow(&apos;src&apos;, img)imgInfo = img.shapeheight = imgInfo[0]width = imgInfo[1]dst = np.zeros((height, width, 3), np.uint8)for i in range(3, height - 3): for j in range(3, width - 3): sum_b = int(0) sum_g = int(0) sum_r = int(0) for m in range(-3, 3): # -3 -2 -1 0 1 2 for n in range(-3, 3): (b, g, r) = img[i + m, j + n] sum_b = sum_b + int(b) sum_g = sum_g + int(g) sum_r = sum_r + int(r) b = np.uint8(sum_b / 36) g = np.uint8(sum_g / 36) r = np.uint8(sum_r / 36) dst[i, j] = (b, g, r)cv2.imshow(&apos;dst5&apos;, dst)cv2.imwrite(&apos;dst5.jpg&apos;, dst) 中值滤波import cv2import numpy as npimg = cv2.imread(&apos;image1.jpg&apos;,1)imgInfo = img.shapeheight = imgInfo[0]width = imgInfo[1]img = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)#cv2.imshow(&apos;src&apos;,img)cv2.imwrite(&apos;src6.jpg&apos;,img)dst = np.zeros((height,width,3),np.uint8)collect = np.zeros(9,np.uint8)for i in range(1,height-1): for j in range(1,width-1): k = 0 for m in range(-1,2): for n in range(-1,2): gray = img[i+m,j+n] collect[k] = gray k = k+1 # 0 1 2 3 4 5 6 7 8 # 1 for k in range(0,9): p1 = collect[k] for t in range(k+1,9): if p1&lt;collect[t]: mid = collect[t] collect[t] = p1 p1 = mid dst[i,j] = collect[4]#cv2.imshow(&apos;dst&apos;,dst)cv2.imwrite(&apos;dst6.jpg&apos;,dst) 皮肤磨皮美白-双边滤波import cv2img = cv2.imread(&apos;image3.png&apos;,1)#cv2.imshow(&apos;src&apos;,img)dst = cv2.bilateralFilter(img,15,35,35) # 滤波函数#cv2.imshow(&apos;dst&apos;,dst)cv2.imwrite(&apos;dst7.png&apos;,dst)]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>图像美化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用工具]]></title>
    <url>%2F2019%2F01%2F07%2Flinux%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[换源ubantu换源阿里源sudo python3 -c &quot;d=&apos;mirrors.aliyun.com&apos;;import re;from pathlib import Path;p=Path(&apos;/etc/apt/sources.list&apos;);s=p.read_text();bak=p.with_name(p.name+&apos;.bak&apos;);bak.exists() or bak.write_text(s);p.write_text(re.sub(r&apos;(cn.archive|security|archive)\.ubuntu\.com&apos;, d, s))&quot; pip换源修改或创建 ~/.pip/pip.conf ：[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple 解压.tar格式# 打包 tar -cvf 文件名.tar # 要打包的文件# 解包 tar -xvf 文件名.tar#查看包里的内容tar -tvf 包的文件名.tar .gz格式tar -zcvf xxx.tar.gz 文件 # 压缩tar -zxvf xxx.tar.gz 文件 # 解压# 解压到指定目录 tar -zxvf xxx.tar.gz -C dirname .bz2格式tar -jcvf xxx.tar.bz2 文件 # 压缩tar -jxvf xxx.tar.bz2 # 解压 .zip格式安装sudo apt-get install zip 使用压缩文件 zip 压缩文件 源文件解压 unzip 压缩文件-d 解压到指定目录 如果目录不存在 会自动创建新目录 并压缩进去unzip test.zip -d filename vim编辑器工作模式：命令模式、输入模式、末行模式 模式切换当打开一个文件时处于命令模式在命令模式下，按 i 进入输入模式在输入模式，按ESC回到命令模式。在命令模式下，按shift+; ，末行出现:冒号，则进入末行模式 进入与退出进入 vim filename退出 :wq 末行模式，wq 保存退出 :q 末行模式，q 直接退出 :q! 末行模式，q! 强制退出，不保存 复制与粘贴复制和粘贴 yy 复制整行内容 3yy 复制3行内容 yw 复制当前光标到单词尾内容 p 粘贴 删除删除 dd 删除光标所在行 dw 删除一个单词 x 删除光标所在字符 u 撤销上一次操作 s 替换 ctrl + r 撤销 查找查找 / 命令模式下输入：/ 向前搜索 不能空格 ? 命令模式下输入：? 向后搜索# / 方式 n 向下查找 N 向上查找# ? 方式 n 向上查找 N 向下查找]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv基础]]></title>
    <url>%2F2019%2F01%2F07%2Fopencv%2F</url>
    <content type="text"><![CDATA[安装# pippip3 install opencv-python# condaconda install --channel https://conda.anaconda.org/menpo opencv3# conda虚拟环境source activate 环境名pip install opencv-python 读取与展示import cv2 # 导入cv库img = cv2.imread(&apos;image0.jpg&apos;,1) # 读取图片文件， 1：彩色， 0：灰色cv2.imshow(&apos;image&apos;,img) # 显示图片cv2.waitKey(0) # 没有会一闪而过 写入import cv2 # 导入cv库img = cv2.imread(&apos;image0.jpg&apos;,1) # 读取图片文件， 1：彩色， 0：灰色cv2.imwrite(&apos;image1.jpg&apos;,img) # 写入文件名字 ， 图片数据 有损压缩import cv2 # 导入cv库img = cv2.imread(&apos;image0.jpg&apos;,1) # 读取图片文件， 1：彩色， 0：灰色cv2.imwrite(&apos;imageTest.jpg&apos;,img,[cv2.IMWRITE_JPEG_QUALITY,50]) # 写入文件名字 ， 图片数据 ， 当前jpg图片保存的质量（范围0-100）#1M 100k 10k 0-100 有损压缩 无损压缩# 1 无损 2 透明度属性import cv2 # 导入cv库img = cv2.imread(&apos;image0.jpg&apos;,1) # 读取图片文件， 1：彩色， 0：灰色cv2.imwrite(&apos;imageTest.png&apos;,img,[cv2.IMWRITE_PNG_COMPRESSION,0]) # 写入文件名字 ， 图片数据 ， 当前jpg图片保存的质量（范围0-100）# jpg 0 压缩比高0-100 png 0 压缩比低0-9 像素操作import cv2 # 导入cv库 img = cv2.imread(&apos;image0.jpg&apos;,1) # 读取图片文件， 1：彩色， 0：灰色(b,g,r) = img[100,100] # 获取图片的（100,100）坐标的像素值，按照bgr的形式读取print(b,g,r)# bgr#10 100 --- 110 100for i in range(1,100): # 总共一百个像素点 img[10+i,100] = (255,0,0) # 写入标准的蓝色cv2.imshow(&apos;image&apos;,img)]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[敬语]]></title>
    <url>%2F2019%2F01%2F06%2F%E6%95%AC%E8%AF%AD%2F</url>
    <content type="text"><![CDATA[敬语用于对会话中涉及的人物或者听话人表示敬意。现在日语的敬语大致分为四类： ①尊他语（尊敬語 そんけいご）：对他人的行为、状态及有关事物等表示敬意的语言。②自谦语（謙譲語 けんじょうご）：以谦逊的态度叙述自己或自己一方的行为、状态及有关事物的语言。③郑重语（丁寧語 ていねいご）：表示客气、有礼貌、文雅、郑重的态度的语言。如：です、ます、ましょう等。④美化语（美化語 びかご）：名词的接头接尾词，指令人听上去很优美很文雅的一些表达方式，给人优雅而又有教养的印象。 尊他语名词的尊他语①加前缀或后缀：前缀：お：お手紙、お話、お宅、お電話 （和语词，日常常用词）ご：ご案内、ご希望、ご協力 （汉语词）后缀：～さん ： 山田さん、学生さん～様（さま）： 田中さま、二人さま～殿（どの）： 吉田殿、会長殿前后缀：お父さん（さま）：お母さん（さま）お嬢さん（さま）：お客さん（さま） ②Ｎ本身变化会社：貴社 宅：お住まい 形容词的尊他语お＋形容詞✿ 歩くのがお速いですね。✿ 最近お忙しいですか。✿ お元気ですね。✿ お上手ですよ。 动词的尊他语一般动词お＋マス形＋になるご＋サ変动词词干＋になる✿ 何時ごろお帰りになりますか。✿ 先生がご案内になってくださったのです。 特殊动词 请求的尊他表示用于请求对方做某事。汉语：请您~接续：お＋マス形＋ください ご＋サ変动词词干＋ください✿ あしたの会議、ぜひご参加ください。✿ もう大丈夫ですので、どうぞご安心ください。✿ どうぞ、おかけください。 ★ 特殊词的接续：特殊词て型＋ください 其他尊他语形式１、お（ご） + ＶＲ（サ変語幹）+ なさるお帰りなさるご心配なさるあなたが行けば、おばあさんはきっとお喜びなさるでしょう。 ２、お（ご） + ＶＲ（サ変語幹）+ ですお客さんがこちらでお待ちです。 客人们请在这里等候。お父さんはご在宅ですか。 您父亲在家吗。 谦让语名词的自谦Ｎ本身变化わたくし茶：粗茶（そちゃ①０）贈り物：つまらない物当社：弊社（へいしゃ①）妻：愚妻（ぐさい０） 动词的自谦一般动词接续：お＋マス形＋ する/いたすご＋サ変动词词干＋する/いたす ★「いたす」自谦程度更高✿ 授業の後で お電話します。✿ それでは お願いいたします。 ★自谦句形不能用在单纯的说话人自己本身的行为动作及不涉及对方的行为动作上。★必须用在与对方有关的自己的动作上。 特殊动词 其他自谦语形式１、お（ご） + ＶＲ（サ変語幹）+ 申し上げるお客様を空港までお見送り申し上げます。把客人送到机场ご援助申し上げるつもりでございます。 我愿意为您效劳 郑重语郑重语不是对话题人物的尊敬，也不是对自己的自谦，而是用郑重地说话来表示对听话人的尊重。也是表示自己有高雅教养的表现。 郑重语的最基本的表现是です和ます。 其他还有ござる、まいる、いたす、おる等。✿ これが弟の写真です。✿ 私の父でございます。✿ 雪が降ってまいりました。✿ 何か変な匂いがいたしますよ。✿ 用意が出来ておりました。 美化语①加前缀：（同名词的尊他变形）お：お手紙、お話、お宅、お電話 （和语词，日常常用词）ご：ご案内、ご希望、ご協力 （汉语词） ②Ｎ本身变化めし：ご飯腹（はら）：お腹（おなか）便所（べんじょ）：お手洗い]]></content>
      <categories>
        <category>日语</category>
      </categories>
      <tags>
        <tag>敬语</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux基础命令]]></title>
    <url>%2F2019%2F01%2F05%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[电源管理shutdown now # 关机shutdown -h 30 # 30分钟后关机 shutdown -r now # 重启shutdown -r 05:30 # 于05:30重启shutdown -c # 取消关机 目录结构/bin： bin是Binary的缩写, 这个目录存放着最经常使用的命令。 /boot： 这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件。 /dev ： dev是Device(设备)的缩写, 该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。 /etc： 这个目录用来存放所有的系统管理所需要的配置文件和子目录。 /home： 用户的主目录，在Linux中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。 /lib： 这个目录里存放着系统最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。 /lost+found： 这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。 /media： linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。 /mnt： 系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。 /opt： 这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。 /proc： 这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。 这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的ping命令，使别人无法ping你的机器：echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all /root： 该目录为系统管理员，也称作超级权限者的用户主目录。 /sbin： s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序。 /selinux： 这个目录是Redhat/CentOS所特有的目录，Selinux是一个安全机制，类似于windows的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。 /srv： 该目录存放一些服务启动之后需要提取的数据。 /sys： 这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。 sysfs文件系统集成了下面3种文件系统的信息：针对进程信息的proc文件系统 针对设备的devfs文件系统 针对伪终端的devpts文件系统。/tmp： 这个目录是用来存放一些临时文件的。/usr： 这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于windows下的program files目录。/usr/bin： 系统用户使用的应用程序。/usr/sbin： 超级用户使用的比较高级的管理程序和系统守护程序。/usr/src：内核源代码默认的放置目录。/var： 这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 显示文件目录ls-a 列出隐藏文件，文件中以“.”开头的均为隐藏文件，如：~/.bashrc-l 列出文件的详细信息-R 连同子目录中的内容一起列出 文件权限-rwx-rwx-rwx # 第一个代表文件类型# 代表所有者的权限 # 代表所属组的权限# 代表其他人的权限 改变权限r 读取权限 如果没有r 就不能 ls 查看里面的内容 对应数字 4w 写权限 如果没有w 就不能在目录下创建新的文件 对应数字 2x 执行权限 如果没有x 就不能cd进入这个目录 对应数字 1- 没权限 对应数字 0chmod 777 filenamerwx-rwx-rwx 切换文件夹cd filename # 进入文件-filename：文件名cd - # 返回上一次进入的目录cd ~ # 进入根目录cd .. # 返回上级目录 查看当前路径pwd 创建目录mkdir filename # 创建一个filename的文件 删除空目录rmdir filename # 删除一个空的filename的文件 复制复制文件或目录cp file1 file2cp file1 dir/cp file1 ../ 拷贝目录cp dir1 dir2 -rcp dir1 ~/ -r 删除文件或目录rm -r # 递归删除文件rm -rf # 强制删除文件***** 查看文件从第一行开始；“-b”显示行号cat file # 一次查看所有的文件cat file1 file2 # 一次查看两个命令 从最后一行开始tac filename 显示的时候，顺道输出行号！nl filename 一页一页的显示文件内容more filename 按Space键：显示文本的下一屏内容。 按Enier键：只显示文本的下一行内容。 按斜线符/：接着输入一个模式，可以在文本中寻找下一个相匹配的模式。 按H键：显示帮助屏，该屏上有相关的帮助信息。 按B键：显示上一屏内容。 按Q键：退出more命令。 查找命令which command # 查看 -二进制文件whereis 可执行文件 # 二进制文件 、man手册帮助文档： 1.man手册 ，帮助文档 man ls 2.--help , ls --help 查找文件find 路径 参数# 常用参数 -name # 按照名字-size # 按照大小find ./ -size +100k -size -10M # 在当前目录下找大于100k 小于 10的文件 文本搜索grep &apos;content&apos; filename# 常用参数-v 显示不包含匹配文本的所有‘行’ (求反)-n 显示匹配行及行号-i 忽略大小写# 内容参数^wu 行首 搜索以wu开头的行wh$ 行尾 索以wh结束的行 创建链接文件ln file hardlink # 硬链接ln -s file softlink # 软链接 软链接: 相当于 window上的快捷方式 源文件删除则软链接失效 硬链接: 硬链接只能连接普通的文件 不能连接目录 注意 如果软链接文件和源文件不在同一个目录 源文件要使用绝对路径 不能使用相对路径 创建别名alias # 查看所有别名 alias c4=&apos;cat 4.txt&apos;unalias # 删除别名 注意 这种定义别名的方式 只在当前登录有效 如果要永久定义生效 可以通过修改~/.bashrc文件 这个修改要下次登录才能生效 想要立即生效 可以输入source ~/.bashrc]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些忘记书名的心理学笔记]]></title>
    <url>%2F2019%2F01%2F04%2F%E4%B8%80%E4%BA%9B%E5%BF%98%E8%AE%B0%E4%B9%A6%E5%90%8D%E7%9A%84%E5%BF%83%E7%90%86%E5%AD%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[酒与污水定律 把一勺酒倒入一桶污水，得到的是一桶污水；把一勺污水倒入一桶酒，得到的还是一桶污水。只要酒里有污水，再多的酒都是污水。 羊群效应 在组织散乱的羊群中，头羊起着关键作用，它的任何一个行动都会引起整个羊群中其他羊的关注，这些羊会效仿头羊。这是一种典型的从众心理，这种现象普遍存在于人类社会群体中。 安泰效应 指的是每种能力都要借助一定的条件和环境，一旦失去这种条件或环境，就可能失去某种能力。它告诉我们，人要学会借力，善于运用自己周围的环境。 踢猫效应 指人的不满情绪和糟糕心情，通常会沿着等级由高到低依次传递，由金字塔尖传到金字塔底，最终将危机转嫁到弱者身上。 矛盾选择定律 只有一只手表，可以告诉人们准确的时间，而拥有两只手表非但不能告诉一个人准确的时间，反而会让看表的人失去对准确时间的判断，这就是“手表定律”，也称“矛盾选择定律”。 贯性原理 你们对某样东西投入了巨大的精力，对它倾注了心血和金钱。你们投入的越多，贯性原理就越会促使你们想：“现在它必须成功。如果我再投入一点，它就会成功。” 如何对付错误和那些改变赢面的新情况，也是你们必须掌握的知识之一。生活有时候就像扑克游戏，有时候你们即使拿到一把非常喜欢的牌，但也必须学会放弃。 这时候，“剥夺性超级反映综合征”也会出现：如果不再投入一点，你们就要前功尽弃啦。人们就是这样破产的——因为他们不懂停下来反思，然后说：“我可以放弃这个，从头再来。我不会执迷不悟下去——那样的话我会破产的。” 社会科学理论 社会科学理论的命运取决于其传染性，而不是其正确性。 预期 假设一项计划预期在79天内完成。在第79天，假如计划还未完成，那么人们预测它还需要25天；但在第90天，假如计划还未完成，它会还需要58天；在第100天还需要89天；在第119天还需要149天；在第600天，如果计划还未完成，你会预测它还需要1590天。如你所见，你等待的时间越长，你预期还要继续等待的时间就越长。 喜欢 我们喜欢可触摸的东西、被证实的东西、显而易见的东西、真实的东西、可见的东西、具体的东西、已知的东西、已观察到的东西、生动的东西、视觉性的东西、有社会特点的东西、被灌输的东西、富有情感的东西、突出的东西、典型的东西、打动人心的东西、富有戏剧性的东西、传奇的东西、美化的东西、官方的东西、学术性的空话、虚有其表的高斯派经济学家、数学废话、华而不实的东西、法兰西学院、哈佛商学院、诺贝尔奖、黑西服白衬衣加领带、令人激动的演讲和耀眼的东西。而我们最喜欢的，是故事。 可惜，现存的人类天性不愿理解抽象事物，我们需要具体背景。随机性和不确定性是抽象事物。我们尊重发生的事，忽视本来可能发生的事。也就是说，我们天生肤浅，却浑然不知。这不是心理学问题，它来自信息的主要特性。人们很难看到月亮的阴面，照亮它是花费能量的。同样，照亮没有被看到的事物既费力又劳神。 假想实验 把一群各式各样的老鼠放进A实验室里，对它们进行越来越高的辐射。把幸存下来的老鼠放入城市B，幸存下来的老鼠在城市B老鼠中显得很强壮。看到的人于是分析为什么这些老鼠更强壮，因为他们来自A实验室，实验室把它们训练得很强壮。 沉默的证据 千多年前西塞罗讲了这样一个故事，有人把一幅画给一个无神论者看，画上画着一群正在祈祷的拜神者，他们在随后的沉船事故中幸存了下来。其寓意在于说明祈祷能保护人们不被淹死。无神论者问，‘那些祈祷后被淹死的人的画像在哪儿？’ 淹死的拜神者已经死了，所以很难从海底出来宣扬他们的事迹。 沉默的证据遍及所有与历史概念有关的一切，历史是具有事后影响的全部事件。 抓猴子的故事 树上挂一个空椰子，上面开个洞，放上米，猴子把手伸进去，抓了米，手就出不来。 长痛与短痛 短时间的巨大痛苦大于将痛苦在长时间中分散的痛苦; 短时间的巨大幸福小于长时间中分散的幸福。 改变他人 世界上唯一能影响对方的方法，就是讨论他所要的，而且还告诉他，如何才能得到。 也许在潜意识里，我们很希望能够通过我们的看法去左右别人的行为，因而会憎恨那些不受我们影响的人。 自重感 人们渴望成为重要的人，即自重感。献出你真实，诚恳的赞赏。 如果你想得到仇人，你就胜过你的朋友，如果你想获得更多的朋友，就让你的朋友胜过你。 当朋友胜过我们，他们会获得自重感。当我们胜过朋友，他会自卑，并引起猜疑和妒忌。 当我们猜疑，妒忌的人，发生一桩不幸的事，会使我们有一种恶意的快感。 有些朋友，看你遭遇困难，比看你成功或许更为满意。 批评与被批评 批评是没有用的，因它使人增加一层防御，而且竭力地替自己辩护。批评也是危险的，它会伤害一个人的自尊，和自重的感觉，并引起他的反抗。 被批评： 1.做你认为正确的事，反正你会受到批评，会因为做了某些事被骂，也会因为什么都不做而被骂。结果都是一样的。 2.如果你身居领导地位，那就注定要被批评，想办法习惯它吧！ 3.只要我不对任何的攻讦作出反应，那这件事就只有到此为止。 了解 你永远也不可能真正了解一个人，除非你穿上他的鞋子走来走去，站在他的角度考虑问题。 从众原则 大多数人好像都认为他们是对的，你是错的…… 他们当然有权利那样想，他们的看法也有权得到充分的尊重，但是，我在接受他人之前，首先要接受自己。有一种东西不能遵循从众原则，那就是人的良心。 为小事烦恼 错过列车，只有在你追赶它时才是痛苦的！同样，不能达到别人对你期望的成功，只有在它也是你所追求的东西时才是痛苦的。 只要是你的决定，放弃一份高薪职位带来的回报会超过金钱带给你的效用。这是向命运说“随你怎么样”的第一步。如果你确定了自己的标准，你对自己的生活会有大得多的控制。 我们很容易忘记我们活着本身就是极大的运气，一个可能性微小的事件，一个极大的偶然。 想象一个10亿倍于地球的行星边上的一粒尘埃。这粒尘埃就代表你出生的概率，庞大的行星则代表相反的概率。所以不要再为小事烦恼了。 是与不 使对方很快地回答“是，是“ 一个不字的反应，是最不容易克服的障碍。当一个人说出不字后，为了他人格的尊严，他不得不坚持到底。事后，他或许觉得他说出这个不字是错误的，可是，他必须考虑到自己的尊严。他所说的每句话，必须坚持到底，所以使人一开始，就往正面走，是非常重要的。 指责 尊重别人的意见，永远别指责对方是错误的。 我们不只反对有人指责我们的表错误，或者我们的汽车太旧，而是不愿意有人想要纠正我们的任何错误。 争辩 永远避免正面冲突、争辩 为什么一定要找出证据来证明别人的错误呢？ 这么做会让人喜欢你？ 他并没有征求你的意见，也不要你的意见，你又何必去跟他争辩呢？ 让人喜欢你的方法1.真诚地对别人发生兴趣2.微笑3.记住你所接触中，每一个人的姓名4.做一个静听的人，鼓励别人多谈谈他们自己。5.就别人的兴趣谈论6.使别人感觉到他的重要，必须真诚的这样做深入人们心底的最佳途径，就是对那人讲他知道得最多的事物。 保持愉快1.现在你何不问问自己：“我到底在烦恼些什么呢？”你多半会发现，你所担心的事既不重要，也没有意义。2.世上最好的医生，就是饮食有度、保持平和以及愉悦的心情。3.罗根·史密斯有一句智慧之言：“人生有两项主要目标，第一，拥有你所向往的；第二，享受它们。只有最具智慧的人才能做到第二点。” 工作1.如果你“假装”对工作有兴趣，一点点假装就会使你的兴趣成真，也可以减少你的疲劳。2.如果你是一个脑力劳动者，使你感觉疲劳的原因很少是因为你的工作超量，相反是由于你的工作量不足。3.要不停地提醒你自己，对自己的工作感兴趣，就能使你不再忧虑；而且最后还可能会给你带来升迁和加薪的机会。即使没有这么好的结果，那至少也可以使你的疲劳降到最低程度。 养成4种良好的工作习惯：第一，将你桌上所有的纸张收拾好，只留下你正要处理的问题。第二，根据事情的重要程度来安排做事的先后顺序。第三，当你遇到必须当场作决定的问题时，就当场解决，不要犹豫不决。第四，学会如何组织、分级负责和监督。 你可以把自己的生活想象成一个沙漏。在沙漏的上一半有成千上万粒的沙子，它们缓慢而均匀地流过中间那条细缝。除非把沙漏弄坏，否则，你和我都不能让两粒以上的沙子同时穿过那条窄缝。我们就如同沙漏。每天清晨醒来的时候，就有许许多多的工作摆在面前，要在这一天之内完成。但我们一定要均匀地安排自己的工作和生活，如果我们每次要几件事情同时做，就像要两粒以上的沙子同时通过窄缝一样，一定会损害自己的身体和精神。 表象 我们的头脑总是被所谓的真相，错误的“常识一样明白的”虚构的故事，以及服务于特别利益集团的带有偏见的结论所填满。一个有判别力的思考者要超越已有的信息，发掘隐藏在信息表面下的真正含义，以理解信息的本质为目标而不被表面的映像和风格所迷惑。技巧：1.避免把相关关系推论为因果关系。2.要求关键术语和概念有操作定义，并对其含义达成一致意见。3.你很容易在寻求辩解时发现确定的证据，但在寻找确定的证据前，首先要考虑如何反驳某一理论、假设或信仰。4.总是对已提出的明显解释寻求其他的可能解释，特别是那些有利于提案人的解释。5.认识到个人偏见能歪曲对现实的理解。6.怀疑对复杂问题给出的简单答案，怀疑对复杂现象和难题给出的单一理由和对策。7.质疑任何关于治疗、参与、或产品效果的声明，办法是找到比较结果的基础：比较什么？8.成为思想开朗而又好怀疑的人：认识到大多数结论都具有尝试性和不确定性；寻找新的证据来减少你的不确定性，同时使自己能不断变革和修正自己的观点。9.向权威挑战，那些权威通常用个人的观点代替证据，而且又不接受建设性的批评。 预期 通常，人们看见的,听到的只是他们所预期的，而不是事实的本来面目。 强迫 没有人喜欢强迫自己去买某样东西，或是被人派遣去做一件事。我们都喜欢随自己心愿去买东西，照自己的意思去做事情。同时希望有人跟我们谈谈我们的愿望，需要和想法。 失眠 下面是五条规则，可以让你不为失眠症而忧虑：第一，如果睡不着的话，就起来工作或看书，直到打瞌睡为止。第二，从来没有人会因为缺乏睡眠而死。因担心失眠而忧虑，通常对你的损害比失眠更厉害。第三，试着祈祷，或者像珍妮·麦克唐纳一样诵读诗篇的第二十三篇。第四，放松全身，看看《消除神经紧张》这本书。第五，多运动，或做一些体力活，直至你累得酣然入睡。 会议1.出了什么问题2.问题的原因是什么？3.有哪些可能解决的办法4.你觉得哪种方法最合适 职业 以下是我想向您请教的问题：①如果您的生命从头开始，您是否愿意再做一名建筑师？②在您仔细打量我之后，我想请问您，您是否认为我具备成为一名成功的建筑师的条件？③建筑师这一行业是否已经人满为患？④假如我学习了4年的建筑学课程，想要找到工作是否困难？我应该首先接受哪一类的工作？⑤如果我的能力属于中等，在头5年中，我可以希望赚多少钱？⑥当一名建筑师，有什么样的好处和坏处？⑦假如我是您的儿子，您愿意鼓励我当一名建筑师吗？ 古希腊哲学家艾皮科蒂塔说，哲学的精华就是：“一个人生活上的快乐，应该来自于尽可能减少对外来事物的依赖。”罗马的政治家及哲学家塞尼加也说：“如果你一直都觉得不满足，那即使是给你整个世界，你也会觉得伤心。” 忧虑1.混乱是产生忧虑的主要原因。在没有以客观的态度搜集到所有的事实之前，不要想如何解决问题。2.一切和我们欲望相符合的，看来都是真理，其他的都会使我们感到愤怒。 解决办法：（亚里士多德法则）第一，清楚地写下我们所担心的是什么。第二，写下我们可以怎么办以及可能发生的结果。第三，决定该怎么办。第四，马上按照决定去做。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>心理学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经济学笔记]]></title>
    <url>%2F2019%2F01%2F04%2F%E7%BB%8F%E6%B5%8E%E5%AD%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[复利 如果有1万，按每年投资回报率10%计算，10年收益为1.59万，20年为5.7万，30年为16.45万，40年为44.26万。所以在20岁投入10万，60岁养老金就有450万，在孩子1岁时，投入10万，孩子30岁时，就有160万。 然而国际公认平均收益率为12%，而投资自己，进行人力资源投资的投资收益最大。 二八定律 20%的客户会带来80%的收益 有个乞丐非常善于乞讨，每天都比同行赚得多，于是有同行就问他：“你有什么乞讨秘诀吗？” 这位乞丐说：“秘诀谈不上，不过我还是有点个人经验的。我从来不粘着顾客满街跑。如果乞讨不成，我会趁早放弃。因为他若肯给我钱，早就会给，就算他最后磨不过我，给了我钱，我也因此浪费了很多时间和精力，不如转而寻找下一个大目标。” 黑洞效应 黑洞效应：在宇宙中，一些大质量的物体在发生坍塌之后，会形成一个致密的点，由于它的质量非常大，所以产生的引力也非常大，大到光线进去之后也无法逃出来，于是就形成了一个黑洞。而且不断被吞噬进去的物质和能量又反过来成为黑洞的一部分，使得黑洞产生更大的吸引力。黑洞效应就是一种自我强化效应。当一个企业达到一定的规模之后，也会像一个黑洞一样产生非常强的吞噬和自我复制能力，把它势力所及的大量资源吸引过去，而这些资源使得企业更加强大，形成一个正向加速循环的旋涡。 黑洞效应使得资源和资本聚集，是产生社会贫富差距的原因之一。 沉没成本 沉没成本是指由于过去的决策已经发生了的，而不能由现在或将来的任何决策改变的成本。人们在决定是否去做一件事情的时候，不仅是看这件事对自己有没有好处，而且也看过去是不是已经在这件事情上有过投入。我们把这些已经发生不可收回的支出，如时间、金钱、精力等称为“沉没成本”。举例来说，如果你预订了一张电影票，已经付了票款且假设不能退票。此时你付的价钱已经不能收回，就算你不看电影钱也收不回来，电影票的价钱算做你的沉没成本。所以，如果你是理性的，那就不该在做决策时考虑沉没成本。 参照点 我们在头脑中形成参照点，比如销售预测，然后开始基于它构造信念，因为把一个观点与一个参照点进行比较比在绝对的环境下对它进行评价所需的思维努力更小。（系统1在起作用！）我们无法在没有参照点的情况下思考。 所以在预测者头脑中设置一个参照点能够带来奇妙的结果。在讨价还价过程中设置起点是一样的道理：你先提出一个较高的数字，如“这所房子要卖100万美元”，买方会说“只能85万”——议价过程将取决于初始报价。 讨价还价的故事 有一天，王先生到一个做服装生意的朋友那里去聊天。一个顾客看好了一套服装，服装的标价是800元。顾客说：“你便宜点吧，500元我就买！”朋友说：“你太狠了吧，再加80元！而且也图个吉利！”顾客说：“不行，就500元！”随后，他们又进行了一番讨价还价，最终朋友说：“好吧，就520元！”顾客去交款了，但是不一会儿又回来了。她有些不好意思地说：“算了，我不能买了，我带的钱不够了！”朋友又说：“有多少？”顾客说：“把零钱全算上也就只有430元了。”朋友为难地说：“那太少了，哪怕给我凑一个整数呢？”顾客说：“不是我不想买，的确是钱不够了！”最后，朋友似乎下了狠心，说：“就430元钱给你吧，算是给我开张了，说实在的，一分钱没有挣你的！”顾客满脸堆笑，兴高采烈地走了。 看着顾客远去的背影，朋友告诉王先生：“这件衣服是180元从广州进的货。”王先生听了哈哈大笑：“真是无商不奸啊，可是你有些太狠了吧？” 朋友说：“这你就是外行了，现在都时兴讲价，顾客讨价，我还价，这很正常。你要给顾客留出来讨价还价的空间，要让顾客心理上获得一种满足！其实这件衣服我300元的价格就卖，到换季的时候我本钱都往外抛。” 巧借名人效应的故事 美国一出版商有一批滞销书久久不能脱手，他忽然想出了一个主意：给总统送去一本书，并三番五次去征求意见。忙于政务的总统不愿与他多纠缠，便回了一句：“这本书不错。”出版商便借总统之名大做广告：“现有总统喜爱的书出售。”于是，这些书被一抢而空。不久，这个出版商又有书卖不出去，又送一本给总统。总统上过一回当，想奚落他，就说：“这书糟透了。”出版商闻之，脑子一转，又做广告：“现有总统讨厌的书出售。”不少人出于好奇争相抢购，书又售尽。 第三次，出版商将书送给总统。总统接受了前两次的教训，便不作任何答复。出版商却大做广告：“现有令总统难以下结论的书，欲购从速。”居然又被一抢而空，总统哭笑不得，商人却善借总统之名大发其财。 ~巧借名人效应，抱大腿。 博弈论智猪博弈 假设猪圈里有一头大猪、一头小猪。猪圈的一头有猪食槽，另一头安装着控制猪食供应的按钮，按一下按钮会有一定单位的猪食进槽，两头隔得很远。假设两头猪都是理性的猪，也就是说它们都是有着认识和懂得实现自身利益的猪。 再假设猪每次按动按钮都会有10个单位的饲料进入猪槽，但是并不是白白得到饲料的，猪在按按钮以及跑到食槽的过程中要付出的劳动会消耗相当于2个单位饲料的能量。此外，当一头猪按了按钮之后再跑回食槽的时候，它吃到的东西比另一头猪要少。也就是说，按按钮的猪不但要消耗2单位饲料的能量，还比等待的那个猪吃得少。再来看具体的情况，如果大猪去按按钮，小猪等待，大猪能吃到6份饲料，小猪4份，那么大猪消耗掉2份，最后大猪和小猪的收益为4∶4；如果小猪去按按钮，大猪等待，大猪能吃到9份饲料，小猪1份，那么小猪消耗掉2份，最后大猪和小猪的收益为9∶-1；若两头猪同时跑向按钮，那么大猪可以吃到7份饲料，而小猪可以吃到3份饲料，最后大猪和小猪收益为5∶1；最后一种情况就是两头猪都不动，那它们当然都吃不到东西，两头猪的收益就为0。 那么小猪努力的结果是-1，比不努力的结果4还差，所以小猪只能不动。大猪为了生存，只能来回跑。 启发：小企业可以搭大企业的便车，坐收渔翁之利。 囚徒困境 警方逮捕甲、乙两名嫌疑犯，但没有足够证据指控二人有罪。于是警方分开囚禁嫌疑犯，分别和二人见面，并向双方提供以下相同的选择： 若一人认罪并作证检控对方（相关术语称“背叛”对方），而对方保持沉默，此人将即时获释，沉默者将判监10年。若二人都保持沉默（相关术语称互相“合作”），则二人同样判监半年。若二人都互相检举（互相“背叛”），则二人同样判监5年。 囚徒困境假定每个参与者（即“囚徒”）都是利己的，即都寻求最大自身利益，而不关心另一参与者的利益。参与者某一策略所得利益，如果在任何情况下都比其他策略要低的话，此策略称为“严格劣势”，理性的参与者绝不会选择。另外，没有任何其他力量干预个人决策，参与者可完全按照自己意愿选择策略。 囚徒到底应该选择哪一项策略，才能将自己个人的刑期缩至最短？两名囚徒由于隔绝监禁，并不知道对方选择；而即使他们能交谈，还是未必能够尽信对方不会反口。就个人的理性选择而言，检举背叛对方所得刑期，总比沉默要来得低。试设想困境中两名理性囚徒会如何作出选择： 1、若对方沉默、我背叛会让我获释，所以会选择背叛。2、若对方背叛指控我，我也要指控对方才能得到较低的刑期，所以也是会选择背叛。 二人面对的情况一样，所以二人的理性思考都会得出相同的结论——选择背叛。背叛是两种策略之中的支配性策略。因此，这场博弈中唯一可能达到的纳什均衡，就是双方参与者都背叛对方，结果二人同样服刑5年。 这场博弈的纳什均衡，显然不是顾及团体利益的帕累托最优解决方案。以全体利益而言，如果两个参与者都合作保持沉默，两人都只会被判刑半年，总体利益更高，结果也比两人背叛对方、判刑5年的情况较佳。但根据以上假设，二人均为理性的个人，且只追求自己个人利益。均衡状况会是两个囚徒都选择背叛，结果二人判监均比合作为高，总体利益较合作为低。这就是“困境”所在。例子有效地证明了：非零和博弈中，帕累托最优和纳什均衡是互相冲突的。 斗鸡博弈 两只公鸡狭路相逢，即将展开一场撕杀。结果有四种可能：两只公鸡对峙，谁也不让谁。或者两者相斗。这两种可能性的结局一样——两败俱伤，这是谁也不愿意的。另两种可能是一退一进。但退者有损失、丢面子或消耗体力，谁退谁进呢？双方都不愿退，也知道对方不愿退。在这样的博弈中，要想取胜，就要在气势上压倒对方，至少要显示出破釜沉舟、背水一战的决心来，以迫使对方退却。但到最后的关键时刻，必有一方要退下来，除非真正抱定鱼死网破的决心。但把自己放在对方的位置上考虑，如果进的一方给予退的一方以补偿？只要这种补偿与损失相当，就会有愿意退者。 这类博弈也不胜枚举。如两人反向过同一独木桥，一般来说，必有一人选择后退。在该种博弈中，非理性、非理智的形象塑造往往是一种可选择的策略运用。如那种看上去不把自己的生命当回事的人，或者看上去有点醉醺醺、傻乎乎的人，往往能逼退独木桥上的另一人。还有夫妻争吵也常常是一个“斗鸡博弈”，吵到最后，一般地，总有一方对于对方的唠叨、责骂装聋作哑，或者干脆妻子回娘家去冷却怒火。冷战期间，美苏两大军事集团的争斗也是一种“斗鸡博弈”。在企业经营方面，在市场容量有限的条件下，一家企业投资了某一项目，另一家企业便会放弃对该项目的觊觎。 斗鸡博弈强调的是，如何在博弈中采用妥协的方式取得利益。如果双方都换位思考，它们可以就补偿进行谈判，最后造成以补偿换退让的协议，问题就解决了。博弈中经常有妥协，双方能换位思考就可以较容易地达成协议。考虑自己得到多少补偿才愿意退，并用自己的想法来理解对方。只从自己立场出发考虑问题，不愿退，又不想给对方一定的补偿，僵局就难以打破。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>经济学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《禅与摩托车维修艺术》摘录]]></title>
    <url>%2F2019%2F01%2F04%2F%E3%80%8A%E7%A6%85%E4%B8%8E%E6%91%A9%E6%89%98%E8%BD%A6%E7%BB%B4%E4%BF%AE%E8%89%BA%E6%9C%AF%E3%80%8B%E6%91%98%E5%BD%95%2F</url>
    <content type="text"><![CDATA[自我爬山者，他谈论的话题永远是别的事和别的地方。他的人虽然在这里，但是他的心却不在这里。因为他拒绝活在此时此刻，他想要赶快爬到山顶，但是一旦爬上去之后仍然不快乐，因为山顶立刻就变成了‘此地’。他追寻的，他想要的都已经围绕在他的四周，但是他并不要这一切，因为这些就在他旁边。于是在体力和精神上，他所跨出的每一步都很吃力，因为他总认为自己的目标在远方。 一个没有目标的人才能爬到最高。（by克伦威尔）例子：取消考试成绩加入良质的概念 归纳法：由个别的经验归纳出普遍的原则 演绎法:从一般的原则推论出特定的结果科学式的思考：问题是什么；假设问题的原因；证实每个问题的假设；预测实验的结果。观察实验的结果；由实验得出结论。 古典的认知认为这个世界是由一些基本形式组成的，而浪漫的认知则是从它的表象来观察。 人在思考和感觉的时候往往会偏向于某一种形式，而且会误解和看轻另一种形式。 熟悉往往会使一个人视而不见。 我们观察周遭成千上万的事物，你知道他们存在，但是你并没有全部注意到它们，除非出现某些奇特的或是我们容易观察到的事物。我们几乎不可能全部意识到这些东西，而且把它们记住。那样，我们心里会充满太多无用的细枝末节，从而无法思考。从这些观察当中，我们必须加以选择，而我们所选择的和所观察到的，永远不一样，因为经由选择而产生了变化。我们从所观察到的事物当中选出一把沙子，然后称这把沙子为世界。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>哲学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《当我遇见一个人》摘录]]></title>
    <url>%2F2019%2F01%2F04%2F%E3%80%8A%E5%BD%93%E6%88%91%E9%81%87%E8%A7%81%E4%B8%80%E4%B8%AA%E4%BA%BA%E3%80%8B%E6%91%98%E5%BD%95%2F</url>
    <content type="text"><![CDATA[当一个生命带着极大的爱和信任降临到家庭中，他最渴望的是被看见， 被父母看见真实的自己，而不是通过一堆‘正确’的数据来评价和矫正自己。 当我放下预期和目的，以我的全部本真与一个人或事物建立关系时，我就会与这个存在的全部本真相遇。 父母看不到孩子本身，他们看到的是孩子的功能价值。父母能否看到孩子本身的存在，而不是用外在价值去定义物质性的‘它’，这一点决定了孩子内心能否直接感受到爱。若孩子本然的存在不被看见，即使父母为孩子倾注一切，孩子也只是父母表达爱的道具。 爱，是如他所是，非吾所愿 我拒绝了这件事情，不等于拒绝你这个人，不等于你提的要求不合理，不等于我不在乎你。我的拒绝仅仅是因为现在我不想这样做。拒绝的同时，我不会把自己关闭，我依然感受到你的爱，理解你的需要，理解自己的需要，让我们的需要共同创造出爱的方式。如果我答应你，一定是因为我也喜欢用这种方式爱你，而不是迫于维持关系的委曲求全，所以即使我付出再多，你也不必内疚。 不带评判地拒绝，没有委屈地付出，爱的流动如此之美。 规则要建立在尊重感受的基础上，一个人内心极度缺爱，同时对得到爱已经绝望，会通过牺牲自己来满足所爱的人，间接地满足自己内在的小孩，而实际上，所爱的人抢走了那个内在小孩的爱，她付出越多，也就越怨恨所爱的人。 最好的教育就是不教育。在爱的陪伴下，不打扰就是对专注力最好的培养。孩子的精神世界我们无须全部了解，但需要时常放下成人已被高度训练过的头脑的假想，带着敬畏之心去体验。 我们面对痛苦时最容易做的就是直接解决掉引起痛苦的外在的人或事。真正的勇士，是直面痛苦，向内看的人。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>心理学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[语法总结（5-15）]]></title>
    <url>%2F2019%2F01%2F04%2F%E8%AF%AD%E6%B3%95%E6%80%BB%E7%BB%93%EF%BC%885-15%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本篇记录综合日语5-15的语法，以便查询 Ｎ１はＮ２です名词谓语句:什么是什么否定形式：Ｎ１はＮ２では/じゃありません。✿问 ：Ｎ１はＮ２ですか。✿回答：はい、そうです。 いいえ、違います。 Ｎ１で、Ｎ２です✿高橋さんは高校の後輩で、今、京華大学の語学研究生です。✿京華大学はそちらで、北京大学はあちらです ～へようこそ迎接客人时使用的寒暄语，相当于汉语中的 {欢迎来到~}正式的说法是：ようこそいらっしゃいました✿中国へようこそ。 ～と申します用于介绍说话人自己的名字，一般用于正式场合，或对上级的人，语气比较郑重。✿高橋と申します。どうぞよろしく。 Ｎ１からＮ２まで相当于汉语的“从什么到什么之意”a:表时间范围b:表处所、顺序等的范围。✿授業は木曜日から金曜日までです。✿授業は何時からですか。 Ｎじゃありませんか不是~吗？✿今は呉先生の中国史の授業じゃありませんか。 どんなＮですか相当于汉语:什么样的，怎么样的✿呉先生はどんな先生ですか。 Ｎ１やＮ２など用于列举两个或两个以上的事物。(など可以省略)✿専門は中国語や中国経済、中国史などです。 注：如果用 “Ｎ１とＮ２とＮ３と” ，必须列举出全部事物 そんなにA１くないです/A２ではありません相当于汉语中的“并没有那么”（主观想法）✿そんなに簡単ではありません。 あまりA１くないです/A２ではありません单纯地表示程度不高，相当于汉语中的“不太，不怎么”（客观上）✿あまり簡単ではありません。 Ｎはどうですか用于询问事物性质，相当于汉语中的“怎么样。如何” ✿问：英語はどうですか。 ✿答：英語はやさしいです。 用于表示建议，相当于汉语中的“怎么样。如何”✿６時はどうですか。 Ｎはどうでしたか用于询问过去发生的事情结果或者情形✿きのうの試験はどうでしたか。 Ｎ１もＮ２も表示并列，相当于“~和~都”✿中国語の聞き取りも発音もとても難しいです。 Ｎのとき用于表示时间，“~时候、时”的意思。✿大学創立の時は、まだ学部は少なかったです。✿一年生のときの相互学習はとてもよかったですね。 時間＋ごろ表示大概时间点，在几点“左右、前后” Ｎと同じ与~相同。✿美穂さんですか。私と同じ名前ですね。 Ｎ１はＮ２がAです大小主语句，即大小主语之间一般为整体与部分✿日本語はアクセントが難しいです。✿私は英語が上手です。 ほとんどVません相当于汉语的“几乎没~，基本上不~”等✿手紙はほとんど書きません。 ＮしかＶません用于限定范围，相当于汉语中的“只，仅仅”之意✿明日しか休みません。 ～とか～とか口语句，“~啦~啦”✿料理はお寿司とかケーキとかです。 Ｎのあとで~之后✿あしたの授業の後、ここで宿題をします。 疑问词＋か表示虚指，不确定之意。回答要用はい、いいえ先说明，相当于“有没有”✿休みにどこかへ遊びに行きましたか。 疑问词+{格助词+}も+动词否定表示全面否定。 格助词是{が}或{を}时，一般省略。✿その後は何もしませんでした。 Ｎ１かN２表示选择性的并列，“N1或者N2” ✿明日、李さんか王さんが行きます。✿散歩か買い物に行きます。 Ｖましょう。表示建议语气的敬体形式，用于建议对方和自己一起做某事。相当于汉语中的“~吧、~怎么样” Ｎ１に｛は｝Ｎ２があります/います某地有某物 ✿机の上に本があります。 Ｎ１はＮ２にあります/います某物在某地✿王さんは教室にいます。 ～んです用于口语，书面语写为｛のです｝。接在动词、形容词的连体形后面，用于说明情况或者询问情况。✿東京の冬はあまり寒くないですね。北京の冬は寒いんですよ。✿人民大会堂もここにあるんですか。 Ｎが見えます看得见~看得到~✿晴れの日に、星がたくさん見える。 までに期限，表示该时间之前完成某一动作。 “在~之前” ✿本は１０日までに返します。 でも接在名词或者に、と等格助词后面；提建议时举出一个例子供对方考虑，语气比较委婉。✿お茶でも飲みましょうか。✿公園にでも行きましょうか。 Ｖませんか建议，“不一起~吗？”✿一緒に公園に行きませんか。 ～でしょう推测，“~吧” ✿１時ごろからは混むでしょう。 Ｎになる/A1くなる/A2になる客观事实的结果或状态✿そろそろ１２時になりますね。✿肌がきれいになりますよ。 Ｎにする、くする、にする人为造成的变化结果或状态✿音をもう少し大きくしてもいいですか。✿教室をきれいにしてください。 Ｎにする表示所选择或决定的事物，相当于 “要~，定~”✿どちらもきれいですね、じゃ、やっぱり赤にしましょう。 Ｖています1、表示某一动作正在进行✿今、授業をしています。 2、表示变化结果的持续✿もう外暗くなっています。 3、表示习惯性的动作，反复进行的动作及长期进行的动作✿学生の大多数は学校の寮に住んでいます。 Ｖていました表示在过去的某一时段或时点上的持续动作 ✿午前中図書館で勉強していました。✿８時から１０時まではテレビを見ていました。 って用于提出话题，在口语中使用。✿鈴木さんってどんな人ですか。 もうＶました表示动作已经完成，“已经~了” ✿もうタイトルは決めましたか。✿王さんはもう帰りました。 まだＶていません。还没，尚未做某事。✿朝からまだ何も食べていません。✿まだ来ていません。 Ｖて、Ｖて、Ｖます。表示连续进行几个动作在时间上的先后顺序。✿朝起きて、運動をして、食事をして、会社に行きます。 Ｖたい接在动词的第一连用形后面构成愿望的派生形容词，“想做~~”。使用Ｖたいですか询问第二、三人称的愿望被视为不礼貌的行为。✿おいしいものが食べたいです。✿有名な大学に入りたいです。 注：当表达第三人称具有进行某一动作的愿望时，一般用Ｖたがる这种形式。这时を格不能够改为が✿父は新しいパソコンを買いたがっています。✿先生は山本に会いたがっていました。 Ｎがほしい表示希望，想要得到某东西，其非过去式只能用于表示第一人称的愿望，直接问对方Ｎがほしいですか是不礼貌的。 ✿家族と友達へのお土産がほしいですが…。✿時間がほしいです。 Ｎができます表示具备某种能力，能做某事，会做某事✿日本語で買い物ができますか。✿王さんはテニスができます。 Ｎがわかります表示对人或事物的了解,助词一般用が，否定句中多用は。相当于”明白，了解”的意思。 ✿店員は大体日本語がわかります。✿この漢字の意味がわかりますか。 Ｖてもいい表允许，同意别人做某事。或者是询问别人的同意。✿試着してもいいですか。✿ここに座ってもいいですか。 Ｎはいかがですか用于提出建议并征求对方意见，或询问情况。表示“~怎么样 ~如何”等。是Ｎはどうですか的郑重表达方式✿もう一杯いかがですか。✿今晩、日本料理はいかがですか。 数量词+も带有说话人的主观评价色彩， “竟，足足，达”。✿駅で１時間も友達を待ちました。 だけ只有，仅仅。用在格助词が、を前面时，が、を有时省略。 ✿これは日英だけの辞書ですか。✿今朝は果物だけ｛を｝食べました。 Ｖてください请求对方做某事，一般不对上级或长者使用。✿それを見せてください。✿日本語を教えてください。 动词词典形+ことができる表示：①具有做某事的能力，②表示某种条件下行动行为的可能性。✿図書館のパソコンは何時まで使うことができますか。✿金さんはフランス語を話すことができます。 ずつ接在表示数量词或表示数量，程度的副词后面，表示数量的均等分配或者动作、变化的等量反复。 ✿毎日、少しずつ練習しています。✿りんごは一人に２つずつある。 Ｖてはいけない表示禁止，相当于不许、不准，多用于规则纪律，由于语气强烈，最好不要用于当面禁止别人做某事。✿教室ではタバコを吸ってはいけない。 Ｖないでください表示禁止，请不要做~、请勿~✿気にしないでください。✿大きい声を出さないでください。 Ｖている/Ｖないとき~的时候✿人が下を歩いているときに、窓からごみを捨ててはいけない。✿私は家に誰もいないとき、自分で料理を作ります。 Ｖなくてもいい表示不做什么也可以 ✿靴を脱がなくてもいいですよ。✿もう薬を飲まなくてもいいですよ。 ＶるＶた/とき{に}✿日本へ行くとき、パソコンを買いました。去日本之前买了电脑。✿日本へ行ったとき、パソコンを買いました。去日本之后买了电脑。 Ｖなくては｛なければ｝いけない同：Ｖなくては｛なければ｝ならない表示：必须做某事，应该做某事在口语中なくては经常说成なくちゃ；なければ经常说成なきゃ✿もう、病院へ戻らなくちゃいけません。✿日本で家に入るとき、靴を脱がなくてはいけないんでしょう。 どうして…（ん）ですか为什么~呢？✿山本さんはどうして来ないんですか。✿どうして先生に話さなかったんですか。 ～でしょう表示确认，要读升调✿もう宿題は終わったでしょう。✿まだいいでしょう。 Ｎについて/Ｎについての接在名词后面表示“关于，有关”✿日本文化についての資料を集めています。 ～と言う表示直接引语✿食事のとき、日本は皆で「いただきます」と言う。✿日本人は夜寝るときに、「お休みなさい」と言う。 N１というＮ2叫做~的~✿渡辺さんという人を知っていますか。✿これは何という花ですか。 Ｎが好き/嫌い喜欢，讨厌某物，表示感情的形容词助词用が✿高橋さん、京劇が好きですよね。✿鈴木さんは高橋さんが好きです。 Ｎ &lt;周期&gt; に Ｎ &lt;数量&gt;表示某一周期内动作的频率✿３ヶ月に１回ぐらい何かを見ていました。✿この薬を一日に３回飲んでください。 Ｖ简体の动词名词化✿明日、三保さんが来るのを知っていますか。 Ｎ1だけで｛じゃ｝なく、Ｎ2も不仅N1~N2也~ ✿あの人は英語だけでなく、日本語も話せます。✿肉だけでなく、野菜も食べなければならないんです。 ＮでもＮでも｛いい｝~和~都可以 ✿土曜日でも日曜日でも大丈夫です。✿男の子でも女の子でもいいんですよ。 なかなかＶ{能动态}ない与动词能动态否定形式搭配，表示该动作很难做到。 ✿みんな忙しい、なかなか会えません。✿この町では刺身はなかなか食べられません。 Ｎ1はＮ2より{比较}N1比~N2要~~~ ✿母は父より朝早く起きます。✿月曜日は火曜日より忙しい。 Ｎ1よりＮ2のほうが～比起N1 ，N2更~✿父より母のほうが朝早く起きます。✿火曜日より月曜日のほうが忙しい。 Ｎのなかで｝Ｎが一番～在~中~是最~✿クラスの中で、王さんが一番日本語が上手です。✿中華料理の中では、北京ダックが一番おいしいです。 ｛Ｎ1もＮ2も｝どちらも｛同じくらい｝～N1还是N2都一样~✿どちらも同じくらい好きです。✿山本さんは英語も日本語も、どちらも上手です。 Ｎ1はＮ2とともにN1和N2一样✿英語は日本語とともに必修科目である。 ～と思う～Ｖ意志形と思う 表达第一人称当时的意愿，想法。✿きょうは早く帰ろうと思います。 ～Ｖ意志形と思っている表达第一人称一直以来的想法，表达其他人称的意愿，想法。✿私は将来教師になろうと思っています。✿王さんは日本に行こうと思っている。 ～Ｖたいと思っている 单纯地表达一个愿望，没有落实行动，只停留在想法上。✿私は日本に行きたいと思っている。 简体句と思う。表示“我认为～，我觉得～” ✿プレゼント交換がいいと思う 简体句と思っている认为~~（无人称区别） ✿英語より日本語のほうが難しいと思っている人が多いです。 动词词典形+予定です表示某人的计划，客观性比较强。✿夏休みは国に帰る予定です。✿私たち日本語学科もコンパを開く予定です。 动词词典形+つもりです表示打算做某事，主观性比较强。✿夏休みには、小説をたくさん読むつもりです。✿あしたからはタバコを吸わないつもりです。 ～かどうか接续：动词、形1简体，名词、形2词干 ~还是不~✿東京の冬は寒いかどうか、日本人の友達に聞きます。✿このことは、あの人が知っているかどうか、わかりません。 だろうだろう接在动词、形容词的简体形式以及形容动词的词干，名词后面表示推测，是的でしょう简体。✿その辞書は高いだろうと思う。✿あそこへは電車よりバスのほうが便利だろう。 Ｖたことがある表示曾经有做过某事的经历✿私は富士山に登ったことがあります。✿私は一度も飛行機に乗ったことがありません。 动词词典形+ことがある有时、偶尔做某事✿私はははと喧嘩するこがある ばかり接在名词或格助词后面，用于限定，带有消极的感情色彩。表示： “光做某事，净做某事”✿あの人は毎日テレビばかり見て、何もしません。 授受动词给给我✿句型：Aは｛私に｝～をくれる。✿意思：A给我某物。✿注：私に可以省略。 给其他人✿句型：A（我或我方的人）はBに～をあげる。✿意思：A给B某物。 收✿句型：AはBに/から～をもらう。✿意思：A从B那里得到某物。✿注：这个句子中的B一般不用わたし。 具体用法 动词 ✿Ｖます：动词敬体（礼貌体），第一连用形。#V变形后去掉的形态为动词的第一连用形，它用于句子的中顿，#大多表示两个或两个以上的动词并列，也可以表示动作先后顺序，#通常用于书面语。 ✿Ｖない动词未然形（动词的简体否定） ✿Ｖた动词简体过去时 ✿Ｖて形表示动作的中顿，语法活用 ✿动词能动态#表示“可能”的意义。#既可以表示人具有某种能力，也可以表示在某种状态下行为动作的可能性。 ✿Ｖ（よ）う 意志形#动词后接表示意志、建议后缀的事构成动词的意志，建议形。 详见 动词总结]]></content>
      <categories>
        <category>日语</category>
      </categories>
      <tags>
        <tag>语法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[on_my_zsh]]></title>
    <url>%2F2019%2F01%2F03%2Fon-my-zsh%2F</url>
    <content type="text"><![CDATA[1. 安装 Oh My Zsh安装 Zsh安装:sudo apt install zsh 将 Zsh 设置为默认 Shellchsh -s /bin/zsh 安装 Oh My Zshwget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | sh 2. 配置 Oh My Zsh配置字体git clone https://github.com/bhilburn/powerlevel9k.git ~/.oh-my-zsh/custom/themes/powerlevel9k autojump更快地切换目录，不受当前所在目录的限制。sudo apt install autojump# 跳转到目录j dir# 可以通过GUI文件管理器打开指定目录，执行命令:jo dir fasd快速访问文件或目录，功能比前一个插件强大。 安装：sudo apt install fasd 使用：alias f=&apos;fasd -f&apos; # 文件alias d=&apos;fasd -d&apos; # 目录alias a=&apos;fasd -a&apos; # 任意alias s=&apos;fasd -si&apos; # 显示并选择alias sd=&apos;fasd -sid&apos; # 选择目录alias sf=&apos;fasd -sif&apos; # 选择文件alias z=&apos;fasd_cd -d&apos; # 跳转至目录alias zz=&apos;fasd_cd -d -i&apos; # 选择并跳转至目录 zsh-autosuggestions命令行命令键入时的历史命令建议插件git clone https://github.com/zsh-users/zsh-autosuggestions $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-autosuggestions zsh-syntax-highlighting语法高亮插件git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-syntax-highlighting 修改「.zshrc」文件Oh My Zsh 配置文件的完整修改结果，只有对配置文件进行如下修改，才能使上述配置生效。# 设置字体模式以及配置命令行的主题，语句顺序不能颠倒POWERLEVEL9K_MODE=&apos;nerdfont-complete&apos;ZSH_THEME=&quot;powerlevel9k/powerlevel9k&quot;# 以下内容去掉注释即可生效：# 启动错误命令自动更正ENABLE_CORRECTION=&quot;true&quot;# 在命令执行的过程中，使用小红点进行提示COMPLETION_WAITING_DOTS=&quot;true&quot;# 启用已安装的插件plugins=( git extract fasd zsh-autosuggestions zsh-syntax-highlighting) 添加环境变量打开配置文件vim ~/.zshrc 添加环境变量export PATH=/usr/local/python/bin:$PATH 注：环境变量中，各个值是以“冒号”分隔开的。上面的语句表示给PATH这个变量重新赋值，让它等于/usr/local/python/bin ；再加上原来的$PATH 使配置生效source ~/.zshrc]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>zsh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anconda常用命令总结]]></title>
    <url>%2F2019%2F01%2F02%2FAnconda%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[anaconda是python发行的包的管理工具，其中自带python的版本，还带很多python的包.安装它比起安装python可以省掉再安装python包的时间。并且利用 Python 进行科学计算，就需要一一安装所需的模块， 而这些模块可能又依赖于其它的软件包或库，安装和使用起来相对麻烦但是用Anaconda只需直接输入conda install &lt;包名&gt;，所有依赖的包和库会自动安装好。 下载地址官网：https://www.anaconda.com/最新版本下载地址：https://www.anaconda.com/download/历史版本：https://repo.anaconda.com/archive/ Anaconda的安装进入文件所在路径bash Anaconda3-4.4.0-Linux-x86_64.sh (下载的对应的文件名) conda的环境管理比如创建名为tensorflow的tensorflow-gpu版的python环境conda create --name tensorflow python=3.6 tensorflow-gpu 激活tensorflow环境source activate tensorflow 测试环境python --version 退出虚拟环境source deactivate tensorflow 删除虚拟环境conda remove --name tensorflow --all Conda的包管理安装包conda install -n tensorflow sklearn注：如果不用-n指定环境名称，则被安装在当前活跃环境 删除包conda remove -n tensorflow numpy 在当前环境下安装anaconda包集合conda install anaconda 添加Anaconda的TUNA镜像conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ jupter notebook添加conda环境首先安装：conda install ipykernel 激活对应的conda环境source activate 环境名称 将环境写入notebook的kernel中python -m ipykernel install --user --name 环境名称 --display-name &quot;Python (环境名称)&quot;]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>anconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[11-18句型总结]]></title>
    <url>%2F2019%2F01%2F02%2F%E5%8F%A5%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[名词化 の vs こと1、一般而言「こと」指事。表示向别人传达的内容或决定的内容、概念、以及 抽象的事情时使用。2、「の」可以指人、物、事。表示自己实际感觉到的做的事情等具体的行动或体 验时使用。有时二者可通用。 ✿ 友達に電話する こと/の を忘れた。 ✿ 明日鈴木さんが来る こと/の を知っていました。 「こと」しか使えない場合1、后续动词为传达、表达等语言行为的动词，如「話す、伝える、教える」等。 ✿ みんなに会議があることを伝えてください。2、名词化的动词直接加「です」结句。 ✿ 私の趣味は映画を見ることです。3、后续为表示意志的动词 ，「命じる、许す、决める、願う、约束する」等。 ✿ 結婚することに決めました。 「の」しか使えない場合1、后续动词是表示感觉、知觉等感官动词，如「聞こえる、見える、感じる、 聞く、見る、気がつく」等。 ✿ 誰か叫んでいるのを聞こえました。 ✿先生が車から降りるのを見ました。2、在句型「Ｖのは、～だ」中，一般不能换。 ✿ 彼女が買ったのは、この赤いかばんです。 たい VS ほしいＶたい“想做某事”１、接续：动词第一连用形+たい（只能用于第一人称） ✿ 美味しいものが食べたいです。2、「Ｖたかった」过去时可以用于第一、二，三人称，陈述已发生的事实。 ✿ 彼女はあのケーキが食べたかったです。３、第三人称一般使用「Vたがる」的形式，构成一类的派生动词。做他动词使用。 ✿ 高橋さんはアイスを食べたがります。 Nがほしい”想要某物”１、表示希望，想要得到某东西。一般只能用于第一人称。 ✿ お金が欲しいです。2、「Nがほしかった」过去时可以用于第一、二，三人称，陈述已发生的事实。 ✿ 李さんはゲーム機が欲しかったです。３、第三人称一般使用「ほしがる」的形式，构成一类的派生动词。做他动词使用。 ✿ 山田さんは休みを欲しがっています。 Vてほしい“希望别人做某事”１、动词て形+ほしい。表示说话人希望听话人采取某一行动，或者造成某种状态。 ✿ 明日の朝も来て欲しい。 ✿ 君にこの仕事を担当して欲しい。 あいだ VS あいだにあいだに&lt;时点&gt;1、表示在某状态持续的阶段或时期内的某一时点上的某一动作或变化。 ✿ 留守のあいだに、泥棒が入った。 あいだ&lt;时段&gt;1、表示在某状态持续的整个阶段，时期内持续地进行了谓语动词的动作。 常与ずっと搭配。后句一般会用到表示持续状态的ている。 ✿ 彼は会議のあいだ、ずっと居眠りをしていた。 Vたら～た&lt;契机~发现&gt;1、表示以前项为契机，紧接着发生了后项的事件或发现了后项的状态。 ✿ 本屋に行ったら、偶然学生時代の友人に会った。★ 后项事物一般为说话人意志所不能及的事物，或是一些新发现、新认识等， 具有意外性。 と～と&lt;条件&gt; “一～就～、（如果）~就~1、达成某种条件就会得到必然结果、现象。 主要用在表述：恒常性状态，真理， 指路，反复性状态，习惯等方面。 ✿ 前の交差点を右に曲がると、学校が見える。 ～と&lt;反复、习惯&gt;“一~就~ ”1、表示特定的人或物的习惯和动作的反复，常伴有“いつも、よく”等副词。 ✿ 毎年、冬になるとスキーに行く。 ～ないと～ない＜否定性条件＞1、否定性条件从句，后项常表否定性意义。口语中后项常省略。表示前项不成立 的话，后项一定会出现某消极或否定结果。 ✿ 早く行かないと、間に合わないよ。 (否定结果) ✿ 急がないと遅刻するよ。(消极结果) よにVる / Ｖないようにする1、表示设法做（或不做）某事。为实现某种状态做（或不做）某事。可以与｢でき るだけ、必ず、ちゃんと｣等构成呼应。 ✿ 夜は甘いものを食べないようにしている。 Vる / Ｖないようにしてください1、对听话人表示要求、忠告或劝告。”请尽量~、请注意~” ✿ ちゃんとメモを取るようにしてください。 句型1、Ｖましょうか/ましょう&lt;意志、征求同意&gt;2、って&lt;话题&gt;3、N/A２にする/A１くする4、どうして&lt;原因（疑问）&gt;5、でしょう&lt;确认&gt;6、Nについて/Nについての7、N１は N２より～ （比较句）8、N２よりN１のほう（方）が～ （比较句）9、N1はＮ２ほど～ない （比较句）10、N１だけで（じゃ）なくN２も～ 不光/不仅……还/而且……11、(N1の中で) Ｎ２が一番～12、Ｎ１でもＮ２でも（いい）13、Ｎ１とＮ２と（では）どちら（のほう）が～14、Ｎ１はＮ２とともに相同15、それで：因果关系16、~かどうか&lt;选择&gt;17、Ｖたことがある&lt;经历&gt;18、Vることがある&lt;频率低&gt;19、~だろう&lt;推测&gt;20、Nがする&lt;感觉&gt;21、Nによって（違う）&lt;基准&gt;22、どうやって～んですか&lt;方式&gt;23、Nにとって&lt;评价的立场和角度&gt;24、はず&lt;判断，估计&gt;25、かもしれない&lt;推测&gt;26、～途中で＜动作进行中＞27、～のは～（ ～からではなくて）からだ28、疑问词 ＋でも＋肯定句＜全面肯定＞29、～まえに＜动作的顺序＞30、～あとで＜先后顺序＞ 31、それに&lt;并列&gt;：~，而且~32、それとも&lt;选择&gt;：是~还是~33、~とおり&lt;基准 标准&gt;34、もう&lt;加强语义&gt;]]></content>
      <categories>
        <category>日语</category>
      </categories>
      <tags>
        <tag>句型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日语助词总结]]></title>
    <url>%2F2019%2F01%2F02%2F%E6%97%A5%E8%AF%AD%E5%8A%A9%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[本篇主要记载日语助词用法，以便个人查询 か1、＜疑问＞ （5.1 P41） ✿ その人は王さんですか。 お元気ですか。2、疑问词 か＜虚指＞ （9.1 P152） ✿ どこかへ行きますか。3、NかN＜选择＞ （9.2 P161） ✿ 朝はミルクかジュースを飲みます4、含疑问词的简体句 か （17.2 P44） ✿ 図書館はどこか教えてください。 で1、＜处所＞（动作发生的场所） （8.1 P129） ✿ 喫茶店でコーヒーを飲みます。 2、＜工具，方式，手段＞ (8.2 P138) ✿ バスで行きます。 3、＜限定範囲，数量，主体＞ ✿ 富士山は日本で一番高い山です。 ✿ 300元ぐらいでシルクのチャイナドレスが買えます。 ✿ 二人で行きましょう。 4、＜原材料＞ （12-1 p232） ✿ 紙で飛行機を作った。（变化小：で 变化大：から） 5、＜原因＞ ✿ 事故で遅刻しました。 6、＜事件发生的场所＞ ✿ 運動場で試合がある。 と1、＜并列＞ （5.2 P55） ✿ 家族は三人です。父と母とわたしです。 2、＜比较的基准＞ （7.2 P100） ✿ 私は鈴木さんと同じ大学です。3、＜相互动作的对象、同一动作的参与者＞ （9.1 P152） ✿ 兄は私の友達と結婚しました。 （双向性） ✿ 私は友達と一緒にお酒を飲みます。４、と＜引用＞ （11.1 ｐ204） ✿ 李さんは来週帰国すると聞きました。５、と言う＜引用＞ （13-3 p274） ✿ 食事のとき、「いただきます」と言います。６、と言うＮ （14-1 p282） ✿ 浜崎あゆみという歌手7、条件 （17-1 p33） ✿ 春になると、花が咲きます。8、习惯，反复 （17-1 p33） ✿ 子供のころ、冬になると毎年スキーに行った。 は1、は＜部分否定＞ (13.3 p275) ✿ 毎日はしません。 も1、も＜主观多量＞ （12.2 p242） ✿ 3万円もするんです。 の1、＜准体＞ （12-1 p231） ✿ 美味しいのが好きです。2、＜连体修饰语从句主语＞ （14-2 p293） ✿ 母の作った料理は美味しい。3、＜名词化＞ (14-2 p293) ✿ 先生が行ったのを見ました。 に1、に＜周期＞ （14-2 p291） ✿ 一週間に2回家族に電話します。2、に＜状态、性质的对象＞ （14-2 p291） ✿ 京劇に詳しい。 ✿ 学生に優しい。 ので VS から1、接续不同：名词/2类形容词词干+なので 名词/2类形容词词干+だから2、意义不同：「ので」一般用于陈述客观的原因。 后项既定事实、客观事实、绝大多数人的看法，语气更缓和； 「から」一般用于陈述主观的理由。 后项可以是既定事实也可以是意志、推测、主张。3、语气不同：から较生硬，因此妇女和儿童一般来说为避免过分强调自己的主张 Ｎ（＋格助词）のＮA、「が/を/に/へ/で/と/から/まで」等格助词一般接名词后与动词搭配使用。B、除此之外，还可以与「の」结合后构成复合形式，用于修饰名词。 ✿ 高橋さんが留学する 高橋さんの留学 ✿ 日本語を学習する 日本語の学習 ✿ 日本に留学する 日本への留学 ✿ 学校まで案内する 学校までの案内 ✿ 海外へ出張する 海外への出張 ✿ 北京で生活する 北京での生活 ✿ 日本人と会話する 日本人との会話 ✿ 母に手紙を送る 母への手紙注意：没有がの、をの，另外にの要变成への。]]></content>
      <categories>
        <category>日语</category>
      </categories>
      <tags>
        <tag>助词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[词型总结]]></title>
    <url>%2F2018%2F12%2F31%2F%E6%97%A5%E8%AF%AD%E5%8F%98%E5%BD%A2%2F</url>
    <content type="text"><![CDATA[本篇主要记载日语常用变形，以便个人查询 词型 词序： 词类：1、独立词:能在句子中单独使用、但需要附属词连接构成句子。2、附属词: 在句子中只能附在独立词后面起一定的语法作用。 体言/二类形容词非过去时：肯定: 私は 学生です/だ否定: 私は 学生では(じゃ)ありません/ では ないです。 过去时：表达过去曾是/不是~肯定:学生でした /だった否定:学生ではなかったです。 ではありませんでした。 形容词一类形容词おもしろい 面白い 敬体非过去时肯定：面白いです否定：おもしろくないです 敬体过去时肯定：おもしろかったです否定：おもしろくなかったです 简体直接去掉です。 二类形容词簡単 敬体非过去时肯定：簡単です否定：簡単ではありません 敬体过去时肯定：簡単でした否定：簡単ではありませんでした 简体非过去时肯定：簡単だ否定：簡単ではない 简体过去时肯定：簡単だった否定：簡単ではなかった 形容词的副词化一类形容词的副词化1: 面白くなりました2: 面白くて 二类形容词的副词化1：簡単にする2：簡単で 动词的分类I类动词 （五段动词）a、词尾是る、倒数第二个假名不在い段或え段（始まる）b、词尾在う段（行く） II类动词 （一段动词）倒数第二个假名在い段（起きる） 或え段（食べる） III类动词カ变动词：来る（只有这一个）サ变动词：a、する（实义动词） b、词干+する：勉强する 词尾是る 动词的敬体I类动词う段变成い段，再接ます：かく ＝＞ かきます II类动词去掉る接ます：みる ＝＞ みます III类动词くる ＝＞ きますする ＝＞ します 动词的て型I类动词① く、ぐ → いて、いで「特例：行く → 行って」 （聞く→ 聞いて） （防ぐ→防いで） ② う、つ、る → って （会う→ 会って） （打つ→打って）（降る→ 降って）③ む、ぶ、ぬ → んで （読む→読んで） （呼ぶ→呼んで）（死ぬ→死んで）④ す →して （探す→探して） II类动词去掉词尾「る」＋て （食べる → 食べて） III类动词来る → 来てする → して 勉強する→勉強して て型用法1、连接两个动词，表示先后/并列/因果关系（句子的时态由后面的动词决定） ✿ 手を洗ってご飯を食べます。 ✿ 町へ行って、服を買いました。2、动词+てから+动词表示先做...再做...（表示先后的动作，强调动作的顺序） ✿ 手を洗ってから、ご飯を食べます。 ✿ 宿題を終わってから、テレビを見ます。3、动词+てもいいです即使...也可以（表示假定条件/让步） ✿ バスで行ってもいいです。 ✿ このケーキを食べてもいい。4、动词+てはいけません 不许做...（轻微命令/禁止） ✿ ここは、写真を撮ってはいけません。 ✿ この本を売ってはいけません。5、动词+ている 表示正在进行的事情/状态 a、表示正在做...（强调正在进行） ✿ 今、雨が降っています。 ✿ 今、ご飯を食べています b、表示动作结果的持续 （强调句子中提到的这个动作正在持续中的一个状态） （一般接一些表示状态变化的动词：開く、行く、来る、帰る、住む、着る「穿戴动词」） ✿ 田中さんは結婚しています。 （表示田中先生现在是一个已婚，结了婚的状态） ✿ あの時、雨が降っていました。 （这里的ている表示的是一种下着雨的状态，而“那个时候”是过去，所以要用过去时态）5、动词+て行く / て来る &lt;主体的移动&gt; a、表示移动性动词动作的方向性。 ✿ 由近及远为ていく(~去)：風船が飛んでいった。 ✿ 由远及近为てくる(~来)：日本から戻ってきた。 b、表示完成V动作后再进行方向性移动，或表示保持某状态进行移动。 ✿ 携帯電話を持って行く。 ✿ 飲み物を買ってくる。 动词的简体否定型I类动词う段变成あ段，再接ない：読む ＝＞ よまない II类动词去掉る接ない：着る ＝＞ きない III类动词くる ＝＞ こないする ＝＞ しない ない型的接续和用法1、动词ない形+ないでください请不要（做某个动作） ✿ 見ないでください。 ✿ 泣かないでください。2、动词ない形+なくてもいい即使不...也不可以 ✿ この薬は飲まなくてもいいです。 ✿ 会社に行かなくてもいいです。3、动词ない形+なければなりません（或なくてはいけない） 一定，必须（比较正式，书面化） ✿ ご飯を食べなければなりません。 ✿ 薬を飲まなければなりません。4、动词ない形+ないといけません 一定，必须（比较口语化） ✿ ご飯を食べないといけません。 ✿ 薬を飲まないといけません。 动词的简体过去式I类动词く、ぐ发生“イ音变”：Vく ＝＞ Vいた； Vぐ ＝＞ Vぃだう、つ、る发生“促音变”：Vう/つ/る ＝＞ Vったぬ、ぶ、む发生“拨音变”：Vぬ/ぶ/む ＝＞ Vんだす不发生音变：Vす ＝＞ Vした特殊动词：行く ＝＞ 行った II类动词直接去掉“る”,然后接“た” III类动词くる ＝＞ きたする ＝＞ した 动词的连体形（体：体言，即名词）连体形的形式1、动词原形+名词：表示现在/即将发生的动作/状态&amp;经常性的动作/状态 電話する人。 話す人。２、动词「た」形（动词过去式）+名词：表示已经发生的动作/状态 服を買った人。 電話をした人。 连体形的接续和用法1、动词连体形+形式体言「こと」（こと一般不翻译） 魚を食べること。 吃鱼（这件事情） 魚を食べることが嫌いです。2、动词连体形+ことができる 会、能（做...） 日本語を喋ることができます。「动词连体形+ことができません 不会、不能（做...）」 日本語を喋ることができません。3、动词连体形+ことがあります表示曾经（做过...） 日本人と話したことがあります。 アメリカに行ったことがあります。4、动词连体形+ほうがいい最好（做...） 水を飲むほうがいいです。 はやく寝たほうがいいです。５、动词连体形（动词原形）+前に＋动词/句子 在做...之前，先做 テレビを見る前に、勉強します。 会社に行く前に、朝ご飯を食べました。６、动词连体形（动词过去式）+後で＋动词/句子 在做了...之后，再做... 勉強した後で、遊びに行きました。 手を洗った後で、お菓子を食べました。 动词的能动态相当于汉语的“能～，会～，可以～”等意 I类动词う段变成え段，再接る：送る ＝＞ 送れる II类动词去掉る接られる：出る ＝＞ 出られる III类动词くる ＝＞ こられるする ＝＞ できる 表示能力的几种方式1、本身具有表示能力的自动词（見える、聞こえる） ✿ 海が見える。2、Ｎができます（できる表示具备某种能力的意思）“能~，会~” ✿ 私はピアノができる。3、Ｎがわかります（わかる表示对人或事物的了解）“明白~，了解~，懂~” ✿ 私は英語はわかりません。 注意：一般用が提示，否定多用「は」4、Vる＋ことができる表示“可能”，表示本身具有某能力做某事。 ✿ 一気に十階まで登ることができます。 动词的意志型表示意志、建议 I类动词う段变成お段，再接う：かく ＝＞ かこう II类动词去掉る接ます：あつめる ＝＞ あつめよう III类动词くる ＝＞ こようする ＝＞ しよう 意志型的使用1、用于第一人称句时，表明说话人的意志； 用于第一、二人称时，表示建议对方做某事。（「Ｖましょう」的简体）2、动词的意志形后接「と思う」构成「Ｖようと思う」句型。 用于表达说话人做某事的决心、打算。“要~，决心~”。 ✿ 僕もそうしよう。 ✿ みんなと一緒に遊ぼうよ。 动词的命令型I类动词う段变成え段，再接う：行く ＝＞ 行け II类动词去掉る接ろ：食べる ＝＞ 食べろ III类动词来る ＝＞ こいする ＝＞ しろ 动词的被动型I类动词う段变成あ段，再接れる：買う → 買われる II类动词去掉る接られる：食べる → 食べられる III类动词来る→ こられる する→ される 动词的使役型I类动词う段变成あ段，再接せる：知る → 知らせる II类动词去掉る接させる：食べる → 食べさせる III类动词来る→ こさせる する→ させる 动词、形容词的条件型相当于汉语的“如果～就～、假如～、要是～”等 I类动词う段变成え段，再接ば：いそぐ ＝＞いそげば II类动词去掉る接れば：おきる ＝＞ おきれば III类动词くる ＝＞ くればする ＝＞ しれば 形容词I类：去掉i加けれ：たかい ＝＞ たかければII类：接なら，再接ば(现代日语中ば大多省略)：げんき ＝＞ げんきなら(ば) 动词的ない型Vない ＝＞ Vなければ]]></content>
      <categories>
        <category>日语</category>
      </categories>
      <tags>
        <tag>词型变换</tag>
      </tags>
  </entry>
</search>
